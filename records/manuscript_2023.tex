\documentclass[10pt,a4paper,draft]{article}
%\documentclass[10pt,a4paper]{article}

%\usepackage[top=3cm, bottom=0cm, left=3.5cm,right=2cm]{geometry}
\usepackage[top=1in, left=1in ,right=1in, bottom=1in, footskip=0in, marginparwidth=0in]{geometry}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}

% clean citations
%\usepackage{cite}
%\usepackage[super,sort&compress,comma]{natbib}
\usepackage[numbers, round, sort&compress, comma]{natbib}
%\usepackage[round]{natbib}

% hyperref makes references clicky. use \url{www.example.com} or \href{www.example.com}{description} to add a clicky url
%\usepackage{nameref,hyperref}

% math
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bbold}

% line numbers
\usepackage[right]{lineno}

% improves typesetting in LaTeX
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }
\usepackage{enumitem}

% text layout - change as needed
%\raggedright
%\setlength{\parindent}{0.5cm}
%\textwidth 5.25in 
%\textheight 8.75in

% Remove % for double line spacing
%\usepackage{setspace} 
%\doublespacing

% adjust caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% remove brackets from references
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% use \textcolor{color}{text} for colored text (e.g. highlight to-do areas)
\usepackage{color}

% define custom colors (this one is for figure captions)
\definecolor{Gray}{gray}{.25}

% this is required to include graphics
\usepackage{graphicx}
\usepackage{sidecap}

% hyperlinks
\usepackage{hyperref}

% ########################################################

%\pagestyle{headings}
\pagestyle{myheadings}
\markright{}

\begin{document}

\thispagestyle{empty}

% title goes here:
\begin{flushleft}
{\Large
\textbf\newline{Title}
}
%\newline
%% authors go here:
%\\
%Loreen Hert\"ag\textsuperscript{1,*},
%Claudia Clopath\textsuperscript{1,ยง}
%\\
%\bigskip
%\bf{1} Bioengineering Department, Imperial College London, London, UK.
%\\
%\bigskip
%* l.hertag@imperial.ac.uk, ยง c.clopath@imperial.ac.uk

\end{flushleft}

% now start line numbers
%\linenumbers

\begin{abstract}
blahhh blahhh blah
\end{abstract}

\section*{Introduction}

XXX
with a special focus on the interneurons and how and hwy those may adapt yhe weighting XXX


\section*{Results}
%
XXX

\subsection*{nPE and pPE neurons as the basis for computing mean and variance of sensory stimuli}
%
We hypothesis that the unique properties of nPE and pPE neurons put them in a perfect position to support both the computation of the mean and the variance of feedforward sensory stimuli. XXX Why is that XXX. 

To this end we simulated a rate-based mean-field network model the core of which represents a PE circuit with nPE and pPE neurons and three types of inhibitory interneurons (XXX). In addition, the PE circuits connect/project to a memory neuron that is modeled as perfect integrator. In accordance with XXX, we assume that the pPE neurons excite the memory neuron while the nPE neuron inhibit the memory neuron (for instance through lateral inhibition not modeled explicitly here). With this connectivity/setup/architecture, the memory neuron holds/encodes the mean of the feedforward sensory stimuli that drive the PE circuit. The memory neuron, on the other hand, connects to the apical dendrites of the PE neurons and some of the inhibitory interneurons (see methods for more details). It therefore serves as a prediction that is dynamically updated when new sensory information is available. We furthermore assume that the PE neurons excite another neuron, modeled as a leaky integrator, whose activity may represent that variance of the feedforward stimuli when both nPE and pPE neurons have a net excitatory effect on it.

To show that such a network can indeed represent mean and variance in the respective neurons, we stimulate it with sequence of step-wise constant inputs drawn from xxx. As shown in Fig. XXX, the memory neuron's activity gradually approaches the mean of the sensory stimuli, while the v neuron approaches the variance of the inputs. This is true for a wide range of inputs statistics (Fig. XXX). Deviations from the true mean occur mainly for larger input variances. The estimated variance is fairly independent of the input statistics tested. Moreover, the precise input distribution does not have a significant effect on the network's ability to estimate the mean and the variance (Supp Fig. XXX).

XXX interneuron activity increases with mean but also with variance, some faster than the other ... XXX

XXX validated that also correct for population network (beyond mean-field network) XXX

XXX assumptions (BL, gain-nPE = gain-pPE = 1) and how important really (see above)

\subsection*{Weighting external and internal signals requires two sets of PE neurons}
%
Coming back to the question at hand/example ...
Following up the same ideas, the weighting of internal and external signals requires a higher PE circuit that integrates over the prediction of the lower PE circuit. 
XXX explain set-up XXX ...
XXX stimulation protocol XXX
XXX two limit cases XXX
XXX validation for a wide range of input statistics XXX
XXX mentioning the dynamic nature of this estimation (Supp material), if you find differences in time scales of adaptation, report here XXX
XXX predictions: nPE/pPE BL increase, cognitive load + trial duration XXX
XXX The network does not require on the squared activation function but it is important that updating is faster in lower than in higher PE circuit XXX

XXX Do we actually propose that predictions are sent up the hierarchy?

\subsubsection*{XXX IN influence, neuromodulators ... mechanisms XXX}
%



\subsection*{XXX contraction bias}

\section*{Discussion}

We solved the brain.


\section*{Models and methods}
%

\subsection*{Network model}
%
Network consists of two subnetworks. Each subnetwork consists of a PE circuit, a memory neuron and a neuron representing the variance. XXX The memory neuron of subnetwork feefdowradly connects to the PE circuit of the second subnetwork.

\subsubsection*{Prediction-error network model}
%
Consider a mean field network in which each population is represented by one representative neuron. The mean-field PE network consists of an excitatory nPE and pPE neuron, as well as two inhibitory PV neurons (one receiving S, the other P), as well as inhibitory SOM and VIP neurons.

Each excitatory pyramidal cell (that is, nPE or pPE neuron) is divided into two coupled compartments, representing the soma and the dendrites, respectively. The dynamics of the firing rate~$r_{\mathrm{E}}$ of the somatic compartment of obeys \citep{wilson1972excitatory}
%
\begin{align}
\tau_E\ \frac{dr_\mathrm{E}}{dt} &= - r_\mathrm{E} + w_\mathrm{ED}\cdot  r_\mathrm{D}  -  w_\mathrm{EP}\cdot r_\mathrm{P} + I_\mathrm{E},
\end{align}
%
where $\tau_\mathrm{E}$ denotes the excitatory rate time constant ($\tau_\mathrm{E}$=60 ms), the weight $w_{\mathrm{ED}}$ describes the connection strength between the dendritic compartment and the soma of the same neuron, and $w_{\mathrm{EP}}$ denotes the strength of somatic inhibition from PV neurons. The overall input $I_\mathrm{E}$ comprises external background and feedforward sensory  inputs (see ``Inputs" below). Firing rates are rectified to ensure positivity.

The dynamics of the activity~$r_\mathrm{D}$ of the dendritic compartment obeys \citep{wilson1972excitatory}
%
\begin{align}
\tau_E\ \frac{dr_\mathrm{D}}{dt} &= - r_\mathrm{D} +  w_\mathrm{DE}\cdot r_\mathrm{E}  - w_\mathrm{DS}\cdot r_\mathrm{S} + I_\mathrm{D},
\end{align}
%
where the weight $w_{\mathrm{DE}}$ denotes the recurrent excitatory connections between PCs, including backpropagating activity from the soma to the dendrites. $w_{\mathrm{DS}}$ represents the strength of dendritic inhibition from SOM neurons. The overall input $I_\mathrm{D}$ comprises fixed, external background inputs and feedback predictions (see ``Inputs" below). We assume that any excess of inhibition in a dendrite does not affect the soma, that is, the dendritic compartment is rectified at zero. 

Just as for the excitatory neurons, the firing rate dynamics of each interneuron is modeled by a rectified, linear differential equation \citep{wilson1972excitatory},
%
\begin{align}
\label{eq:RateEqINs}
\tau_I\ \frac{dr_\mathrm{X}}{dt} =& - r_\mathrm{X} + I_{\mathrm{X}} + w_\mathrm{XE}\cdot r_\mathrm{E} - w_\mathrm{XP}\cdot r_\mathrm{P}  - w_\mathrm{XS}\cdot r_\mathrm{S} -  w_\mathrm{XV}\cdot r_\mathrm{V}, 
\end{align}
%
where $r_\mathrm{X}$ denotes the firing rate of neuron type $X$, and the weight matrices $w_\mathrm{XY}$ denote the strength of connection between the presynaptic neuron population $Y$ and the postsynaptic neuron population $X$ ($X, Y\in \lbrace P,S,V\rbrace$). The rate time constant $\tau_I$ was chosen to resemble a fast GABA\textsubscript{A} time constant, and set to 2 ms for all interneuron types included. The overall input $I_\mathrm{X}$ comprises fixed, external background inputs, as well as feedforward sensory inputs and feedback predictions (see ``Inputs" below).

\subsubsection*{Memory and variance neuron}
%
\begin{align}
\tau_m \cdot \frac{dr_\mathrm{M}}{dt} = w_\mathrm{M\leftarrow pPE} \cdot r_\mathrm{pPE} - w_\mathrm{M\leftarrow nPE} \cdot r_\mathrm{nPE} 
\end{align}
%
\begin{align}
\tau_v \cdot \frac{dr_\mathrm{V}}{dt} = -r_\mathrm{V} + (w_\mathrm{V\leftarrow pPE} \cdot r_\mathrm{pPE} - w_\mathrm{V\leftarrow nPE} \cdot r_\mathrm{nPE})^2 
\end{align}

\subsubsection*{Weighted output}
%
\begin{align}
r_\mathrm{out} = \alpha \cdot S + (1-\alpha) \cdot P
\end{align}
%
\begin{align}
\alpha &= \frac{1/r_\mathrm{V1}}{1/r_\mathrm{V1} + 1/r_\mathrm{V2}}\nonumber\\
& = \left( 1 + \frac{r_\mathrm{V1}}{r_\mathrm{V2}} \right)^{-1}
\end{align}

\subsection*{Connectivity}
%
%All neurons are randomly connected with connection probabilities motivated by the experimental literature \citep[e.g.][]{fino2011dense, packer2011dense, pfeffer2013inhibition, lee2013disinhibitory, pi2013cortical, jiang2015principles, jouhanneau2015vivo, pala2015vivo},
%%
%\begin{align}
%p = \begin{pmatrix}
%  p_\mathrm{EE}  & p_\mathrm{ED} & p_\mathrm{EP} & p_\mathrm{ES} & p_\mathrm{EV} \\
%  p_\mathrm{DE}  & p_\mathrm{DD} & p_\mathrm{DP} & p_\mathrm{DS} & p_\mathrm{DV} \\
%  p_\mathrm{PE}  & p_\mathrm{PD} & p_\mathrm{PP} & p_\mathrm{PS} & p_\mathrm{PV} \\
%  p_\mathrm{SE}  & p_\mathrm{SD} & p_\mathrm{SP} & p_\mathrm{SS} & p_\mathrm{SV} \\
%   p_\mathrm{VE} & p_\mathrm{VD} & p_\mathrm{VP} & p_\mathrm{VS} & p_\mathrm{VV} \\
%\end{pmatrix}
%=
%\begin{pmatrix}
%  - & 1 & 0.6 & - & -\\
%  0.1  & - & - & 0.55 & - \\
%  0.45  & - & 0.5 & 0.6 & 0.5 \\
%  0.35  & - & - & - & 0.5 \\
%   0.1 & - & - & 0.45 & - \\
%\end{pmatrix}.
%\label{Mtx:ConnProb}
%\end{align}
%%
%All cells of the same neuron type have the same number of incoming connections. The mean total connection strengths are set to
%%
%\begin{align}
%w = \begin{pmatrix}
%  w_\mathrm{EE}  & w_\mathrm{ED} & w_\mathrm{EP} & w_\mathrm{ES} & w_\mathrm{EV} \\
%  w_\mathrm{DE}  & w_\mathrm{DD} & w_\mathrm{DP} & w_\mathrm{DS} & w_\mathrm{DV} \\
%  w_\mathrm{PE}  & w_\mathrm{PD} & w_\mathrm{PP} & w_\mathrm{PS} & w_\mathrm{PV} \\
%  w_\mathrm{SE}  & w_\mathrm{SD} & w_\mathrm{SP} & w_\mathrm{SS} & w_\mathrm{SV} \\
%   w_\mathrm{VE} & w_\mathrm{VD} & w_\mathrm{VP} & w_\mathrm{VS} & w_\mathrm{VV} \\
%\end{pmatrix}
%=
%\begin{pmatrix}
%  - & 1 & 2^{*} & - & -\\
%  0.5 & - & - & 0.5^{*} & -\\
%  1.2 & - & 1 & 0.3^{*} & 0.3^{*} \\
%  1 & - & - & - & 0.6 \\
%  1 & - & - & 0.7 & - \\
%\end{pmatrix},
%\label{Mtx:ConnStrengths}
%\end{align}
%%
%where $^{*}$ denotes either the weights that are free for optimization to satisfy the equations describing an E/I balance (see Supporting Information), or the initial mean connection strengths that are subject to synaptic plasticity during learning. For plastic networks, the initial connections between neurons are drawn from uniform distributions 
%%
%\begin{align*}
%w_{ij}^{initial} \in \mathcal{U} \left(0.5\ w, 1.5\ w\right),
%\end{align*}
%%
%where $w$ denotes the mean connection strengths given in (\ref{Mtx:ConnStrengths}). Please note that the system is robust to the choice of connection strengths. The connection strengths are merely chosen such that the solutions of the equations describing an E/I balance comply with Dale's principle.
%
%In plastic networks, $w_\mathrm{EP}$ is subdivided into assemblies. While one-third of PCs receive stronger initial inhibition from PV neurons that are driven by sensory input, another third receives stronger initial inhibition from PV neurons that are driven by feedback predictions. More precisely, for two-thirds of the excitatory neurons, half of the connections from PV neurons are strengthened by $1.5$, while the remaining ones are weakened by $0.5$.
%
%All weights are scaled in proportion to the number of existing connections (i.e., the product of the number of presynaptic neurons and the connection probability), so that the results are independent of the population size.

\subsection*{Inputs}
%
%All neurons receive external background input that ensures reasonable baseline firing rates in the absence of sensory inputs and predictions thereof. In the case of non-plastic networks, these inputs were set such that the baseline firing rates are $r_\mathrm{E}=1\, s^{-1}$, $r_\mathrm{P} = r_\mathrm{S}=r_\mathrm{V}=4\, s^{-1}$ and $r_\mathrm{D}=0\, s^{-1}$. In the case of plastic networks, we set the external inputs of all neuron types to $x_\mathrm{E}=x_\mathrm{P}=x_\mathrm{S}=x_\mathrm{V}=5\, s^{-1}$, while the background input to the dendrites is dynamically computed during training such that $r_\mathrm{D}=0\, s^{-1}$.
%
%In addition to the external background inputs, the neurons receive either sensory input~($S$), a prediction thereof ($P$), or both. We distinguish between phases of fully predicted ($P=S>0$), overpredicted ($P>S$) and underpredicted ($P<S$) sensory stimuli, as well as baseline phases ($P=S=0$). During training, the network is exposed to baseline phases and fully predicted sensory inputs (Figs.~\ref{fig:Fig_Plasticity} and \ref{fig:Fig_Experience}), or in addition to over- and underpredicted sensory stimuli (Fig.~\ref{figsupp:Fig_Experience_Predictability}). Stimuli are drawn from a uniform distribution from the interval $[0, 5\, s^{-1}]$. Mean and SD of test stimuli for each simulation are listed below (see ``Simulations").


\subsection*{Simulations}
%
%All simulations were performed in customized Python code written by LH. Differential equations were numerically integrated using a 2\textsuperscript{nd}-order Runge-Kutta method with time steps ranging between $0.1$ and $0.2$ ms. Neurons in the PE circuits were initialized with $r=0/s$. The memory neurons were initialized at the mean of the two distributions (see above), and each prediction neuron was either set to the mean of the distribution it is associated with if the stimulus at $t=0\ \mathrm{ms}$ was drawn from that distribution, or set to zero otherwise. Each stimulus was presented for $1$ second. During training of the PE circuit, we presented 350 stimuli alternated with 350 zero-input (baseline) phases. We made sure that the weights converged to a configuration that satisfied the objective given by our homeostatic plasticity rules (see Eqs.~\ref{eq:Plasticity_I}-\ref{eq:Plasticity_II} in ``Plasticity model"). We defined the steady-state firing rate per stimulus as the activity in the last 500 ms of stimulus presentation. The onset firing rate was computed as the activity of the first 10 ms.\\\\
%%
%\textbf{Figures 1 \& S2:} Test stimulus was set to $5/s$ with a SD of $1/s$. Stimulus to compute total excitatory and inhibitory inputs was set to $1/s$.\\
%%
%\textbf{Figures 2 \& S3:} Test stimulus was set to $3/s$ with a SD of $1/s$. The perturbation stimuli ranged from $-1.5/s$ to $1.5/s$.\\
%%
%\textbf{Figures 3, S4 \& S5:} Test stimulus was set to $5/s$ with a SD of $1.5/s$. 50\% of the PV neurons, 70\% of the SOM neurons and 30\% of the VIP neurons receive the actual sensory input, while the remaining ones of each population received a prediction thereof. Perturbation stimulus was $\pm 2/s$. Panels D \& G of Fig. 3 show the median over all PE neurons and the SEM.\\
%%
%\textbf{Figures 4 \& S6:} Test stimulus was set to $5/s$ with a SD of $1.5/s$. In main figure, square: $w_\mathrm{EP}\in[2,4]$, $w_\mathrm{PS}\in[0.5,1]$, $w_\mathrm{PV}\in[1.5,2.5]$; circle: $w_\mathrm{EP}\in[2.5,8]$, $w_\mathrm{PS}\in[1.5,2.5]$, $w_\mathrm{PV}\in[0.5,1]$; triangle: $w_\mathrm{EP}\in[2.5,8]$, $w_\mathrm{PS}\in[1,2.5]$, $w_\mathrm{PV}\in[0.5,2]$. Half of the PV neurons and all SOM neurons receive the actual sensory input, while the remaining PV and SOM neurons as well as all VIP neurons receive a prediction thereof.
%In supporting figure: Total number of stimuli presented during training was increased, so that the number of fully predicted sensory stimuli was constant at 350. Results were averaged over 5 simulations, mean and SD are shown.\\
%%
%\textbf{Figure 5:} For panel E, the performance error was computed as the squared difference between the activity of the respective line attractor and the stimulus presented. For panel F, the initial weight between the stimulus and the representation neuron was set to $0.5$. The basis learning rate (fixed) was set to $5e^{-4}$. And the initial speed was computed as the derivative of the rate with respect to time, averaged over the first 50 ms. 
%
%Source code to reproduce the simulations, analyses and figures will be available after publication at \url{github.com/lhertaeg/SourceCode_Hertaeg2021}. 

\section*{Acknowledgments}
%
%We are grateful to Vezha Boboeva, Douglas Feitosa Tom\'e, J\'ulia Gallinaro and Klara Kaleb for helpful comments on earlier versions of this manuscript, and we want to thank all members of the Clopath lab for discussion and support. This work was supported by BBSRC BB/N013956/1, BB/N019008/1, Wellcome Trust 200790/Z/16/Z, Simons Foundation 564408 and EPSRC EP/R035806/1.

\bibliographystyle{naturemag} %{plainnat}
\bibliography{../References_Mismatch}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section*{Supplementary Information}
\appendix


\subsection*{Sensory weight and contraction bias}
%
If P is rather constant, the slope in the contraction bias is exactly the sensory weight
%
 \begin{align*}
 r_\mathrm{out} = \alpha_\mathrm{S} \cdot S + \left( 1 -\alpha_\mathrm{S} \right) \cdot P \equiv m \cdot S + n
 \end{align*}
%
However, P is usually/normally a function of $S$. For simplicity, let's assume that P decays exponentially to a new value of $S$:
%
\begin{align*}
P = P_\mathrm{0} \cdot \mathrm{e}^{-t/\tau} + f(S) \cdot \left( 1 -   \mathrm{e}^{-t/\tau} \right)
\end{align*}
%
Within each trial with trial duration T, P can be expressed by n sections of length t in which the stimulus is constant and, for the sake of simplicity, drawn from a uniform distribution $U\left( s - \frac{\sigma_\mathrm{S}}{12}, s + \frac{\sigma_\mathrm{S}}{12} \right)$. $P_\mathrm{0}$ is drawn from $U\left( \mu - \frac{\sigma_\mathrm{P}}{12}, \mu + \frac{\sigma_\mathrm{P}}{12} \right)$. $P_\mathrm{n}$ is then given by
%
\begin{align*}
P_\mathrm{n} = P_\mathrm{0} \cdot \mathrm{e}^{-t/\tau}  + \left( 1 -   \mathrm{e}^{-t/\tau} \right) \sum_{i=1}^{n} s_i \cdot \mathrm{e}^{-(n-1)\cdot t/ \tau}
\end{align*}
%
This needs to be averaged over all possible states
%
\begin{align*}
P_\mathrm{n} = \mathrm{e}^{-t/\tau}  \int\limits_{\mu - \frac{\sigma_\mathrm{P}}{12}}^{\mu + \frac{\sigma_\mathrm{P}}{12}} P_\mathrm{0} \ f(P_\mathrm{0})\ dP_0+ \left( 1 -   \mathrm{e}^{-t/\tau} \right) \sum_{i=1}^{n} \cdot \mathrm{e}^{-(n-1)\cdot t/ \tau} \int\limits_{s - \frac{\sigma_\mathrm{S}}{12}}^{s + \frac{\sigma_\mathrm{S}}{12}} s\ f(s)\ ds
\end{align*}
%
This gives
%
\begin{align*}
P_\mathrm{n} = \mu \cdot \mathrm{e}^{-T/\tau} + \left( 1 -   \mathrm{e}^{-t/\tau} \right) \sum_{i=1}^{n} \mathrm{e}^{-(n-i)\cdot t/ \tau} \cdot S
\end{align*}
%
By making use of the geometric series, this simplifies to
%
\begin{align*}
P_\mathrm{n} =  \mu \cdot \mathrm{e}^{-T/\tau} + \left( 1 -   \mathrm{e}^{-T/\tau} \right) \cdot S
\end{align*}
%
Together, this yields for the weighted output
%
\begin{align*}
 r_\mathrm{out} = \left[ \alpha_\mathrm{S}\  \mathrm{e}^{-T/\tau} + \left( 1 -   \mathrm{e}^{-T/\tau} \right)\right] \cdot S + \left( 1 -\alpha_\mathrm{S} \right)\ \mathrm{e}^{-T/\tau}\ \mu
\end{align*}
%
Hence, the slope is a function of both the sensory weight and the trial duration. 
 
In a prediction-driven input regime ($alpha_\mathrm{S} \sim 0)$, the slope is independent of the sensory weight and only determined by the trial duration, $m \sim \left( 1 -   \mathrm{e}^{-T/\tau} \right)$. In a sensory-driven input regime ($alpha_\mathrm{S} \sim 0)$, the contraction bias vanishes ($m \sim 1$). 

If the trail duration is short ($T \rightarrow 0$), the slope is given by the sensory weight. If the trail duration approaches infinity, the slope would be 1 again (however, this seems rather unrealistic, this would only be true in an ideal system without memory decay or reproduction and accumulation noise ...).

\subsection*{Effect of nPE and pPE gain}
%
In the steady state, the averaged effect of nPE and pPE must be equal (so the gain must be equal):
%
\begin{align*}
g_{pPE}\ \langle \mathrm{nPE}\rangle &= g_{nPE}\ \langle \mathrm{pPE}\rangle \\
g_{pPE} \langle \left[ S-P\right]_+\rangle &= g_{nPE} \langle \left[ P-S\right]_+\rangle \\
g_{pPE} \int\limits_P^b \left( x-P\right)\ f(x)\ dx &= g_{nPE} \int\limits_a^P \left( P-x\right)\ f(x)\ dx
\end{align*}
%
Example, uniform distribution:
\begin{align*}
g_{pPE}\ \left[ \frac{1}{2} \left(b^2 - P^2\right) - P\left(b - P\right)\right] = g_{nPE}\ \left[  P\left(P - a\right) - \frac{1}{2} \left(P^2 - a^2\right)\right] 
\end{align*}
%
which gives
\begin{align*}
P = \frac{g_{pPE}\ b - g_{nPE}\ a \pm \sqrt{g_{nPE}\ g_{pPE}} (a-b)}{g_{pPE} - g_{nPE}}
\end{align*}
%
Only if $g_\mathrm{nPE}=g_\mathrm{pPE}$, the prediction $P$ is given by $\frac{a+b}{2}$.

For variance, we need $g_{nPE}=g_{pPE}=1$. In case of a uniform distribution, we get
%
\begin{align*}
V &= \langle (S-P)^2\rangle \\
   &= g_\mathrm{pPE}\ \langle \langle[ S-P \rangle]_+ \rangle + g_\mathrm{nPE}\ \langle \langle[ P-S \rangle]_+ \\
   &= \frac{g_\mathrm{pPE}}{b-a} \int\limits_P^b (u-P)^2\ du + \frac{g_\mathrm{nPE}}{b-a} \int\limits_a^P (P-u)^2\ du \\
   &= \frac{g_\mathrm{pPE}}{3} \cdot \frac{(b-P)^3}{b-a} + \frac{g_\mathrm{nPE}}{3} \cdot \frac{(P-a)^3}{b-a} 
\end{align*}
%
if $P = (b+a)/2$
%
\begin{align*}
V = \frac{g_\mathrm{pPE} + g_\mathrm{nPE}}{24} \cdot (b-a)^2
\end{align*}
%
Only if $g_\mathrm{nPE}=g_\mathrm{pPE}=1$, the V neuron encodes the variance, that for a uniform distribution is given by
%
\begin{align*}
V = \frac{ (b-a)^2}{12}
\end{align*}
%
XXXXXXX add the version where P is given by the g version ...
%
Of course, P is also a function of the gains (see above). If we take this into account, V is given by
%
\begin{align*}
V = \frac{(b-a)^2}{3\ (g_\mathrm{pPE} - g_\mathrm{nPE})^3} \cdot \left[ g_\mathrm{nPE} \cdot( g_\mathrm{pPE} \mp \sqrt{g_\mathrm{nPE}\ g_\mathrm{pPE}})^3 - g_\mathrm{pPE} \cdot (g_\mathrm{nPE} \mp \sqrt{g_\mathrm{nPE}\ g_\mathrm{pPE}})^3\right]
\end{align*}



\subsection*{Effect of BL activity on mean an variance}
%
\begin{align*}
\langle pPE \rangle &= \langle nPE \rangle \\
\langle \left[ S - P\right]_+ + p_0\rangle &= \langle \left[ P - S\right]_+ + n_0\rangle \\
\int\limits_P^b (x - P)\ f(x)\ dx + p_0 \underbrace{\int\limits_a^b f(x)\ dx}_{=1}  &= \int\limits_a^P (P - x)\ f(x)\ dx + n_0 \underbrace{\int\limits_a^b f(x)\ dx}_{=1} 
\end{align*}
%
In case of a uniform distribution, that is $f(x) = \frac{1}{b-a}$ for $x\in [a,b]$, 0 otherwise, this leads to
%
\begin{align*}
P = \frac{b+a}{2} + \frac{p_0 + n_0}{b-a}
\end{align*}
%

Variance as function of bias in mean:
\begin{align*}
V &= \frac{1}{n} \sum_i \left( x - \left(\mu \pm \delta\mu\right)\right)^2 \\
&= \frac{1}{n} \sum_i \lbrace  \left( x_i - \mu \right)^2 + \delta\mu^2 \mp 2\delta\mu (x_i - \mu)\rbrace \\
&= V_\mathrm{uniform} + \delta\mu^2 \mp 2\ \delta\mu \left( \frac{1}{n} \sum_i x_i- \mu\right) \\
&= V_\mathrm{uniform} + \delta\mu^2
\end{align*}
%

Variance as a function of BL in nPE and pPE:
%
\begin{align*}
V &= \langle \left( \mathrm{pPE} + \mathrm{nPE} \right)^2 \rangle \\
&= \langle \left[ S-P\right]_+\rangle + \langle \left[ P-S\right]_+\rangle + (p_0 + n_0)^2 + 2\ (p_0 + n_0)\ \left( \langle \left[ S-P\right]_+\rangle + \langle \left[ P-S\right]_+ \rangle\right)
\end{align*}
%
In case of a uniform distribution, this can be expressed by
%
\begin{align*}
V = \frac{1}{3\ (b-a)} \left[ (b-P)^3 + (P-a)^3\right] + (p_0 + n_0)^2 + \frac{(p_0 + n_0)}{b-a} \left[ (b-P)^2 + (a-P)^2\right]
\end{align*}
%
Since $P = \frac{b+a}{2} + \frac{p_0 + n_0}{b-a}$, this yields
%
\begin{align*}
V =& \frac{1}{3\ (b-a)} \left[ \left( \frac{b-a}{2} - \frac{p_0 - n_0}{b-a}\right)^3 + \left( \frac{b-a}{2} + \frac{p_0-n_0}{b-a}\right)^3\right] + (p_0 + n_0)^2 \\
&+ \frac{(p_0 + n_0)}{b-a} \left[ \left( \frac{b-a}{2} + \frac{p_0-n_0}{b-a}\right)^2 + \left( \frac{b-a}{2} - \frac{p_0 - n_0}{b-a}\right)^2\right]
\end{align*}
%
Simplifying this expression, leads to
%
\begin{align*}
V =  \frac{(b-a)^2}{12} + \frac{(p_0-n_0)^2}{(b-a)^2} \left( 1 + 2\ \frac{p_0+n_0}{b-a}\right) + (p_0 + n_0) \left( p_0 + n_0 + \frac{b-a}{2}\right)
\end{align*}
%


\subsection*{Influence of a population of nPE and pPE neurons}
XXX


\subsection*{Analysis of simplified network model, effect of time constants}

simplified model: dynamics and steady state of rM and rV, rM and rV as a function of time constants and trial duration etc., weighting, then use those expressions to discuss when weighting goes awry and how long transitions take from one state to another ...


\subsection*{Comparison to Kalman filter and Bayes Factor surprise}
%
Kalman filter. Initialisation
%
\begin{align*}
x_{0|init} &= 0 \\
P_{0|init} &= \sigma^2\ I
\end{align*}
%
with x being the system state (in my terms the prediction), P is the covariance matrix of the errors of x (in my terms the var of the predictions) and I is the identity matrix.
%
Then the "correction" is given by
%
\begin{align*}
K_k &= P_{k|k-1}\ H_k^T\ \left( H_k\ P_{k|k-1}\ H_k^T + R_k \right)^{-1} \\
x_k &= x_{k|k-1} + K_k\ \left( z_k - H_k\ x_{k|k-1}\right) \\
P_k &= \left( I - K_k\ H_k\right)\ P_{k|k-1}
\end{align*}
%
with K the kalman gain matrix, H the observation matrix ($z_k = H_k\ x_k + noise$), R the covaraince of the measurement noise and z a new observation. The last part of the Kalman filter is the "prediction":
%
\begin{align*}
x_{k|k-1} &= F_{k-1}\ x_{k-1} + B_{k-1}\ u_{k-1} \\
P_{k|k-1} &= F_{k-1}\ P_{k-1}\ F_{k-1}^T + Q_{k-1}
\end{align*}
%
with F the transition matrix ($x_{k|k-1} = F_{k-1}\ x_{k-1}$, u a deterministic perturbation, B the dynamics of the deterministic perturbation. In our terms
%
\begin{align*}
\alpha = K_k = \frac{P_{k|k-1}}{R_k + P_{k|k-1}}
\end{align*}
%
$P_{k|k-1}$, is however $\sigma_P^2$ in my implementation and $R_k$ is fixed variance of inputs $\sigma_S^2$. Hence, my implementation represents (?) the Kalman filter. Important to note is, that in my implementation we estimate the variance of inputs dynamically, so it is not set! Another nice advantage here is that I don't need a good estimate for P. I can basically initiate it as I want. Another difference is that I consider the optimal weighting in my "output neuron" and not the prediction itself ... .

XXX Comparison to Bayes Factor surprise


\end{document}
