%\documentclass[10pt,a4paper,draft]{article}
\documentclass[10pt,a4paper]{article}

%\usepackage[top=3cm, bottom=0cm, left=3.5cm,right=2cm]{geometry}
\usepackage[top=1in, left=1in ,right=1in, bottom=1in, footskip=0in, marginparwidth=0in]{geometry}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}

\usepackage{fancyhdr}

% clean citations
%\usepackage{cite}
%\usepackage[super,sort&compress,comma]{natbib}
%\usepackage[numbers, round, sort&compress, comma]{natbib}
\usepackage[round]{natbib}

% hyperref makes references clicky. use \url{www.example.com} or \href{www.example.com}{description} to add a clicky url
%\usepackage{nameref,hyperref}

% math
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bbold}

% line numbers
\usepackage[right]{lineno}

% improves typesetting in LaTeX
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }
\usepackage{enumitem}

% text layout - change as needed
%\raggedright
%\setlength{\parindent}{0.5cm}
%\textwidth 5.25in 
%\textheight 8.75in

% Remove % for double line spacing
%\usepackage{setspace} 
%\doublespacing

% adjust caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% remove brackets from references
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% use \textcolor{color}{text} for colored text (e.g. highlight to-do areas)
\usepackage{color}

% define custom colors (this one is for figure captions)
\definecolor{Gray}{gray}{.25}

% this is required to include graphics
\usepackage{graphicx}
\usepackage{sidecap}

% hyperlinks
\usepackage{hyperref}

% change name of Table of contents
\renewcommand*\contentsname{Supporting Information}

% ########################################################

%\pagestyle{headings}
\pagestyle{myheadings}
\markright{}

\hyphenation{math-e-mat-i-cal-ly} 
\hyphenation{inter-neurons} 

\begin{document}

\thispagestyle{empty}

% title goes here:
\begin{flushleft}
{\Large
\textbf\newline{Knowing what you don't know: Estimating the uncertainty of feedforward and feedback inputs with prediction-error circuits} % Uncertainty estimation with prediction-error circuits
}
\newline
% authors go here:
\\
Loreen Hert\"ag\textsuperscript{1,*},
Katharina A. Wilmes\textsuperscript{2},
Claudia Clopath\textsuperscript{3}
\\
\bigskip
1 Modeling of Cognitive Processes, TU Berlin, Berlin, Germany.\\
2 Department of Physiology, University of Bern, Switzerland.\\
3 Bioengineering Department, Imperial College London, London, UK.
\\
\bigskip
* loreen.hertaeg@tu-berlin.de

\end{flushleft}

% now start line numbers
%\linenumbers

\begin{abstract}
At any moment, our brains receive a stream of sensory stimuli arising from the world we interact with. Simultaneously, neural circuits are shaped by feedback signals carrying predictions about the same inputs we experience. Those feedforward and feedback inputs often do not perfectly match. Thus, our brains have the challenging task of integrating these conflicting streams of information according to their reliabilities. However, how neural circuits keep track of both the stimulus and prediction uncertainty is not well understood. Here, we propose a network model whose core is a hierarchical prediction-error circuit. We show that our network can estimate the variance of the sensory stimuli and the uncertainty of the prediction using the activity of negative and positive prediction-error neurons. In line with previous hypotheses, we demonstrate that neural circuits rely strongly on feedback predictions if the perceived stimuli are noisy and the underlying generative process, that is, the environment is stable. Moreover, we show that predictions modulate neural activity at the onset of a new stimulus, even if this sensory information is reliable. In our network, the uncertainty estimation, and, hence, how much we rely on predictions, can be influenced by perturbing the intricate interplay of different inhibitory interneurons. We, therefore, investigate the contribution of those inhibitory interneurons to the weighting of feedforward and feedback inputs. Finally, we show that our network can be linked to biased perception and unravel how stimulus and prediction uncertainty contribute to the contraction bias. 
%Hence, our work suggests a neural circuit level mechanism underlying the brain's astonishing ability to integrate feedforward and feedback signals.
\end{abstract}

\section*{Introduction}
%
To survive in an ever-changing environment, animals must flexibly adapt their behavior based on previously encoded and novel information. This adaptation is reflected in the information processing of neural networks underlying context-dependent behavior. For instance, when you walk down an unknown staircase in a fully lit basement, your brain might entirely rely on the feedforward (bottom-up) input your senses receive (Fig. \ref{fig:Fig_1}A, left). In contrast, when you walk down the same stairs in complete darkness, your brain might rely entirely on feedback (top-down) signals generated from a staircase model it has formed over previous experiences (Fig. \ref{fig:Fig_1}A, middle). But how do neural networks switch between a feedforward-dominated and a feedback-dominated processing mode? And how do neural networks in the brain combine both input streams wisely? For instance, if you hike down an unexplored mountain in very foggy conditions, your brain receives unreliable visual information. In addition, it can only draw on a shaky prediction about what to expect (Fig. \ref{fig:Fig_1}A, right). 

A common hypothesis is that the brain weights different inputs according to their reliabilities. A prominent example of this hypothesis is Bayesian multisensory integration \citep[see, e.g.,][]{deneve2004bayesian}. According to this theory, neural networks represent information from multiple modalities by a linear combination of the uncertainty-weighted single-modality estimates. Multisensory integration is supported by several observations showing that animals can combine information from different modalities in a fashion that minimizes the variance of the final estimate \citep{ernst2002humans, battaglia2003bayesian, kording2004bayesian, alais2004ventriloquist, rowland2007bayesian, gu2008neural, fetsch2012neural}. Here, we propose that the same concepts could be employed for the weighting of sensory inputs and predictions thereof \citep{kording2004bayesian, yon2021precision}. A central point in the weighting of inputs is the estimation of their variances as a measure of uncertainty. However, how the variance of both the sensory input and the prediction can be computed on the circuit level is not resolved yet.  

We hypothesized that prediction error (PE) neurons provide the basis for the neural computation of variances. PEs are an integral part of the theory of predictive processing which states that the brain constantly compares incoming sensory information with predictions. If those predictions are wrong, the resulting PEs allow the network to revise the model of the world, thereby ensuring that the predictions become more accurate \citep{keller2018predictive}. Experimental evidence suggests that these PEs may be represented in the activity of distinct groups of neurons, termed PE neurons \citep{eliades2008neural, keller2009neural, ayaz2019layer, audette2021temporally}. Moreover, these neurons may come in two types when excitatory neurons exhibit near-zero, spontaneous firing rates \citep{rao1999predictive, keller2018predictive}: negative PE (nPE) neurons only increase their activity when the prediction is \textit{stronger} than the sensory input, while positive PE (pPE) neurons only increase their activity when the prediction is \textit{weaker} than the sensory input. Indeed, it has been shown that excitatory neurons in rodent primary sensory areas can encode negative or positive PEs \citep{keller2012sensorimotor, attinger2017visuomotor, jordan2020opposing, audette2021temporally}. 

Here, we show that the unique response patterns of nPE and pPE neurons may provide the backbone for computing both the mean and the variance of sensory stimuli. Furthermore, we suggest a network model with a hierarchy of PE circuits to estimate the variance of the prediction, in addition to the variance of the sensory inputs. We show that in line with the ideas of multisensory integration, predictions are weighted more strongly than the sensory stimuli when the environment is stable (that is, predictable) and the sensory inputs are noisy. Moreover, we find that predictions are taken into account more at the beginning of a new trial than at the end, especially when the new sensory stimulus is reliable. In addition, we unravel the mechanisms underlying a neuromodulator-induced shift in the weighting of sensory inputs and predictions. In our model, these neuromodulators activate groups of inhibitory neurons such as parvalbumin-expressing (PV), somatostatin-expressing (SOM), and vasoactive intestinal peptide-expressing (VIP) interneurons \citep{markram2004interneurons, rudy2011three, pfeffer2013inhibition, jiang2015principles, tremblay2016gabaergic, campagnola2022local}. These interneurons have been suggested to establish a multi-pathway balance of excitation and inhibition that is the basis for nPE and pPE neurons \citep{hertag2020learning, hertag2022prediction}. By breaking this balance, the PE neurons change their baseline firing rate and gain, leading to a biased variance estimation. Finally, we show that this weighting can be understood as a neural manifestation of the contraction bias, that is, the magnitude of the represented sensory input is biased towards the mean of the past stimuli experienced \citep{hollingworth1910central, jazayeri2010temporal, ashourian2011bayesian, petzschner2011iterative, akrami2018posterior, meirhaeghe2021precise}. 


%
\begin{figure}[t!]
	\centering
    \includegraphics{../results/figures/final/Fig_1}
\caption{\footnotesize{\bf Neural network model to track both the uncertainty of sensory inputs and predictions.\newline} 
{\bf (A)} Example illustration for context-dependent integration of information. Left: When walking down an unfamiliar staircase that is visible, the brain might rely solely on external sensory information. Middle: When walking down the same stairs without visual information, the brain might rely on predictions formed by previous experience. Right: When climbing down an unexplored mountain in foggy conditions, the brain might need to integrate sensory information and predictions simultaneously.
{\bf (B)} Top: Illustration of a prediction-error (PE) circuit with both negative and positive PE (nPE/pPE) neurons that receive inhibition from three different inhibitory interneuron types: parvalbumin-expressing (PV), somatostatin-expressing (SOM), and vasoactive intestinal peptide-expressing (VIP) interneurons. Local excitatory connections are not shown for clarity. Bottom: Responses of an nPE and pPE neuron. The nPE neuron only increases its activity relative to a baseline when the sensory input is weaker than predicted, while the pPE neuron only increases its activity relative to a baseline when the sensory input is stronger than predicted.
{\bf (C)} Illustration of network model that estimates the mean and variance of the external sensory stimuli. The core of this network model is the PE circuit shown in (B). The lower-level V neuron encodes the variance, while the lower-level M neuron encodes the mean of the sensory input.
{\bf (D)} Same as in (C) but the feedforward input is the activity of the lower-level M neuron.
}
\label{fig:Fig_1}
\end{figure}
%


\section*{Results}
%

\subsection*{Prediction-error neurons as the basis for estimating the mean and variance of sensory stimuli}
%
We hypothesize that the distinct response patterns of negative and positive prediction-error (nPE/pPE) neurons act as a backbone for estimating the mean and the variance of sensory stimuli. An nPE neuron only increases its activity relative to a baseline when the sensory input is weaker than predicted, while a pPE neuron only increases its activity relative to a baseline when the sensory input is stronger than predicted. Moreover, both nPE and pPE neurons remain at their baseline activities when the sensory input is fully predicted (Fig. \ref{fig:Fig_1}B). If the prediction equals the mean of the sensory stimulus, the PE neurons, hence, encode the deviation from the mean. Thus, the squared sum of nPE and pPE neuron activity represents the variance of the feedforward input (provided that the PE neurons are silent without sensory stimulation).
%
\begin{figure}[t!]
	\centering
	%\makebox[\textwidth][c]{\includegraphics{../results/figures/final/Fig_2}}
    \includegraphics[width=1\linewidth]{../results/figures/final/Fig_2}
\caption{\footnotesize{\bf Prediction-error neurons as the basis for estimating mean and variance of sensory stimuli.\newline} 
{\bf (A)} Illustration of the inputs with which the network (Fig. \ref{fig:Fig_1}C) is stimulated. Network is exposed to a sequence of constant stimuli drawn from a uniform distribution. The gray shaded boxes symbolize different values from the distribution.
{\bf (B)} PE neuron activity hardly changes with stimulus strength (left) but strongly increases with stimulus variability (right).
{\bf (C)} Interneuron activity strongly changes with stimulus strength (left) but hardly changes with stimulus variability (right).
{\bf (D)} M neuron correctly encodes the mean of the sensory stimuli. Left: Illustration of the input synapses onto the M neuron. Middle: Activity of the M neuron over time for one example distribution (black star in right panel). Right: Normalised absolute difference between the averaged mean and the activity of the M neuron in the steady state for different parametrizations of the stimulus distribution.
{\bf (E)} V neuron correctly encodes the variance of the sensory stimuli. Left: Illustration of the input synapses onto the V neuron. Middle: Activity of the V neuron over time for one example distribution (black star in right panel). Right: Normalised absolute difference between the averaged variance and the activity of the V neuron in the steady state for different parametrizations of the stimulus distribution.
}
\label{fig:Fig_2}
\end{figure}
%

To test our hypothesis, we study a rate-based mean-field network: the core network is a prediction-error (PE) circuit with excitatory nPE and pPE neurons, as well as inhibitory parvalbumin-expressing (PV), somatostatin-expressing (SOM), and vasoactive intestinal peptide-expressing (VIP) interneurons (Fig. \ref{fig:Fig_1}B). While the excitatory neurons are simulated as two coupled point compartments to emulate the soma and dendrites of elongated pyramidal cells, respectively, all inhibitory cell types were modeled as point neurons. The connectivity of and inputs to the network were chosen such that the excitatory (E) and inhibitory (I) pathways onto the pyramidal cells were partially balanced. This balance that is only temporarily broken during mismatches has been shown to be necessary for nPE and pPE neurons to emerge \citep[][see Methods]{hertag2020learning, hertag2022prediction}. 

We assume that this core circuit is shaped by feedback connections \citep{larkum2013cellular, harris2015neocortical} that have been hypothesized to carry information about expectations or predictions \citep{mumford1992computational, larkum2013cellular, friston2008hierarchical}. To account for predictions, we model a memory (M) neuron that integrates the activity of the PE neurons (Fig. \ref{fig:Fig_1}C). Following \cite{keller2018predictive}, we assume that the pPE neuron excites the memory neuron, while the nPE neuron inhibits this neuron (for instance, through lateral inhibition, here not modeled explicitly). Because feedback connections are shown to target the apical dendrites of pyramidal cells \citep{larkum2013cellular} and interneurons located in superficial layers of the cortex \citep[see, e.g.][]{tremblay2016gabaergic}, the memory neuron makes connections with the dendritic compartment of the PE neurons and some of the interneurons (here, VIP and PV neurons, see Methods for more details). We, furthermore, simulate a downstream neuron (termed V neuron), modeled as a leaky integrator with a quadratic activation function, that receives excitatory synapses from the PE neurons. In this setting, the V neuron encodes the variance of the sensory stimuli (see the lower-level subnetwork in Fig. \ref{fig:Fig_1}C, the higher-level circuit is described later). 

To show that this network can indeed represent the mean and the variance in the respective neurons, we stimulate it with a sequence of step-wise constant inputs drawn from a uniform distribution (Fig. \ref{fig:Fig_2}A). We, hence, assume that the sensory stimulus varies over time. In line with the distinct response patterns for nPE and pPE neurons, these neurons change only slightly with increasing stimulus mean but increase strongly with input variance (Fig. \ref{fig:Fig_2}B). In contrast, the three interneurons strongly increase with stimulus mean and only moderately increase with stimulus variance (Fig. \ref{fig:Fig_2}C). The activity of the memory neuron gradually approaches the mean of the sensory inputs (Fig. \ref{fig:Fig_2}D, middle), while the activity of the V neuron approaches the variance of those inputs (Fig. \ref{fig:Fig_2}E, middle). We show that this holds for a wide range of input statistics (Fig. \ref{fig:Fig_2}D-E, right) and input distributions (Fig. \ref{fig:Fig_2_S1}). Small deviations from the true mean occur mainly for large input variances, while the estimated variance is fairly independent of the input statistics tested. 

We verified our results in a heterogeneous network in which a population of neurons belongs to each neuron type of the PE circuit, and the synaptic connection strengths from each PE neuron onto the M and V neuron are different (see Methods, Fig. \ref{fig:Fig_2_S2}A). As before, the network can correctly estimate the mean and the variance of the sensory stimuli (Fig. \ref{fig:Fig_2_S2}B). Furthermore, we show that the errors with which the M and V neurons encode the stimulus statistics are independent of uncorrelated modulations of those connection strengths (Fig. \ref{fig:Fig_2_S2}C) and the sparsity of the network (Fig. \ref{fig:Fig_2_S2}E). When all connection strengths are collectively shifted to higher values, the error increases for the variance neuron, while it remains unaffected for the memory neuron. 

While our mean-field network was designed to track the mean and the variance of stimuli that vary in time, we reasoned that the same principles apply to stimuli that vary across space. To show that, we simulated a population network that consists of unconnected replicates of the mean-field network described above (Fig. \ref{fig:Fig_2_S3}A). Each mean-field network receives a short, constant input from a different part of the receptive field. If the connection strengths from the PE neurons to the M and V neurons are adjusted accordingly (see Methods), the network correctly estimates the stimulus average and spatial uncertainty (Fig. \ref{fig:Fig_2_S3}B-C).

In summary, nPE and pPE neurons can serve as a basis to estimate the mean and the variance of sensory stimuli which vary over time and space.


\subsection*{Estimating the uncertainty of both the sensory input and the prediction requires a hierarchy of PE circuits}
%
Following the ideas of Bayesian multisensory integration, the weighting of sensory stimuli and predictions would require knowledge about their uncertainties. As we have shown in the previous section, the variance of the sensory stimulus can be estimated using PE neurons. We hypothesize that the same principles apply to computing the variance of the prediction. To show this, we augment the network with a \textit{higher} PE circuit that receives feedforward synapses from the memory (M) neuron of the \textit{lower} PE circuit (Fig. \ref{fig:Fig_1}D). Both subnetworks are identical except for the M neuron in the higher PE circuit which is modeled with slower dynamics than the one in the lower PE circuit.
%
\begin{figure}[t!]
	\centering
	%\makebox[\textwidth][c]{\includegraphics{../results/figures/final/Fig_3}}
    \includegraphics[width=1\linewidth]{../results/figures/final/Fig_3}
\caption{\footnotesize{\bf Estimating the uncertainty of both the sensory input and the prediction.\newline} 
{\bf (A)} Illustration of the stimulation protocol. The network is exposed to a sequence of stimuli (one stimulus per trial). To account for stimulus variability, each stimulus is represented by $10$ stimulus values drawn from a normal distribution. To account for the volatility of the environment, in each trial, the stimulus mean is drawn from a uniform distribution (denoted trial-to-trial variability). 
{\bf (B)} Illustration of how the weighted output is calculated. The sensory weight $\alpha$ lies between zero (system relies perfectly on prediction) and one (system relies solely on the sensory input).
{\bf (C)} Limit case example in which the stimulus variability is zero but the trial-to-trial variability is high. Left: Illustration of the stimulation protocol. Middle: Weighted output follows closely the sensory stimuli. Right: Sensory weight (function of the variances, see B) close to 1, indicating that the network ignores the prediction. Input statistics shown in E.
{\bf (D)} Limit case example in which the stimulus variability is high but the trial-to-trial variability is zero. Left: Illustration of the stimulation protocol. Middle: Weighted output pushed towards the mean of the sensory stimuli. Right: Sensory weight close to zero, indicating that the network ignores the sensory stimuli. Input statistics shown in E.
{\bf (E)} Sensory weight for different input statistics. Predictions are weighted more strongly when the stimulus variability is larger than the trial-to-trial variability.
{\bf (F)} Sensory weight, averaged over many trials, for two different trial durations. Gray shading denotes the SEM. Predictions are weighted more strongly at the beginning of a new trial. 
}
\label{fig:Fig_3}
\end{figure}
%

To test the network's ability to estimate the variances correctly, we stimulated the network with a sequence of inputs whose mean can vary from trial to trial. More precisely, in each trial, the network is given a stimulus that is composed of $N_\mathrm{in}$ constant values, each drawn from a normal distribution and presented for $N_\mathrm{step}$ consecutive time steps. The variance of this distribution represents the stimulus noise. To account for potential changes in the environment, we draw the stimulus mean from a uniform distribution (Fig. \ref{fig:Fig_3}A). Hence, the inputs change on two different time scales.
%, where the stimulus variability has a faster time scale than the trial-to-trial variability.

Following the formalism of multisensory integration \citep[see, e.g.][]{pouget2013probabilistic}, we assume that the network's output is a weighted sum of the feedforward sensory input and the feedback prediction. The weights assigned to each input stream are functions of the uncertainties, that is, the activities of the V neurons. The sensory weight captures how much the network relies on the sensory input (Fig. \ref{fig:Fig_2}B). To test our network, we first consider two limit cases. In the first limit case, we show a low-variance stimulus that differs in each trial (low stimulus uncertainty, high trial-to-trial uncertainty, see Fig. \ref{fig:Fig_3}C, left). According to the theory, the network should follow the sensory inputs closely and ignore the predictions. When we arithmetically calculate the weighted output (Fig. \ref{fig:Fig_3}C, middle) and the sensory weight (Fig. \ref{fig:Fig_3}C, right), the network indeed shows a clear preference for the sensory input. In the second limit case, we show a high-variance stimulus, the mean of which does not change from trial to trial (high stimulus uncertainty, low trial-to-trial uncertainty, see Fig. \ref{fig:Fig_3}D, left). According to the theory, the network should downscale the sensory feedforward input and weight the prediction more strongly. Indeed, the weighted output of the network shows a clear tendency to the mean of the stimuli (Fig. \ref{fig:Fig_3}D, middle), also reflected in the low sensory weight (Fig. \ref{fig:Fig_3}D, right). 

To validate the network responses fully, we systematically varied the trial and stimulus variability independently. If both variances are similar, the sensory weight approaches \textit{0.5}, reflecting equal contribution of the sensory input and the prediction to the weighted output. Only if both variances are zero, the network represents the sensory input perfectly. In line with the limit case examples above, if the stimulus variance is larger than the trial variance, the network weights the prediction more strongly than the sensory input (Fig. \ref{fig:Fig_3}E). Because the network dynamically estimates the sensory and prediction uncertainty, the sensory weight changes when the input statistics shifts (Fig. \ref{fig:Fig_3_S1}). 

Inspecting closely the dynamics of our network, we noticed that the prediction is typically weighted higher at the beginning of a new trial than in the steady state. This is particularly pronounced in a sensory-driven input regime (see Fig. \ref{fig:Fig_3}C). This is further confirmed in simulations in which the trail duration was shortened (Fig. \ref{fig:Fig_3}F). Our model makes therefore the following experimentally testable prediction: sensory predictions influence neural activity more significantly in experiments that rely on fast stimulus changes. 

It has been hypothesized, that some symptoms in psychiatric disorders like autism and schizophrenia can be ascribed to a pathological weighting of sensory inputs and predictions \citep{yon2021precision}. We thus wondered which network properties might bias the estimation of the variances, and, consequently, the weighting of different input streams. We identified the time scales at which the memory neurons incorporate new information as a decisive factor in the integration of inputs. To show this, we varied the weights from the PE neurons onto the lower-level memory neuron. If the weights are too small (the memory neuron updates too slowly), the system relies too much on feedback predictions. In contrast, if the weights are too large (the memory neuron updates too fast), the system relies too much on the feedforward sensory information (Fig. \ref{fig:Fig_3_S2}A). While the speeds at which the activity of the memory neurons evolve influence the weighting of inputs, the precise activation function of the variance neurons is less pivotal. When we replaced the quadratic activation function with a linear, rectified function, the V neurons did not encode the variance but the average absolute deviation of the sensory stimuli. However, the sensory weight is only slightly shifted to larger values for low trial/high stimulus variability (Fig. \ref{fig:Fig_3_S2}B). 

In summary, we show that the variances of both the sensory inputs and predictions thereof can be dynamically computed in networks comprising a lower and higher PE circuit. In such a network, predictions are given more weight at the beginning of a new stimulus, and if the sensory inputs are noisy while the environment is stable. 

\subsection*{Biasing the weighting of sensory inputs and predictions by neuromodulators}
%
The brain's flexibility and adaptability are supported by a plethora of neuromodulators which influence the activity of neurons in a variety of ways \citep{avery2017neuromodulatory}. A prominent target of neuromodulatory inputs is inhibitory neurons \citep{cardin2019functional,  hattori2017functions, swanson2019hiring}. Moreover, distinct interneuron types are differently (in-)activated by those neuromodulators \citep{wester2014behavioral, hattori2017functions, swanson2019hiring}. We, therefore, wondered if and how the weighting of sensory inputs and predictions thereof may be biased when neuromodulators activate distinct interneuron types.
%
\begin{figure}[t!]
	\centering
	\makebox[\textwidth][c]{\includegraphics{../results/figures/final/Fig_4.pdf}}
    %\includegraphics{../results/figures/final/Fig_4} % [width=1\linewidth]
\caption{\footnotesize{\bf Neuromodulator-based shifts in the weighting of sensory inputs and predictions.
\newline} 
{\bf (A)} Neuromodulators acting on the interneurons can shift the weighting of sensory inputs and predictions. The changes depend on the type of interneuron targeted and the modulation strength (here simulated through an additional excitatory input). Considered are two limit cases (upper row: more sensory-driven before modulation, lower row: more prediction-driven before modulation). The results are shown for three different PE circuits (denotes by different markers).
{\bf (B)} When SOM and VIP neurons are equally modulated, the sensory weight remains unaffected. 
{\bf (C)} The V neurons' activities depend on the PE neurons. Hence, perturbing the nPE and pPE neuron changes the uncertainty estimation. Left: stimulating the pPE (triangle) or nPE (upside down triangle) affects the V neuron of the same subnetwork, denoted by matching marker and line colors (dark brown: lower circuit, light brown: higher circuit). Right: while stimulating the lower PE neurons affects the higher-order V neuron, stimulating the higher-order PE neurons does not change the activity of the lower-order V neuron (denoted by not matching marker and line colors).
{\bf (D)} The V neuron activity, and hence the sensory weight, changes as a result of the modulated PE neuron activity. The PE neuron activity, on the other hand, changes as a result of the interneurons being modulated. The interneurons change the baseline (left) and the gain (right) of the PE neurons. Whether an interneuron increases or decreases the estimated variance depends on both factors.
}
\label{fig:Fig_4}
\end{figure}
%

To this end, we modeled the presence of a neuromodulator by injecting an additional excitatory input into an interneuron type (while a neuromodulator can also suppress neuronal activity, we focus on the more common excitatory effects that have been described). We reasoned that the network effect of a neuromodulator not only depends on the interneuron type it targets but also on the inputs this neuron receives and the connections it makes with other neurons in the network. We, therefore, tested different mean-field networks that differ in the distribution of sensory inputs and predictions onto the interneurons, and the underlying connectivity. The commonality across those networks is that they exhibit an E/I balance of excitatory and inhibitory pathways onto the PE neurons \citep{hertag2022prediction}. 

Across the different mean-field networks tested, increasing the activity of PV neurons biases the network's output toward predictions (Fig. \ref{fig:Fig_4}A left). In contrast, increasing VIP activity forces the networks to weigh both inputs more equally. As a consequence, predictions are overrated in a sensory-driven input regime, and, sensory inputs are overrated in a prediction-driven input regime (Fig. \ref{fig:Fig_4}A right). Increasing SOM neuron activity, while qualitatively similar to increasing VIP neuron activity, depends on the mean-field network tested and the strength of activation (Fig. \ref{fig:Fig_4}A middle). 

Neuromodulators are most likely increasing the activity of more than one interneuron type. To account for the co-activation of interneurons, we injected an excitatory input into two interneuron types at the same time and varied the strength with which each interneuron was modulated (Fig. \ref{fig:Fig_4_S1}). If PV neurons are the major target of a neuromodulator, the network is still biased toward predictions. If SOM and VIP neurons are equally stimulated, the weighting of sensory inputs and predictions remains largely unaffected (Fig. \ref{fig:Fig_4}B), suggesting that the individual effects cancel out.

What are the network mechanisms underlying these observations? The sensory weight is a function of the lower and higher variance (V) neuron activity. Hence, any changes to the sensory weight result from changes to the neurons encoding the variances. In our network, the V neurons only receive excitatory synapses from PE neurons. As a consequence, any changes in the sensory weights upon activation of interneurons must be due to changes in the PE neurons. To disentangle the effect of nPE and pPE neurons, we perturbed those neurons individually in both the lower or higher subnetwork by injecting either an inhibitory or excitatory additional input (Fig. \ref{fig:Fig_4}C).
Stimulating either PE neuron in the lower subnetwork increases the activity of the lower-level V neuron strongly. Moreover, the higher-level V neuron is also slightly affected. This is because the lower-level memory neuron is also modulated by the lower-level PE neurons and makes feedforward connections to the higher-level PE circuit. In contrast, stimulating either PE neuron in the higher subnetwork increases the activity of the higher-level V neuron but leaves the lower-level neurons unaffected. 

This suggests that to understand the effect of neuromodulators on the sensory weight, we need to unravel the effect of interneuron activation on PE neurons. Increasing interneuron activity leads to changes in the baseline and gain of PE neurons that bias the estimation of mean and variance (Fig. \ref{fig:Fig_4_S3}, see Methods). In all three networks tested, activating PV neurons decreases both baseline and gain of the PE neurons, leading to a decrease in the estimated variance (Fig. \ref{fig:Fig_4}D \& Fig. \ref{fig:Fig_4_S4}). Stimulating the SOM or VIP neuron decreases the gain in either nPE or pPE neuron. However, the baseline of those neurons can either decrease or increase depending on the connectivity with other neurons in the network. The summed effect over nPE and pPE neuron (Fig. \ref{fig:Fig_4_S4}) suggests that whether the activity of the V neuron increases or decreases depends on the input statistics: for low-mean stimuli, the elevated baseline activity dominates the changes in the variance, while for high-mean stimuli the changes in the gain dominate.

Altogether, we show that neuromodulators increasing the activity of interneurons bias the weighting of sensory inputs and predictions by changing the gain and baseline of PE neurons. Whether the sensory weight increases or decreases depends not only on the interneuron it targets but also on the network it is embedded in and the input regime.


\subsection*{Explaining the contraction bias with the weighting of sensory inputs and predictions}
% 
We hypothesized that the weighted integration of sensory inputs and predictions thereof manifests in all-day behavior, in the form of a phenomenon called \textit{contraction bias}. The contraction bias describes the tendency to overestimate sensory stimuli drawn from the lower end of a stimulus distribution and to underestimate stimuli drawn from the upper end of the same distribution. This \textit{bias toward the mean} has been reported in different species and modalities \citep{hollingworth1910central, jazayeri2010temporal, ashourian2011bayesian, petzschner2011iterative, akrami2018posterior, meirhaeghe2021precise}. 
%
\begin{figure}[t!]
	\centering
    \includegraphics{../results/figures/final/Fig_5.pdf}
\caption{\footnotesize{\bf Mechanisms underlying the contraction bias.\newline} 
{\bf (A)} Contraction bias in the model for two different stimulus uncertainties depicted in the inset. Bias is defined as the weighted output minus the stimulus mean. The absolute value of the slope of the linear fit, \textit{m}, is a measure of the bias. The larger the slope, the larger the bias.
{\bf (B)}  As a consequence of the sensory weight, the slope increases with stimulus variability (bias increases) and decreases with trial-to-trial variability (bias decreases).
{\bf (C)} Bias is independent of the trial-to-trial variability when the stimulus variability is zero.
{\bf (D)} Bias is independent of the stimulus variability when the trial-to-trial variability is zero.
{\bf (E)} The slope depends on the trial duration.
}
\label{fig:Fig_5}
\end{figure}
%

We first investigated whether the network's output can be interpreted as a neuronal manifestation of the contraction bias (see Methods for an illustrative analysis). To this end, we define the contraction bias as the trial-averaged difference between the weighted output and the sensory stimulus. When plotted over the trail-averaged stimuli, the bias is positive for stimuli below the mean of the input distribution and negative for stimuli above the mean (Fig. \ref{fig:Fig_5}A), in line with a \textit{bias toward the mean}. To measure the amount of bias in the network, we use the slope of the linear fit to the relationship between bias and trial stimulus. The larger the absolute slope, the larger the bias. 

What are the underlying network factors that contribute to the neuronal contraction bias in the network? We have seen that how much the prediction is taken into account is determined by both the lower and higher-level V neurons encoding the variance of the stimulus and the prediction. Hence, the bias must be similarly influenced by these factors. When we increase the stimulus uncertainty, the bias increases (Fig. \ref{fig:Fig_5}B). In contrast, when we increase the trial-to-trial uncertainty, the bias decreases (Fig. \ref{fig:Fig_5}B). 

To further disentangle the different sources of the bias, we first simulated a network without stimulus uncertainty (variance set to zero) for two trial-to-trail variances (volatility of the environment). In this case, the emerging contraction bias is independent of the volatility of the environment (Fig. \ref{fig:Fig_5}C). We show mathematically that the bias results from the network output not yet reaching its new steady state within the trial duration (see Methods). In other words, the bias is the difference between the weighted output at the end of the trial and its steady state (the shown stimulus). How fast the new steady state is reached depends only on the time constants in the network and not the trial-to-trial variability. We next resume the limit case in which the stimulus uncertainty is high while the trial-to-trial uncertainty is zero. In this case, the contraction bias is also largely independent of the stimulus variance (Fig. \ref{fig:Fig_5}D). Our mathematical analysis reveals that the bias is well described by the difference between the prediction, that is, mean stimulus over the history of all stimuli shown, and the current stimulus, weighted by a function of the trial duration. 

The analysis of both limit cases suggests that the bias also depends on the trial duration. To confirm this, we extended the trial duration for either limit case. As expected from the analysis, the bias decreases steadily in the simulations (Fig. \ref{fig:Fig_5}E). We, therefore, predict that the contraction bias can be reduced for sufficiently long trials. 

So far, we assumed that the stimulus variance is independent of the stimulus mean. A consequence of this choice is that the bias on either end of the input distribution is largely the same (but with reversed signs). However, behavioral data \citep[see, e.g.][]{rakitin1998scalar} shows that the bias increases for stimuli drawn from the upper end of the distribution, a phenomenon usually attributed to \textit{scalar variability}. To capture this in the model, we assume that the stimulus standard deviation linearly increases with the stimulus mean. In these simulations, as expected, the bias increases for a stimulus distribution shifted to higher trial means (Fig. \ref{fig:Fig_5_S1}).

In summary, we show that the weighted integration of sensory inputs and predictions can be interpreted as a neural manifestation of the contraction bias. While the stimulus and trial-to-trial variability shape the contraction bias, their contributions differ. Moreover, we reveal that the trial duration contributes to the bias.


\section*{Discussion}
%
% Inspiration and motivation for our work and summary of hypothesis
Our work has been driven by the puzzling question of how the brain integrates top-down feedback predictions with the sensory feedforward bottom-up inputs it constantly receives during behavior. This task may be particularly challenging when the prediction and the sensory information differ \citep{han2023behavior}. Conflicting information may be caused by noise in the sensory feedforward inputs or by changes in the environment that could not be predicted. A prominent hypothesis is that how much we rely on our predictions and new sensory evidence is determined by an intricate balance between both, based on how reliable they are \citep[see e.g.][]{kording2004bayesian, yon2021precision}. 

This idea is consistent with Bayesian theories on the optimal integration of multiple sensory cues (multisensory integration). \cite{ernst2002humans} showed that to estimate the height of a bar humans combine visual and haptic information in a fashion that minimizes the variance of the final estimate. Similar studies confirmed that animals can optimally combine multiple sensory information by taking into account their uncertainties \citep{battaglia2003bayesian, kording2004bayesian, alais2004ventriloquist, rowland2007bayesian, gu2008neural, fetsch2012neural}. These behavioral studies were accompanied by neural recordings identifying populations of neurons that can form the basis of multisensory integration \citep{wallace1998multisensory, gu2008neural, fetsch2012neural}.

%Following the same ideas from Bayesian multisensory integration, the brain must hence have mechanisms to track both the reliability of the sensory signals and the predictions thereof \citep{yon2021precision}.

\subsection*{Summary of findings}

Here, we show that PE neurons can serve as the backbone for estimating the uncertainty of both the feedforward sensory inputs and the feedback predictions (Figs. \ref{fig:Fig_2} \& \ref{fig:Fig_3}). In our model, we assume a hierarchy of PE circuits that are feed-forwardly connected through the lower-order memory neuron whose activity encodes the mean of the sensory bottom-up inputs. This local prediction is fed back to the lower-order circuit and at the same time feed-forwarded to the higher-order subnetwork (Fig. \ref{fig:Fig_1}). With this architecture in place, we show that we rely more strongly on our internal signals when the perceived sensory cues are noisier than the predictions. Moreover, our work suggests that predictions modulate neural activity more at the onset of a new sensory input, even if the stimulus is not noisy. 
%This implies that revising our inner model of the world, that is, learning from PEs, should be suppressed if the sensory noise is high or the environment switches rapidly \citep{herzfeld2014memory}. 
As a consequence, studying neural signatures of predictions in the brain might require experiments that involve sufficiently short trials.

Furthermore, we show that the weighting of sensory inputs and predictions can be biased by neuromodulators, as has been suggested before \citep[see, e.g.,][]{yon2021precision}. In our model, those modulatory signals act through interneurons \citep{cardin2019functional} whose activities increase in the presence of neuromodulators. When PV neuron activity increases, the network weighs predictions stronger than without modulation. In contrast, when VIP neuron activity increases, the network underestimates the uncertainty of the prediction in a sensory-driven regime, and it underestimates the uncertainty of the sensory input in a prediction-driven regime. Hence, the system leans toward weighting sensory inputs and predictions more equally (Fig. \ref{fig:Fig_4}A). When SOM and VIP neuron activities are modulated to the same degree, the weighting remains unaffected, suggesting that the individual contributions cancel (Fig. \ref{fig:Fig_4}B). We show that these findings can be explained by changes in the baseline and gain of PE neurons arising through the modulation of interneuron activity (Fig. \ref{fig:Fig_4}D). These results can be tested experimentally by optogenetically or pharmacologically stimulating specific interneuron types.  

Finally, we illustrate that the weighted integration of feedforward and feedback inputs can be interpreted as a neural manifestation of the contraction bias. We show that the bias is strongly driven by the variances of the sensory cue and the prediction, as well as the trial duration. While the sensory noise increases the contraction bias, the uncertainty in the prediction or long trials decreases the bias (Fig. \ref{fig:Fig_5}). However, we note that we only consider a neural representation of the stimulus and do not account for other sources of noise, like execution noise, that surely impacts the contraction bias observed in behavioral studies.
 
\subsection*{Biological evidence for model choices and assumptions}
%
In our model, we assumed that there are dedicated neurons that encode the variance of the feedforward sensory inputs and the prediction \citep[see also][]{wilmes2023uncertainty}. This assumption is consistent with the idea that neurons explicitly encode in their activity the parameters (for instance, mean or variance) of a probability distribution \citep{o2010coding, o2012can}. However, how variances of signals are represented in the brain is still not comprehensively understood and alternative ideas have been put forward. For instance, it is conceivable that the variance is encoded in a population of neurons, each differently tuned to a specific parameter \citep{knill2004bayesian}. The neurons' activities represent how close the sensory input is to the preferred (predicted) input of each neuron. Similarly, a neuron's response variability has been suggested to be related to the uncertainty of sensory stimuli \citep{hoyer2002interpreting, ma2006bayesian}.

There has been evidence that indeed (population of) neurons can encode uncertainty \citep{soltani2019adaptive}. For instance, neurons in the parietal cortex in monkeys encode the degree of confidence in a perceptual decision \citep{kiani2009representation}. Similarly, the firing rate of neurons in the orbitofrontal cortex have been shown to encode confidence irrespective of sensory modality \citep{masset2020behavior}. Neural signatures of uncertainty have been found in regions of the prefrontal cortex \citep{rushworth2008choice}, the rat insular and orbitofrontal cortex \citep{jo2016differential}, or the dorsal striatum in monkeys \citep{white2016neurons}. Moreover, the accuracy of memory recalls is encoded in single neurons of the human parietal and temporal lobes \cite{rutishauser2015representation, rutishauser2018single}.

In our model, we assume a hierarchy of predictions that are locally computed in memory neurons. These memory neurons are consistent with the idea of internal representation neurons hypothesized in predictive processing theories \citep{bastos2012canonical, keller2018predictive}. While it has been hypothesized that these internal representation neurons might be deeper L5 neurons \citep{bastos2012canonical, heindorf2022reduction}, there is also evidence that a group of excitatory L2/3 neurons integrates over negative and positive prediction errors \citep{o2022prediction}.

The core hypothesis of our model is the presence of sensory PE neurons. Those neurons have been found in different cortical areas in various species \citep{eliades2008neural, keller2009neural, ayaz2019layer, audette2021temporally}. Moreover, while first only hypothesized theoretically \citep{rao1999predictive}, the presence of two types of PE neurons, the negative and positive PE neurons, has been confirmed in several recent studies \citep{keller2012sensorimotor, attinger2017visuomotor, jordan2020opposing, audette2021temporally}. In our model, the nPE neuron inhibits the memory neuron while the pPE neuron excites it, in line with \cite{keller2018predictive}. The weights from those PE neurons onto the memory neuron are larger for the lower than the higher PE circuit, so that the lower-order memory neuron evolves faster than the higher-order counterpart. This assumption is consistent with the observation that time constants increase along the cortical hierarchy \citep{murray2014hierarchy, chaudhuri2015large, runyan2017distinct}.

If the relation of the paces at which the memory neurons evolve is strongly modulated, the network either shows a bias towards the sensory cues or the prediction (Figs. \ref{fig:Fig_3_S2}). It has been hypothesized that symptoms in psychiatric diseases may derive from an erroneous uncertainty estimation \citep{yon2021precision}. For instance, hallucinations may arise from an underestimation of the expectation uncertainty or an overestimation of the sensory uncertainty. Conversely, a fixation on the environment, even when the sensory cues indicate a switch in the environment, may originate from an overestimation of the expectation uncertainty or an underestimation of the sensory uncertainty \citep{yon2021precision}.


\subsection*{Neuromodulators and uncertainty}
%
A popular hypothesis is that neuromodulators shape the weighting of sensory inputs and predictions thereof \citep{yon2021precision}. Theoretical work by \cite{yu2005uncertainty} suggests that acetylcholine (ACh) correlates with \textit{expected uncertainty}, while noradrenaline (NA) correlates with \textit{unexpected uncertainty}. Expected uncertainty is usually interpreted as known cue-outcome unreliabilities. In contrast, unexpected uncertainty relates to the changes in the environment that produce large PEs outside the expected range of uncertainties \citep{yu2005uncertainty}. While in our network the stimulus and trail-to-trial variability can only be loosely interpreted as 'expected' and 'unexpected' uncertainty, we want to compare the effects of ACh and NA on the weighting with the effects hypothesized in the literature.

It is assumed that NA increases in more volatile environments and enhances bottom-up processes \citep{hasselmo1997noradrenergic, yon2021precision}. In line with this idea, NA blockade impairs cognitive flexibility \citep{ridley1981new, janitzky2015optogenetic}. In recent work by \cite{lawson2021computational}, it has been shown that humans receiving propranolol (blocking NA) rely more strongly on their expectations and are slower to update these predictions despite new sensory evidence \citep{yon2021precision}. A main target for noradrenergic inputs is SOM neurons whose activity increases in the presence of NA \citep[reviewed in, e.g.,][]{urban2016somatostatin, hattori2017functions, swanson2019hiring}. In our model, activating SOM neurons does not enhance sensory bottom-up input. In a volatile environment, that is, a sensory-driven regime, the system takes into account predictions slightly more than without SOM modulation (Figs. \ref{fig:Fig_4}A and \ref{fig:Fig_4_S1}). 

However, we note that in our simulations, we assumed that neuromodulators act globally, that is, on the interneurons in both the lower and the higher PE circuit. While this agrees with the view that neuromodulators can control network states globally, there is also evidence that they can have a more local, finely adjusted impact on neural circuits \citep{nadim2014neuromodulation}. In our model, increasing SOM activity only in the lower-order circuit shows a slight enhancement of the sensory weight (Fig. \ref{fig:Fig_4_S2}), that is, the bottom-up inputs. This suggests that whether a neuromodulator biases the network toward feedforward bottom-up or feedback top-down inputs depends on its spatial and temporal scale of influence.

Similarly to NA, ACh has also been shown to enhance bottom-up, feedforward inputs \citep[reviewed in, e.g.,][]{yu2005uncertainty, marshall2016pharmacological}. For instance, subjects relied more strongly on prior beliefs when given cholinergic receptor antagonists \citep{marshall2016pharmacological}. A major target for cholinergic inputs is VIP neurons whose activity increases in the presence of ACh \citep[reviewed in, e.g.,][]{wester2014behavioral, hattori2017functions, swanson2019hiring}. In our model, activating VIP neurons globally only enhances bottom-up input in stable environments for noisy stimuli. However, increasing VIP activity only in the higher-order PE circuit generally enhances sensory bottom-up inputs (Fig. \ref{fig:Fig_4_S2}).


\subsection*{Limitations \& future steps}
%
As for any computational model, we brush over several biological details to keep the model simple and interpretable. However, those details, while beyond the scope of this study, may be well investigated in future work. For instance, neuromodulatory systems have been suggested to gate plasticity \citep{pawlak2010timing}. In a recent study by \cite{jordan2023locus}, locus coeruleus (LC) axon activity is shown to correlate with the magnitude of unsigned visuomotor prediction errors. The authors hypothesize that LC output modulates the learning rate at which the internal model evolves \citep{jordan2023locus}. In our model, we do not consider precision-weighted PEs \citep[but see][]{wilmes2023uncertainty, granier2023precision}. Hence, a sensible extension to our work would be to adjust the weights from PE neurons onto the memory neurons by a function of the stimulus and prediction uncertainty, respectively. This would allow us to compare our results more closely to work showing that ACh and NA can adjust the rate at which new sensory evidence is incorporated when environments change \citep{marshall2016pharmacological, bruckner2022understanding}.

Our model suggests \textit{one} potential neuronal circuit mechanism for the estimation of sensory inputs and predictions. However, in the light of evidence showing that the integration of feedforward and feedback inputs is species- and modality-dependent, it is conceivable that a plethora of neural mechanisms are used in neural circuits. For multisensory integration, \cite{wong2023computational} showed that in Drosophila larva the chosen cue-combination strategy varies depending on the type of sensory information available. Also, humans put typically more weight on visual than auditory cues \citep{battaglia2003bayesian, alais2004ventriloquist}, but trust vestibular information more than visual information about head direction \citep{butler2010bayesian}, a finding also observed for monkeys \citep{fetsch2009dynamic}. Moreover, \cite{summerfield2011perceptual} showed that humans diverge from an optimal Bayesian strategy in very volatile environments and act according to their experience in the last trial. It has been suggested that the brain may use different strategies to combine signals depending on the task demands \citep{o2012can}. While these behavioral results cannot speak to the underlying circuit mechanisms, it is conceivable that neural implementations for the integration of feedforward and feedback inputs may also vary.

Furthermore, while we provide a neuronal circuit model for estimating the mean and variance of both sensory signals and predictions, we do not explicitly model the weighting of inputs. A respective neural circuit model would require nested inhibitory interneurons providing divisive inhibition. How this subnetwork interacts with the PE circuits, and how the presence of neuromodulators acting on the interneurons directly involved in the weighting, impacts our findings is subject to future work. 


% Relation to other work that came before and closing remarks
\subsection*{Relation to other work \& conclusions}
%
Many normative models have been proposed for state estimation and prediction under uncertainty \citep{soltani2019adaptive}, ranging from the classical Kalman filter to more recent models like \textit{Bayes Factor Surprise} \citep{liakoni2021learning}. For instance, the Bayes factor surprise formularizes the trade-off between integrating new observations in an existing belief system and resetting this belief system with novel evidence. The surprise factor captures how much an animal’s current belief deviates from the new observation. 

In recent years, normative models have been squared with biological constraints. For instance, \cite{kutschireiter2023bayesian} showed that a \textit{Bayesian ring attractor} model can encode uncertainty in the amplitude of the network activity and matches the performance of a circular Kalman filter when the recurrent connections are tuned appropriately. In other seminal work, it has been proposed that Bayesian inference in time can be linked to the dynamics of leaky integrate-and-fire neurons with spike-dependent adaptation \citep{deneve2008bayesian}.

Here, we proposed an alternative view in which PE neurons serve as the backbone for estimating both the uncertainty of the feedforward sensory stimuli arising from the external world and the feedback signals carrying predictions about the same feedforward inputs our brains receive. Our work is an important step toward a better understanding of the brain’s ability to integrate these unreliable feedforward and feedback signals that often do not match perfectly. 


\section*{Models and methods}
%
\subsection*{Network model}
The mean-field network model consists of a \textit{lower} and \textit{higher} PE circuit (Fig. \ref{fig:Fig_1}C-D). Each PE circuit contains an excitatory nPE neuron and pPE neuron ($\mathrm{N}_\mathrm{nPE} = \mathrm{N}_\mathrm{pPE} = 1$), as well as inhibitory neurons. The inhibitory neurons comprise PV, SOM and VIP neurons ($\mathrm{N}_\mathrm{SOM} = \mathrm{N}_\mathrm{VIP} = 1$, $\mathrm{N}_\mathrm{PV} = 2$), further explained in \cite{hertag2022prediction}. In addition to the core PE circuit, each subnetwork also includes one memory neuron $M$ and one variance neuron $V$. 

The excitatory neurons in the PE circuit are simulated as two coupled point compartments, representing the soma and the dendrites of elongated pyramidal cells. All other neurons are modeled as point neurons. The activities of all neurons are represented by a set of differential equations describing the network dynamics. 

The dynamics of the neurons in the lower and higher PE circuits ($\underline{r}_\mathrm{PE}^\mathrm{low}$ and $\underline{r}_\mathrm{PE}^\mathrm{high}$) are given by
%
\begin{align}
\underline{r}_\mathrm{PE}^\mathrm{low} = & \left[ \underline{h}_\mathrm{PE}^\mathrm{low} \right]_+ \nonumber\\
%
\underline{r}_\mathrm{PE}^\mathrm{high} = & \left[ \underline{h}_\mathrm{PE}^\mathrm{high} \right]_+ 
\end{align}
%
with
%
\begin{align}
T_{c} \cdot \underline{\dot{h}}_\mathrm{PE}^\mathrm{low} =& -\underline{h}_\mathrm{PE}^\mathrm{low} + W_\mathrm{PE\leftarrow PE} \cdot \underline{r}_\mathrm{PE}^\mathrm{low} + \underline{w}_\mathrm{PE\leftarrow M} \cdot r_\mathrm{M}^\mathrm{low} + \underline{w}_\mathrm{PE\leftarrow FF} \cdot s + \underline{I}_\mathrm{PE} \nonumber\\
%
T_{c} \cdot \underline{\dot{h}}_\mathrm{PE}^\mathrm{high} =& -\underline{h}_\mathrm{PE}^\mathrm{high} + W_\mathrm{PE\leftarrow PE} \cdot \underline{r}_\mathrm{PE}^\mathrm{high} + \underline{w}_\mathrm{PE\leftarrow M} \cdot r_\mathrm{M}^\mathrm{high} + \underline{w}_\mathrm{PE\leftarrow FF} \cdot r_\mathrm{M}^\mathrm{low} + \underline{I}_\mathrm{PE}.
%
\end{align}
%
We follow the notation that column and row vectors are indicated by letters with an underscore $\underline{\bullet}$, matrices are denoted by capital letters, and scalars are given by small letters without an underscore. Furthermore, a time derivative (e.g., $\frac{dx}{dt}$) is denoted by a dot above the letter (e.g., $\dot{x}$). The rate vector $\underline{r}_\mathrm{PE}^\mathrm{loc} = \left[r_\mathrm{nE}^\mathrm{loc},\ r_\mathrm{pE}^\mathrm{loc},\ r_\mathrm{nD}^\mathrm{loc},\ r_\mathrm{pD}^\mathrm{loc},\ r_\mathrm{PV_1}^\mathrm{loc}, r_\mathrm{PV_2}^\mathrm{loc},\ r_\mathrm{SOM}^\mathrm{loc}, r_\mathrm{VIP}^\mathrm{loc} \right]$  with $\mathrm{loc} \in [\mathrm{low}, \mathrm{high}]$ contains the activities of all neurons or compartments in the PE circuit (soma of nPE/pPE neurons: nE/pE, dendrites of nPE/pPE neurons: nD/pD). The network receives time-dependent stimuli $s$ and neuron/compartment-specific external background input $\underline{I}_\mathrm{PE}$. The connection strengths between the \textit{pre}-synaptic population and the neurons of the PE circuit are denoted by $W_\mathrm{PE\leftarrow pre}$ (if $r$ is a vector) or $\underline{w}_{\mathrm{PE} \leftarrow pre}$ (if $r$ is a scalar). The activities of the neurons evolve with time constants summarized in $T_{c}$.

The activities of the lower and higher memory (M) neuron evolve according to a perfect integrator. The M neurons receive synapses from both nPE and pPE neurons of the same subnetwork,
%
\begin{align}
\dot{r}_\mathrm{M}^\mathrm{low} =& \underline{w}_\mathrm{M \leftarrow PE}^\mathrm{low} \cdot \underline{r}_\mathrm{PE}^\mathrm{low} = w_\mathrm{M \leftarrow pPE}^\mathrm{low} \cdot r_\mathrm{pPE}^\mathrm{low} - w_\mathrm{M \leftarrow nPE}^\mathrm{low} \cdot r_\mathrm{nPE}^\mathrm{low} \nonumber \\
%
\dot{r}_\mathrm{M}^\mathrm{high} =& \underline{w}_\mathrm{M \leftarrow PE} ^\mathrm{high} \cdot \underline{r}_\mathrm{PE}^\mathrm{high} = w_\mathrm{M \leftarrow pPE}^\mathrm{high}  \cdot r_\mathrm{pPE}^\mathrm{high} - w_\mathrm{M \leftarrow nPE}^\mathrm{high}  \cdot r_\mathrm{nPE}^\mathrm{high} .
\end{align}
%

The activities of the lower and higher V neuron evolve according to a leaky integrator with quadratic activation function. The variance neurons receive synapses from both nPE and pPE neurons of the same subnetwork,
%
\begin{align}
\tau_\mathrm{V}^\mathrm{low} \cdot \dot{r}_\mathrm{V}^\mathrm{low} =& - r_\mathrm{V}^\mathrm{low} + \left( \underline{w}_\mathrm{V \leftarrow PE} \cdot \underline{r}_\mathrm{PE}^\mathrm{low}\right)^2 = - r_\mathrm{V}^\mathrm{low} + \left( w_\mathrm{V \leftarrow pPE} \cdot r_\mathrm{pPE}^\mathrm{low}\ + w_\mathrm{V \leftarrow nPE} \cdot r_\mathrm{nPE}^\mathrm{low}\right)^2 \nonumber\\
%
\tau_\mathrm{V}^\mathrm{high} \cdot \dot{r}_\mathrm{V}^\mathrm{high} =& - r_\mathrm{V}^\mathrm{high} + \left( \underline{w}_\mathrm{V \leftarrow PE} \cdot \underline{r}_\mathrm{PE}^\mathrm{high}\right)^2 = - r_\mathrm{V}^\mathrm{high} + \left( w_\mathrm{V \leftarrow pPE} \cdot r_\mathrm{pPE}^\mathrm{high}\ + w_\mathrm{V \leftarrow nPE} \cdot r_\mathrm{nPE}^\mathrm{high}\right)^2.
\end{align} 
%
All values for neuron and network parameters, details on the model equations for the mean-field and the population network, as well as supporting analyses can be found in the supplementary material.

\subsection*{Weighting of sensory inputs and predictions}
%
We arithmetically calculated the weighted output of sensory inputs and predictions, $r_\mathrm{out}$, based on ideas of Bayesian multisensory integration \citep[see, e.g.][]{pouget2013probabilistic},
%
\begin{align}
r_\mathrm{out} = \alpha \cdot s + (1-\alpha) \cdot r_\mathrm{M}^\mathrm{low},
\end{align}
%
where $\alpha$ denotes the sensory weight (that is, the reliability of the sensory input) and is given by 
%
\begin{align}
\alpha &= \left( 1 + \frac{r_\mathrm{V}^\mathrm{low}}{r_\mathrm{V}^\mathrm{high}} \right)^{-1}.
\end{align}


\subsection*{Inputs}
%
The network receives feedforward stimuli $s$ that may vary between trials. To account for noise, each stimulus is composed of N$_\mathrm{in}$ constant values drawn from a normal distribution with mean $\mu_\mathrm{in}$ and standard deviation $\sigma_\mathrm{in}$, and are presented for N$_\mathrm{step}$ consecutive time steps. To account for changes in the environment, $\mu_\mathrm{in}$ is drawn from a uniform distribution $U(a,b)$ with mean $\mu_\mathrm{trial} = \frac{a+b}{2}$ and standard deviation $\sigma_\mathrm{trial} = \frac{b-a}{\sqrt{12}}$. The parameterization of both distributions varies across the experiments. All stimulus/input parameters can be found in the supplementary material.

\subsection*{Simulations}
%
All simulations were performed in customized Python code written by LH. Source code to reproduce the simulations, analyses, and figures will be available after publication at \url{https://github.com/lhertaeg/weighted_sensory_prediction}. Differential equations were numerically integrated using a 2\textsuperscript{nd}-order Runge-Kutta method. Neurons were initialized with $r=0/s$. Further details and values for simulation parameters can be found in the supplementary material.


\section*{Acknowledgments}
%
We thank In\^es C. Guerreiro for comments on earlier versions of this manuscript. This work was supported by Deutsche Forschungsgemeinschaft (DFG) Grant 460088091, Biotechnology and Biological Sciences Research Council (BBSRC) Grants BB/N013956/1 and BB/N019008/1, Wellcome Trust Grant 200790/Z/16/Z, Simons Foundation Grant 564408, and Engineering and Physical Sciences Research Council (EPSRC) Grant EP/R035806/1.


%\bibliographystyle{naturemag} %{plainnat}
\bibliographystyle{plainnat}
\bibliography{References_HertaegWilmesClopath_2023}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\clearpage
\appendix
\tableofcontents

\setcounter{figure}{-1}
\renewcommand{\thefigure}{S\arabic{figure}}

\section{Detailed Methods}
%
In the following, we describe in more detail the equations for the dynamics of the neurons in the prediction-error circuit, as well as the memory and variance neurons. We then provide the connectivity of the network and the inputs to the neurons for both the mean-field and multi-cell population model. Finally, to ensure reproducibility, we summarize all simulation parameters used for the results shown in the figures.  

\subsection{Network model}
%
The network model consists of a \textit{lower} and \textit{higher} mean-field PE circuit (Fig. \ref{fig:Fig_1}). Each PE circuit contains an excitatory nPE neuron and pPE neuron ($\mathrm{N}_\mathrm{nPE} = \mathrm{N}_\mathrm{pPE} = 1$), as well as inhibitory neurons. The inhibitory neurons comprise PV, SOM and VIP neurons ($\mathrm{N}_\mathrm{SOM} = \mathrm{N}_\mathrm{VIP} = 1$, $\mathrm{N}_\mathrm{PV} = 2$). As has been shown in \cite{hertag2022prediction}, we need two soma-targeting interneurons, one receiving the sensory input and one receiving the prediction, to obtain a perfect nPE and pPE neuron in the same recurrent network. We, therefore, used two PV neurons (as suggested in the original paper). In addition to the core PE circuit, each subnetwork also includes one memory neuron $M$ and one variance neuron $V$. 

In Figure \ref{fig:Fig_2} and the corresponding supporting figures, only the lower subnetwork is simulated. In Fig. \ref{fig:Fig_2_S2}, we replaced this lower mean-field PE circuit with a heterogeneous multi-cell population model containing $200$ neurons ($\mathrm{N}_\mathrm{SOM} = \mathrm{N}_\mathrm{VIP} = \mathrm{N}_\mathrm{PV} = 20$, $140$ excitatory neurons). In Fig. \ref{fig:Fig_2_S3}, the lower PE circuit comprises $1000$ copies of the mean-field network to account for selectivity.

In the following, we describe the dynamics of the neurons/compartments in the mean-field network. The equations for the population PE circuit (Fig. \ref{fig:Fig_2_S2}) are directly deduced from the mean-field equations and can also be found in \citep{hertag2022prediction}.

\subsubsection{Prediction-error network model}
%
Each excitatory pyramidal cell (that is, nPE or pPE neuron) is divided into two coupled compartments, representing the soma and the dendrites, respectively. The dynamics of the firing rates of the somatic compartments~$r_{\mathrm{nE}}$ (nPE neuron) and~$r_{\mathrm{pE}}$ (pPE neuron) obey \citep{wilson1972excitatory}
%
\begin{align}
r_\mathrm{nE} = [h_\mathrm{nE}]_+ \ \mbox{ with }\ \tau_E\ \frac{dh_\mathrm{nE}}{dt} &= - h_\mathrm{nE} + w_\mathrm{nE\leftarrow nD}\cdot  r_\mathrm{nD}  -  w_\mathrm{nE\leftarrow PV_1}\cdot r_\mathrm{PV_1}  -  w_\mathrm{nE\leftarrow PV_2}\cdot r_\mathrm{PV_2} + I_\mathrm{nE}, \nonumber\\
r_\mathrm{pE} = [h_\mathrm{pE}]_+ \ \mbox{ with }\ \tau_E\ \frac{dh_\mathrm{pE}}{dt} &= - h_\mathrm{pE} + w_\mathrm{pE\leftarrow pD}\cdot  r_\mathrm{pD}  -  w_\mathrm{pE\leftarrow PV_1}\cdot r_\mathrm{PV_1}  -  w_\mathrm{pE\leftarrow PV_2}\cdot r_\mathrm{PV_2} + I_\mathrm{pE}
\end{align}
%
where $\tau_\mathrm{E}$ denotes the excitatory rate time constant ($\tau_\mathrm{E}$=60 ms), the weights $w_{\mathrm{nE\leftarrow nD}}$ and $w_{\mathrm{pE\leftarrow pD}}$ describe the connection strength between the dendritic compartment and the soma of the same neuron, and $w_{\mathrm{nE\leftarrow PV_1}}$, $w_{\mathrm{nE\leftarrow PV_2}}$, $w_{\mathrm{pE\leftarrow PV_1}}$ and $w_{\mathrm{pE\leftarrow PV_2}}$ denote the strength of somatic inhibition from PV neurons. The overall input $I_\mathrm{nE}$ and $I_\mathrm{pE}$ comprise the external background and feedforward inputs (see ``Inputs" below). Firing rates are rectified to ensure positivity ($[\bullet]_+$). 

The dynamics of the activity of the dendritic compartments~$r_\mathrm{nD}$ (nPE neuron) and~$r_\mathrm{pD}$ (pPE neuron) obey \citep{wilson1972excitatory}
%
\begin{align}
r_\mathrm{nD} = [h_\mathrm{nD}]_+ \ \mbox{ with }\ \tau_E\ \frac{dh_\mathrm{nD}}{dt} =& - h_\mathrm{nD} +  w_\mathrm{nD\leftarrow nE}\cdot r_\mathrm{nE} +  w_\mathrm{nD\leftarrow pE}\cdot r_\mathrm{pE} + w_\mathrm{nD\leftarrow M}\cdot  r_\mathrm{M} \nonumber\\
& - w_\mathrm{nD\leftarrow SOM}\cdot r_\mathrm{SOM} + I_\mathrm{nD}, \nonumber\\
r_\mathrm{pD} = [h_\mathrm{pD}]_+ \ \mbox{ with }\ \tau_E\ \frac{dh_\mathrm{pD}}{dt} =& - h_\mathrm{pD} +  w_\mathrm{pD\leftarrow nE}\cdot r_\mathrm{pE} +  w_\mathrm{pD\leftarrow pE}\cdot r_\mathrm{pE}+ w_\mathrm{pD\leftarrow M}\cdot  r_\mathrm{M}  \nonumber\\
&- w_\mathrm{pD\leftarrow SOM}\cdot r_\mathrm{SOM} + I_\mathrm{pD},
\end{align}
%
where the weights $w_{\mathrm{nD\leftarrow nE}}$, $w_{\mathrm{nD\leftarrow pE}}$, $w_{\mathrm{pD\leftarrow nE}}$ and $w_{\mathrm{pD\leftarrow pE}}$ denote the recurrent excitatory connections between PCs. $w_{\mathrm{nD\leftarrow SOM}}$ and $w_{\mathrm{pD\leftarrow SOM}}$ represent the strength of dendritic inhibition from the SOM neuron. $w_{\mathrm{nD\leftarrow M}}$ and $w_{\mathrm{pD\leftarrow M}}$ denote the strength of connection between the memory neuron and the dendrites. The overall inputs $I_\mathrm{nD}$ and $I_\mathrm{pD}$ comprise fixed, external background inputs (see ``Inputs" below). We assume that any excess of inhibition in a dendrite does not affect the soma, that is, the dendritic compartment is rectified at zero. 

Similarly, the firing rate dynamics of each interneuron is modeled by a rectified, linear differential equation,
%
\begin{align}
\label{eq:RateEqINs}
r_\mathrm{X} = [h_\mathrm{X}]_+ \ \mbox{ with }\ \tau_I\ \frac{dh_\mathrm{X}}{dt} =& -h_\mathrm{X} + I_{\mathrm{X}} + w_\mathrm{X\leftarrow nE}\cdot r_\mathrm{nE} + w_\mathrm{X\leftarrow pE}\cdot r_\mathrm{pE}  +  w_\mathrm{X\leftarrow M}\cdot  r_\mathrm{M} - w_\mathrm{X\leftarrow PV_1}\cdot r_\mathrm{PV_1}  \nonumber\\
&- w_\mathrm{X\leftarrow PV_2}\cdot r_\mathrm{PV_2}  - w_\mathrm{X\leftarrow SOM}\cdot r_\mathrm{SOM} -  w_\mathrm{X\leftarrow VIP}\cdot r_\mathrm{VIP},
\end{align}
%
where $r_\mathrm{X}$ denotes the firing rate of interneuron type $X$, and the weight $w_\mathrm{X\leftarrow Y}$ denotes the strength of connection between the presynaptic neuron $Y$ and the postsynaptic neuron $X$ ($X \in \lbrace \mathrm{PV_1}, \mathrm{PV_2}, \mathrm{SOM}, \mathrm{VIP}\rbrace$, $Y\in \lbrace \mathrm{nPE}, \mathrm{pPE}, \mathrm{PV_1}, \mathrm{PV_2}, \mathrm{SOM}, \mathrm{VIP}, \mathrm{M}\rbrace$). The rate time constant $\tau_I$ was chosen to resemble a fast GABA\textsubscript{A} time constant, and set to 2 ms for all interneuron types included. The overall input $I_\mathrm{X}$ comprises fixed, external background inputs and feedforward sensory inputs (see ``Inputs" below).

\subsubsection{Memory and variance neuron}
%
In addition to the core PE circuit, we simulate a memory neuron \textit{M} and a variance neuron \textit{V}. The memory neuron is modeled as a perfect integrator, receiving synapses from both the nPE and pPE neuron,
%
\begin{align}
\tau_E \cdot \frac{dr_\mathrm{M}}{dt} = w_\mathrm{M\leftarrow pE} \cdot r_\mathrm{pE} - w_\mathrm{M\leftarrow nE} \cdot r_\mathrm{nE}.
\end{align}
%
$w_\mathrm{M\leftarrow pE}$ denotes the connection strength between the pPE neuron and the memory neuron, and  $w_\mathrm{M\leftarrow nE}$ denotes the connection strength between the nPE neuron and the memory neuron. The time constant $\tau_E = 60$ ms.

The dynamics of the variance neuron obeys a non-linear differential equation with leak term,
%
\begin{align}
\tau_V \cdot \frac{dr_\mathrm{V}}{dt} = -r_\mathrm{V} + (w_\mathrm{V\leftarrow pE} \cdot r_\mathrm{pE} + w_\mathrm{V\leftarrow nE} \cdot r_\mathrm{nE})^2.
\end{align}
%
The weight $w_\mathrm{V\leftarrow pE}$ represents the connection strength between the pPE neuron and the variance neuron, while  $w_\mathrm{V\leftarrow nE}$ denotes the connection strength between the nPE neuron and the variance neuron. To ensure that the V neuron encodes the variance, we chose a quadratic activation function. In Fig. \ref{fig:Fig_3_S2}, we used a linear activation function to investigate the impact of the input-output transfer function on the weighting of sensory inputs and predictions. The time constant $\tau_V$ was $5\,s$ in the mean-field model, $2\,s$ in the heterogeneous multi-cell population model (Fig. \ref{fig:Fig_2_S2}), and $0.5\,s$ in the network model with selectivity (Fig. \ref{fig:Fig_2_S3}).

\subsubsection{Weighted output}
%
The weighted output $r_\mathrm{out}(t) $ is a linear combination of the current sensory input $s(t)$ and the activity of the memory neuron, $r_\mathrm{M}(t)$, inspired by Bayesian multisensory integration \citep[see, e.g.][]{pouget2013probabilistic},
%
\begin{align}
r_\mathrm{out}(t) = \alpha \cdot s(t) + (1-\alpha) \cdot r_\mathrm{M}(t).
\end{align}
%
How strongly either the sensory input or the prediction thereof contributes to the output is denoted by the sensory weight $\alpha$,
%
\begin{align}
\alpha &= \frac{r_\mathrm{V_{lower}}^{-1}}{r_\mathrm{V_{lower}}^{-1} + r_\mathrm{V_{higher}}^{-1}}\nonumber\\
& = \left( 1 + \frac{r_\mathrm{V_{lower}}}{r_\mathrm{V_{higher}}} \right)^{-1}.
\end{align}

\subsection{Connectivity}
%
%
\subsubsection{Connections between neurons of the PE circuit}
%
The connectivity between neurons of the PE circuit, both for the mean-field and the population network, were taken from \citep{hertag2022prediction}. We considered three mean-field networks (see Table \ref{tab:wXM}) that differed in terms of the inputs (feedforward vs. feedback) onto the SOM and VIP neurons, and, hence, in their connectivity that established an E/I balance in the excitatory neurons.

\subsubsection{Connections between the PE circuit and the M neuron}
%
While the nPE neurons inhibit the M neuron, the pPE neurons excite it. To ensure that the activities of the memory neurons represent the mean of the sensory stimuli in the lower PE circuit and the mean of the prediction in the higher subnetwork, respectively, the net effect of nPE and pPE neurons must cancel in the steady state (see Analysis in \ref{sec:gain_impact}). Hence, the weights need to account for the neurons' potentially different gain factors ($g_\mathrm{nPE}$ and $g_\mathrm{pPE}$) and the neuron numbers ($N_\mathrm{nPE}$ and $N_\mathrm{pPE}$):
%
\begin{align*}
w_\mathrm{M\leftarrow nE}\  &=\ \frac{-\lambda^\mathrm{loc}}{g_\mathrm{nPE} \cdot N_\mathrm{nPE}} \nonumber\\
w_\mathrm{M\leftarrow pE}\  &=\ \frac{\lambda^\mathrm{loc}}{g_\mathrm{pPE} \cdot N_\mathrm{pPE}}
\end{align*}
%
where $\lambda^\mathrm{loc}$ denotes a weight for the lower or higher-order PE circuit, $\mathrm{loc}\in\lbrace\mathrm{lower}, \mathrm{higher}\rbrace$. In the lower PE circuit,  $\lambda^\mathrm{lower}=3\cdot 10^{-3}$ for the mean-field model in Fig. \ref{fig:Fig_2} and $\lambda^\mathrm{lower}=4.5\cdot 10^{-2}$ for Figs. \ref{fig:Fig_3}-\ref{fig:Fig_5}. In the higher PE circuit, $\lambda^\mathrm{higher} = 7\cdot 10^{-4}$.

For the mean-field networks ($ N_\mathrm{nPE} = N_\mathrm{pPE} = 1 $), the gain factors $g_\mathrm{nPE}$ and $g_\mathrm{pPE}$ are given in Table~\ref{tab:gain_factors_MFN}. For the population network, the gain factors for all PE neurons are shown in Fig. \ref{fig:Fig_gains}. 
%
\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c| }
\hline
 & \textbf{MFN 1} & \textbf{MFN 2} & \textbf{MFN 3}  \\
\textbf{Network} & FF $\rightarrow$ SOM  & FB $\rightarrow$ SOM  & FF $\rightarrow$ SOM  \\
 & FB $\rightarrow$ VIP  & FF $\rightarrow$ VIP  & FF $\rightarrow$ VIP  \\
\hline
\hline
nPE & 1 & 1.7 & 2.5\\
pPE & 1 & 1.7 & 2.5 \\
\hline
\end{tabular}
\caption{\footnotesize{Gain factors for nPE and pPE neurons in three different mean-field networks (MFN). Each MFN differs with respect to the inputs onto SOM and VIP neurons. The interneurons either receive the feedforward (FF) or feedback (FB) input. All numbers are rounded to the first digit.}}
\label{tab:gain_factors_MFN}
\end{table}
%
%
\begin{figure}[h!]
	\centering
    \includegraphics{../results/figures/final/Figure_gains}
\caption{\footnotesize{\bf Gain factors of nPE and pPE neurons in the multi-cell population model.\newline}
{The logarithm of the gain factors of nPE (top) and pPE (bottom) neurons in the multi-cell population model from \citep{hertag2022prediction}. The network contains $67$ nPE neurons and $66$ pPE neurons. The remaining excitatory neurons were not classified as PE neurons and were not connected to the $M$ neuron.}}
\label{fig:Fig_gains}
\end{figure}
%
%

The memory neuron \textit{M} connects to the post-synaptic neurons \textit{X} in the PE circuit with the connection strength $w_\mathrm{X\leftarrow M} = 1$, if a connection exists, $w_\mathrm{X\leftarrow M} = 0$ otherwise. In all mean-field networks and the population network, the dendrites of nPE and pPE neurons and one of the two (populations of) PV neurons receive connections from the memory neuron. Furthermore, we assume that the \textit{M} neuron does not excite the soma of PCs. Whether the SOM or VIP neurons are the target of the feedback projections depend on the specific mean-field network (see Table \ref{tab:wXM}). In the multi-cell population model, 30\% of the SOM neurons and 70\% of the VIP neurons receive input from the memory neuron.
%
\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|  }
\hline
 & \textbf{MFN 1} & \textbf{MFN 2} & \textbf{MFN 3}  \\
\textbf{Network} & FF $\rightarrow$ SOM  & FB $\rightarrow$ SOM  & FF $\rightarrow$ SOM  \\
 & FB $\rightarrow$ VIP  & FF $\rightarrow$ VIP  & FF $\rightarrow$ VIP  \\
\hline
\hline
SOM & 0 & 1 & 0\\
VIP & 1 & 0 & 0 \\
\hline
\end{tabular}
\caption{\footnotesize{$w_\mathrm{X\leftarrow M}$ for the post-synaptic SOM and VIP neurons in all three mean-field networks considered.}}
\label{tab:wXM}
\end{table}
%

\subsubsection{Connections between the PE circuit and the V neuron}
%
Both nPE and pPE neurons excite the \textit{V} neuron. To ensure that the activity of the V neuron represents the variance of the input (see Analysis in \ref{sec:gain_impact}), the weights must account for differences in the gains ($g_\mathrm{nPE}$ and $g_\mathrm{pPE}$, see Table \ref{tab:gain_factors_MFN} and Fig. \ref{fig:Fig_gains}) and numbers ($N_\mathrm{nPE}$ and $N_\mathrm{pPE}$) of the PE neurons,
%
\begin{align*}
w_\mathrm{V\leftarrow nE}\  &=\ \frac{\theta}{g_\mathrm{nPE} \cdot N_\mathrm{nPE}} \nonumber\\
w_\mathrm{V\leftarrow pE}\  &=\ \frac{\theta}{g_\mathrm{pPE} \cdot N_\mathrm{pPE}}.
\end{align*}
%
The factor $\theta$ denotes the unscaled weight, and can be chosen to compensate for potential  deviations from the definition of the variance as a result of a quadratic activation function. In the mean-field network, $\theta = 1$ because nPE and pPE neuron activity is mutually exclusive, and, hence, the cross-term $\mathrm{nPE}\cdot\mathrm{pPE}$ would be zero (under the assumption that they have a negligible baseline activity). This is also true for the multi-cell population model. Each PE neuron receives the same feedforward stimulus and contributes only a small (scaled) fraction to the overall PE (adding up to the same PE used in the mean-field network).

However, in Supp Fig. \ref{fig:Fig_2_S3}, each mean-field network receives a different stimulus $s_i$ drawn from a distribution at time $t$. While the stimuli below the mean of the distribution activate the nPE neurons (each located in a different mean-field network), the stimuli above the mean of the distribution activate the pPE neurons (each located in a different mean-field network). Because the V neuron first sums all the contributions from the PE neurons before applying the non-linearity, its steady state activity is similar to the squared sum of the \textit{averaged} nPE and \textit{averaged} pPE neuron activity. In this case, $\theta$ must be chosen such that deviations from the true variance can be mitigated or fully corrected. The true $\theta$ depends on the distribution at hand. In our simulations, we used a uniform distribution $U(a,b)$, in which case $\theta$ can be derived from
%
\begin{align}
\left( \sum_{i} r_\mathrm{nE,i} + \sum_{i} r_\mathrm{pE,i}\right)^2 \overset{(i)}{=} & \left( \frac{\theta}{N}  \sum_{s_i \geq \mu}^{N/2} (s_i - \mu) + \frac{\theta}{N}  \sum_{s_i \leq \mu}^{N/2} (\mu - s_i )\right)^2 \nonumber \\
%
=&\ \frac{\theta^2}{4} \left( \frac{2}{N} \sum_{s_i \geq \mu}^{N/2} s_i - \frac{2}{N} \sum_{s_i \leq \mu}^{N/2} s_i\right)^2\nonumber\\
%
\overset{(ii)}{=}&\ \frac{\theta^2}{4} \left( \frac{b+\mu}{2} - \frac{\mu + a}{2} \right)^2 = \frac{(b-a)^2}{16} \cdot \theta^2,
\label{eq:theta}
\end{align}
%
where we assumed that (i) the number of nPE and pPE neurons is equal, and (ii) this number goes to infinity. Comparing eq. (\ref{eq:theta}) with the equation for the variance of a uniform distribution, $\frac{(b-a)^2}{12}$, we get $\theta = \frac{2}{\sqrt{3}}$.

\subsection{Inputs}
%
Each neuron (type) receives an overall input $I_i$,
%
\begin{align*}
I_i = I_i^{BL} + w_i \cdot I_{i}^{FF}
\end{align*}
%
where $I_{i}^{FF}$ denotes a feedforward input and $I_i^{BL}$ represents an external background input that ensures reasonable baseline firing rates in the absence of sensory inputs and predictions thereof. In the case of the mean-field network, these inputs were set such that the baseline firing rates are $r_\mathrm{pE}=r_\mathrm{pD}=r_\mathrm{nE}=r_\mathrm{nD}=0\, s^{-1}$ and $r_\mathrm{P} = r_\mathrm{S}=r_\mathrm{V}=4\, s^{-1}$. In the case of the population network, we set the external inputs of all neuron types to $5\, s^{-1}$, while the background inputs to the dendrites were computed such that the dendrites are inactive during baseline.

The feedforward input is either the direct sensory input $s$ for the lower PE circuit, or the activity of the M neuron, $r_\mathrm{M}$, for the higher PE circuit. In general, for the three mean-field networks tested, we chose $w_i = 1 - w_\mathrm{X\leftarrow M}$  (see Table \ref{tab:wXM}). In the population network, 70\% of the SOM neurons and 30\% of the VIP neurons receive the feedforward input.


\subsection{Simulations}
%
All simulations were performed in customized Python code written by LH. Source code to reproduce the simulations, analyses and figures will be available after publication at \url{https://github.com/lhertaeg/weighted_sensory_prediction}. Differential equations were numerically integrated using a 2\textsuperscript{nd}-order Runge-Kutta method. Neurons were initialized with $r=0/s$. 

The qualitative results were fairly robust to the choice of the simulation parameters and are here stated merely to ensure the reproducibility of all figures. However, we note that we made use of PE circuits that had been trained on steady state inputs \citep{hertag2022prediction}. Hence, we must simulate the network long enough to ensure that the PE neurons reach their steady state. Moreover, the lower-level M neuron must evolve faster than the higher-level M neuron as indicated in Fig. \ref{fig:Fig_3_S2}. Finally, the time constant of the V neurons must be of the same magnitude as the trial duration.

In the following, we give all figure-specific parameters not directly visible or mentioned in the figures and captions. Furthermore, to increase readability, we do not include units for the parameters. All units can be deduced from the equations above. We simulated the network in Figures \ref{fig:Fig_1} and \ref{fig:Fig_2} for $10^5$ simulation time steps. In Figure \ref{fig:Fig_2}, we presented $200$ constant values, each $500$ time steps long. In Figure \ref{fig:Fig_3} (including supporting figures) we simulated $100$ trials, while in Figures \ref{fig:Fig_4} to \ref{fig:Fig_5} (including supporting figures), we simulated $200$ trials, each $5000$ time steps long. In a trail, $10$ constant values were drawn from a normal distribution $N(\mu_\mathrm{in}, \sigma^2_\mathrm{in})$, each $500$ time steps long. The stimulus mean was drawn from an uniform distribution, $U(a, b)$, with mean $\mu_\mathrm{in}$ and variance $\sigma^2_\mathrm{trial}$.\newline\\ 
%
\textbf{Figure 1:} Constant prediction (fixed) = $5$, input mean $\in [0,10]$, input standard deviation $0$.\\
%
\textbf{Figure 2:} Inputs drawn from a uniform distribution. B and C: input standard deviation fixed at $4.5$ when mean is varied, input mean fixed at $4.5$ when input variance is varied.\\
%
\textbf{Figure 2 Supplementary Fig. 1:} Inputs drawn from different distributions with mean of $5$ and variance of $4$. Number of repetitions with different seeds: 20.\\
%
\textbf{Figure 2 Supplementary Fig. 2:} Inputs drawn from a uniform distribution with mean of $5$ and variance of $4$. Time step was $0.1$. The connections from the PE neurons to the M or V neuron were altered by a factor $\gamma$ drawn from a normal distribution. If not stated otherwise, the mean of this normal distribution was $1$ and the variance $0$, while the connection probability was $1$. Number of repetitions with different seeds: 10.\\
%
\textbf{Figure 2 Supplementary Fig. 3:} Number of time steps were $4000$. Number of identical mean-field networks: $1000$. \\
%
\textbf{Figure 3:} C: $\sigma^2_\mathrm{in} = 0$ and $U(1,9)$, D:  $\sigma^2_\mathrm{in} = 5$ and $U(5,5)$, F: $\sigma^2_\mathrm{in} = 0$ and $U(a,b)$ was parameterized such that the trail variability was $3$.\\
%
\textbf{Figure 3 Supplementary Fig. 1:}  Switch of input statistics occurs after 50 trails. State 1: stimulus $\in N(\mu_\mathrm{in}, 0)$ with $\mu_\mathrm{in} \in U(5,5)$, State 2:  stimulus $\in N(\mu_\mathrm{in}, 3)$ with $\mu_\mathrm{in} \in U(5,5)$, State 3: stimulus $\in N(\mu_\mathrm{in}, 3)$ with $\mu_\mathrm{in} \in U(0,10)$, State 4: stimulus $\in N(\mu_\mathrm{in}, 0)$ with $\mu_\mathrm{in} \in U(0,10)$.\\
%
\textbf{Figure 3 Supplementary Fig. 2:} $\mu_\mathrm{in} = 5$ and $\sigma^2_\mathrm{trial} \in \lbrace0, 0.75, 1.5, 2.25, 3\rbrace$, $\sigma^2_\mathrm{in} \in \lbrace 3, 2.25, 1.5,0.75, 0\rbrace$. A: scaling factors of $w_\mathrm{M\leftarrow PE}$ were 0.3 and 7.\\
%
\textbf{Figure 4:} $\mu_\mathrm{in} = 5$. A, top: $\sigma^2_\mathrm{in} = 0$ and $\sigma^2_\mathrm{trial} = 1$. A, bottom: $\sigma^2_\mathrm{in} = 1$ and $\sigma^2_\mathrm{trial} = 0$. C: $\sigma^2_\mathrm{in} = 1$ and $\sigma^2_\mathrm{trial} = 1$. D: $\sigma^2_\mathrm{in} = 5$ and $\sigma^2_\mathrm{trial} = 0$. Additional input (perturbation) was either fixed at $0.5$ (A, D) or systematically varied between $-1$ and $1$ (C), and was \textit{on} for the last 50\% of the trials. To estimate changes in baseline and gain of nPE and pPE neurons, we fitted a linear function to the PE neuron activity for the input range $[0, 2.5]$.\\
%
\textbf{Figure 4 Supplementary Fig. 1:} All parameters are from Figure 4 A.\\
%
\textbf{Figure 4 Supplementary Fig. 2:} In the default setting baseline was $0$ and gain of PE neurons was $1$. The results (left) were computed for baselines $\in [0, 3]$, while the results (right) were computed for gains $\in [0.5, 1.5]$. The results are based on the Eqs. (\ref{eq:prediction_gain}), (\ref{eq:variance_gain}), (\ref{eq:condition_baseline_mean_1}) and  (\ref{eq:condition_baseline_variance_2}). \\
%
\textbf{Figure 4 Supplementary Fig. 3:} All parameters are from Figure 4 D.\\
%
\textbf{Figure 5:} A: $\sigma_\mathrm{in} \in \lbrace1,7\rbrace$, $\mu_\mathrm{in} \in U(15, 25)$. B: $\sigma_\mathrm{in} \in [0,8]$, $\mu_\mathrm{in} \in U(15, 25)$, and $\sigma_\mathrm{in} = 5$, $\mu_\mathrm{in} \in U(15, b)$ with $b\in [20, 48]$. C: $\sigma_\mathrm{in} \in \lbrace2,5\rbrace$, $\mu_\mathrm{in} \in U(15, 15)$. D: $\sigma_\mathrm{in} = 0$, $\mu_\mathrm{in} \in U(15, 25)$ or $U(10, 30)$. E: $\sigma_\mathrm{in} = 0$, $\mu_\mathrm{in} \in U(15, 25)$, or $\sigma_\mathrm{in} = 5$, $\mu_\mathrm{in} \in U(15, 15)$,  Time steps per trail increased from $5000$ to $10^4$.\\
%
\textbf{Figure 5 Supplementary Fig. 1:} We used two different uniform distributions $U(15, 25)$ and  $U(25, 35)$, and introduced scalar variability so that $\sigma_\mathrm{in}$ is a linear function of $\mu_\mathrm{in}$. Specifically, we chose $\sigma_\mathrm{in} = \left[\mu_\mathrm{in} -14\right]_+$.


\section{Supporting analyses}

We first describe a simplified model and show that the M neuron represents the mean, while the V neuron represents the variance of the feedforward input. We then investigate the impact of the gain and baseline of PE neurons on estimating the mean and variance. Furthermore, we use the simplified model to discuss the effect of neuromodulators in our network. Finally, we reveal the connection between the sensory weight and the contraction bias.

\subsection{Activity of M and V neuron in a simplified model}\label{sec:toy}
%
To show that the M neuron encodes the mean, while the V neuron encodes the variance of the feedforward input, we resume a toy model in which the activity of the nPE and pPE neuron is replaced by its ideal output
%
\begin{align}
r_\mathrm{nE} = \left[ r_\mathrm{M} - s_\mathrm{FF}\right]_+ \nonumber \\
r_\mathrm{pE} = \left[ s_\mathrm{FF} - r_\mathrm{M} \right]_+
\end{align}
%
with $s_\mathrm{FF}$ denoting the time-dependent feedforward input. The activity of the M neuron can then be described as
%
\begin{align}
\tau_M \cdot \frac{dr_\mathrm{M}}{dt} = r_\mathrm{pE} - r_\mathrm{nE}
\end{align}
%
If $r_\mathrm{M} \geq s_\mathrm{FF}$, we get
%
\begin{align}
\tau_M \cdot \frac{dr_\mathrm{M}}{dt} = -r_\mathrm{nE} = -r_\mathrm{M} + s_\mathrm{FF}.
\end{align}
%
If $r_\mathrm{M} \leq s_\mathrm{FF}$, we also get
%
\begin{align}
\tau_M \cdot \frac{dr_\mathrm{M}}{dt} = r_\mathrm{pE} = -r_\mathrm{M} + s_\mathrm{FF}.\nonumber
\end{align}
%
Hence, the activity of $r_\mathrm{M}$ is given by
%
\begin{align}
r_\mathrm{M} = \frac{1}{\tau_M} \int\limits_0^t e^{-(t-x)/\tau_M}\cdot s_\mathrm{FF}(x)\ dx
\end{align}  
%
for zero activity at time $t=0$. In the limit of $t\rightarrow \infty$ (steady state), this is the exponential moving average of the feedforward input, $E(s_\mathrm{FF})$.

With the simplified activity of the nPE and pPE neuron, the activity of the V neuron can then be described as
%
\begin{align}
\tau_V \cdot \frac{dr_\mathrm{V}}{dt} = -r_\mathrm{V} + (r_\mathrm{pE} +  r_\mathrm{nE})^2 = -r_\mathrm{V}  + (r_\mathrm{M} -  s_\mathrm{FF})^2,
\end{align}
%
leading to the time-dependent solution
%
\begin{align}
r_\mathrm{V} = \frac{1}{\tau_V} \int\limits_0^t e^{-(t-x)/\tau_V}\cdot \left[r_\mathrm{M}(x) -  s_\mathrm{FF}(x)\right]^2\ dx.
\end{align}  
%
In the limit of $t\rightarrow \infty$, $r_\mathrm{V}$ approaches $E(s_\mathrm{FF} - E(s_\mathrm{FF}))$.


\subsection{Impact of PE neurons' gain on estimating mean and variance}\label{sec:impact_gain}\label{sec:gain_impact}
%
The gains of the PE neurons, if not equal between the nPE and pPE neuron on average, can bias the activity of both the M and V neuron. To show this, we resume the toy model from section \ref{sec:toy}. 
%
\begin{align}
\label{eq:condition_mean_gain_equal}
g_\mathrm{pPE}\ \langle r_\mathrm{nPE}\rangle &= g_\mathrm{nPE}\ \langle r_\mathrm{pPE}\rangle \\
g_\mathrm{pPE}\ \langle \left[ s_\mathrm{FF}-P\right]_+\rangle &= g_\mathrm{nPE}\ \langle \left[ P-s_\mathrm{FF}\right]_+\rangle \nonumber\\
g_\mathrm{pPE} \int\limits_P^{\infty} \left( x-P\right)\ f(x)\ dx &= g_\mathrm{nPE} \int\limits_{-\infty}^P \left( P-x\right)\ f(x)\ dx. \nonumber
\end{align}
%
Here, $P$ denotes the prediction encoded in the M neuron, and $f(x)$ is the distribution of feedforward inputs. In case of a uniform distribution, $f(x) = 1/(b-a)$ for $x\in [a,b]$ and $0$ otherwise, we get
%
\begin{equation}
\label{eq:prediction_gain}
    P=
    \begin{cases}
      \frac{a + b}{2} & \text{if}\ g_\mathrm{nPE} = g_\mathrm{pPE} = g \\
      \frac{g_\mathrm{pPE}\cdot b - g_\mathrm{nPE}\cdot a + \sqrt{g_\mathrm{nPE}\ g_\mathrm{pPE}}\ (a-b)}{g_\mathrm{pPE} - g_\mathrm{nPE}} & \text{otherwise.}
    \end{cases}
\end{equation}
%
Hence, the mean of the feedforward input is overpredicted when $g_\mathrm{nPE} < g_\mathrm{pPE}$. Similarly, the mean of the feedforward input is underpredicted when $g_\mathrm{nPE} > g_\mathrm{pPE}$ (Fig. \ref{fig:Fig_4_S2}).

Likewise, the variance is affected by the gain of the nPE and pPE neuron,
%
\begin{align}
\label{eq:condition_variance_gain}
V &= \langle \left(r_\mathrm{pPE} + r_\mathrm{nPE}\right)^2 \rangle \overset{(i)}{=} \langle r_\mathrm{pPE}^2 \rangle + \langle r_\mathrm{nPE}^2 \rangle \nonumber\\
&=g_\mathrm{pPE}^2\ \langle \left[ s_\mathrm{FF}-P \right]_+^2 \rangle + g_\mathrm{nPE}^2\ \langle \left[ P-s_\mathrm{FF} \right]_+^2 \rangle,
\end{align}
%
where we assume (i) that both the nPE and pPE neuron have a zero baseline activity. In case of a uniform distribution, we get
%
\begin{align}
V &= \frac{g_\mathrm{pPE}^2}{b-a} \int\limits_P^b (x-P)^2\ dx + \frac{g_\mathrm{nPE}^2}{b-a} \int\limits_a^P (P-x)^2\ dx \\
   &= \frac{g_\mathrm{pPE}^2}{3} \cdot \frac{(b-P)^3}{b-a} + \frac{g_\mathrm{nPE}^2}{3} \cdot \frac{(P-a)^3}{b-a}. \nonumber
\end{align}
%
Inserting eqs. (\ref{eq:prediction_gain}) yields
%
\begin{equation}
\label{eq:variance_gain}
    V=
    \begin{cases}
      \frac{(b - a)^2}{12} & \text{if}\ g_\mathrm{nPE} = g_\mathrm{pPE} = 1 \\
      \frac{(b-a)^2}{3\ (g_\mathrm{pPE} - g_\mathrm{nPE})^3} \cdot \left[ g_\mathrm{nPE}^2 \cdot( g_\mathrm{pPE} - \gamma)^3 - g_\mathrm{pPE}^2 \cdot (g_\mathrm{nPE} - \gamma)^3\right] & \text{otherwise.}
    \end{cases}
\end{equation}
%
with $\gamma = \sqrt{g_\mathrm{nPE}\ g_\mathrm{pPE}}$. Hence, the variance of the feedforward input is overpredicted when $g_\mathrm{nPE}>1$ or $g_\mathrm{pPE}>1$. Similarly, the variance of the feedforward input is underpredicted when $g_\mathrm{nPE} < 1$ or $g_\mathrm{pPE} < 1$ (Fig. \ref{fig:Fig_4_S2}).


\subsection{Impact of PE neurons' baseline on estimating mean and variance}\label{sec:impact_baseline} 
%
The baselines of the PE neurons, if not equal between the nPE and pPE neuron on average, can bias the activity of both the M and V neuron. By means of the toy model from section \ref{sec:toy}, we can write
%
\begin{align}
\label{eq:condition_baseline_mean}
\langle r_\mathrm{pPE} \rangle &= \langle r_\mathrm{nPE} \rangle \\
\langle \left[s_\mathrm{FF} - P\right]_+ + p_0\rangle &= \langle \left[P - s_\mathrm{FF}\right]_+ + n_0\rangle \nonumber\\
\int\limits_P^\infty (x - P)\ f(x)\ dx + p_0 \underbrace{\int\limits_a^b f(x)\ dx}_{=1}  &= \int\limits_{-\infty}^P (P - x)\ f(x)\ dx + n_0 \underbrace{\int\limits_a^b f(x)\ dx}_{=1} . \nonumber
\end{align}
%
$n_0$ and $p_0$ denote the baseline activity of the nPE and pPE neuron, respectively.
In case of a uniform distribution (c.f. \ref{sec:impact_gain}), we get 
%
%
\begin{align}
\label{eq:condition_baseline_mean_1}
P = \frac{b+a}{2} + \frac{p_0 - n_0}{b-a}.
\end{align}
%
Thus, the M neuron encodes the true mean of the feedforward input only if $p_0 = n_0$. As a result, the mean is overpredicted if $p_0 > n_0$. Likewise, the mean is underpredicted if $p_0 < n_0$ (see Fig. \ref{fig:Fig_4_S2}).

With non-zero baseline activities, the steady state activity of the V neuron is given by
%
\begin{align}
\label{eq:condition_baseline_variance}
V &= \langle \left( r_\mathrm{pPE} + r_\mathrm{nPE} \right)^2 \rangle \\
&= \langle \left[ s_\mathrm{FF}-P\right]_+^2\rangle + \langle \left[ P-s_\mathrm{FF}\right]_+^2\rangle + (p_0 + n_0)^2 + 2\ (p_0 + n_0)\ \left( \langle \left[ s_\mathrm{FF}-P\right]_+\rangle + \langle \left[ P-s_\mathrm{FF}\right]_+ \rangle\right) \nonumber
\end{align}
%
In case of a uniform distribution $U(a,b)$, this expression yields
%
\begin{align}
\label{eq:condition_baseline_variance_1}
V = \frac{1}{3\ (b-a)} \left[ (b-P)^3 + (P-a)^3\right] + (p_0 + n_0)^2 + \frac{(p_0 + n_0)}{b-a} \left[ (b-P)^2 + (a-P)^2\right].
\end{align}
%
Inserting the expression for P (Eq. \ref{eq:condition_baseline_mean_1}) which is itself a function of the baseline activities, gives
%
\begin{align}
\label{eq:condition_baseline_variance_2}
V =  \frac{(b-a)^2}{12} + \frac{(p_0-n_0)^2}{(b-a)^2} \left( 1 + 2\ \frac{p_0+n_0}{b-a}\right) + (p_0 + n_0) \left( p_0 + n_0 + \frac{b-a}{2}\right).
\end{align}
%
Thus, for the V neuron to encode the variance unbiased, $n_0 = p_0 = 0$. The variance is overpredicted if either $n_0 > 0$ or $p_0 > 0$ (see Fig. \ref{fig:Fig_4_S2}).


\subsection{Modelling the impact of neuromodulators on the sensory weight}
%
We modeled the presence of a neuromodulator by simulating an additive excitatory input onto (groups of) interneurons. These interneurons, in turn, modulate the gain and baseline of PE neurons. As shown in sections \ref{sec:impact_gain} and \ref{sec:impact_baseline}, changes in the input-output transfer function of the PE neurons may bias the variance estimation in the network, and, hence, the sensory weight. Thus, understanding changes in the sensory weight requires an understanding of whether and how different types of interneurons change the PE neurons.

If a neuromodulator only acts on interneurons of the lower-level subnetwork, the sensory weight changes as a consequence of the modulated firing rates of the lower-level and higher-level $V$ neurons. The lower-level $V$ neuron is directly affected by the changes in the lower-level PE neurons and indirectly affected by changes in the $M$ neuron of the same network. The higher-level $V$ neuron is also affected by a neuromodulator acting in the lower-level subnetwork because the lower-level $M$ neuron projects onto the neurons in the higher-level PE circuit. Hence, if the lower-level $M$ neuron represents a biased mean $\mu \pm \delta\mu$, the variance estimation will be biased as well. This can be seen directly from the definition of the variance,
%
\begin{align*}
V &= \frac{1}{n} \sum_i \left( x_i - \left(\mu \pm \delta\mu\right)\right)^2 \\
&= \frac{1}{n} \sum_i \lbrace  \left( x_i - \mu \right)^2 + \delta\mu^2 \mp 2\, \delta\mu\,  (x_i - \mu)\rbrace \\
&= V_\mathrm{unmod} + \delta\mu^2 \mp 2\ \delta\mu \left( \frac{1}{n} \sum_i x_i- \mu\right) \\
&= V_\mathrm{unmod} + \delta\mu^2
\end{align*}
%

In contrast, if a neuromodulator only acts on interneurons of the higher-level subnetwork, the sensory weight changes as a consequence of the modulated firing rates of the higher-level $V$ neuron. The higher-level $V$ neuron is directly affected by the changes in the higher-level PE neurons and indirectly affected by changes in the $M$ neuron of the same network.

Together, this suggests that whether the sensory weight decreases, increases, or remains the same in the presence of a neuromodulator depends on several factors:
%
\begin{itemize}
\item Does the neuromodulator act on the lower-level or higher-level subnetwork (that is, local impact), or does the neuromodulator act on both to the same degree (that is, global impact)?
\item Which interneuron type/s is/are affected by the neuromodulator? And are these interneurons inhibited or excited by the neuromodulator?
\item How are these interneurons embedded in the network, that is, what are the connectivity and the inputs to those neurons?
\end{itemize}
%
As a result, different neuromodulators may have the same effect on the sensory weight or the same neuromodulator may have different effects depending on brain area, species, etc..


\subsection{Sensory weight and contraction bias}
%
In the simulations, we define the bias as the trial-averaged difference between the weighted output and the true stimulus. For the sake of simplicity, we use $r_\mathrm{out}$ at the end of a trial, $T$, as a proxy for the trial average in the subsequent analysis. Hence, 
%
\begin{align}
 \mathrm{bias} = r_\mathrm{out}(T) - s.
\end{align}
%
To investigate how the bias depends on the sensory weight and potentially other factors, let us resume a toy model in which we assume that the prediction decays exponentially with time constant $\tau$ to a presented constant stimulus value, $s$,
%
\begin{align}
P = P_\mathrm{0} \cdot \mathrm{e}^{-t/\tau_M} +  s \cdot \left( 1 -   \mathrm{e}^{-t/\tau} \right)
\end{align}
%
with $P_0$ describing the initial value at time $t=0$. Let us further assume that within a trial with trial duration $T$, the stimulus value changes $n$ times ($T = n\cdot \Delta t$).  The prediction during the presentation of the $n$th stimulus value can be expressed as
%
\begin{align}
P_\mathrm{n} = P_\mathrm{0} \cdot \mathrm{e}^{-\Delta t/\tau_M}  + \left( 1 -   \mathrm{e}^{-\Delta t/\tau_M} \right) \sum_{i=1}^{n} s_i \cdot \mathrm{e}^{-(n-i)\cdot \Delta t/ \tau_M}.
\end{align}
%
To obtain an estimate for the prediction at the end of a trial, $P_\mathrm{n}$ must be averaged over the stimulus distribution, $\langle P_\mathrm{n} \rangle_s$. For the sake of simplicity, let us assume the stimulus values are drawn from a uniform distribution $U\left( s - \frac{\sigma_\mathrm{S}}{12}, s + \frac{\sigma_\mathrm{S}}{12} \right)$. Moreover, we assume that the initial state, $P_0$, at the beginning of a new trial is drawn from a uniform distribution $U\left( \mu - \frac{\sigma_\mathrm{P}}{12}, \mu + \frac{\sigma_\mathrm{P}}{12} \right)$. With these assumptions, $\langle P_\mathrm{n} \rangle_s$ is given by
%
\begin{align}
\langle P_\mathrm{n} \rangle_s = \mathrm{e}^{-\Delta t/\tau_M}  \int\limits_{\mu - \frac{\sigma_\mathrm{P}}{12}}^{\mu + \frac{\sigma_\mathrm{P}}{12}} P_\mathrm{0} \ f(P_\mathrm{0})\ dP_0+ \left( 1 -   \mathrm{e}^{-\Delta t/\tau_M} \right) \sum_{i=1}^{n} \mathrm{e}^{-(n-i)\cdot \Delta t/ \tau_M} \int\limits_{s - \frac{\sigma_\mathrm{S}}{12}}^{s + \frac{\sigma_\mathrm{S}}{12}} x\ f(x)\ dx.
\end{align}
%
Solving the integrals yield
%
\begin{align}
\langle P_\mathrm{n} \rangle_s = \mu \cdot \mathrm{e}^{-T/\tau_M} + \left( 1 -   \mathrm{e}^{-\Delta t/\tau_M} \right) \sum_{i=1}^{n} \mathrm{e}^{-(n-i)\cdot \Delta t/ \tau_M} \cdot s.
\end{align}
%
Making use of the geometric series, the expression simplifies to
%
\begin{align*}
\langle P_\mathrm{n} \rangle_s =  \mu \cdot \mathrm{e}^{-T/\tau_M} + \left( 1 -   \mathrm{e}^{-T/\tau_M} \right) \cdot s.
\end{align*}
%
Inserting the expression in the equation for the weighted output yields
%
\begin{align*}
 r_\mathrm{out} = \left[ \alpha_\mathrm{S}\  \mathrm{e}^{-T/\tau_M} + \left( 1 -   \mathrm{e}^{-T/\tau_M} \right)\right] \cdot s + \left( 1 -\alpha_\mathrm{S} \right)\ \mathrm{e}^{-T/\tau_M}\ \mu.
\end{align*}
%
Hence, the bias in our toy model can be expressed by
%
\begin{align*}
 \mathrm{bias} = \left( 1 -\alpha_\mathrm{S} \right)\cdot \mathrm{e}^{-T/\tau_M}\cdot \left(\mu - s \right).
\end{align*}
%
The absolute slope $\left( 1 -\alpha_\mathrm{S} \right)\cdot \mathrm{e}^{-T/\tau_M}$ indicates how strong the bias is. It depends on the sensory weight $\alpha_\mathrm{S}$, the trial duration $T$ and time constant $\tau_M$. Please note that the sensory weight is a function of the trial duration itself (see Fig. \ref{fig:Fig_3}F). However, for illustration purposes, we take $\alpha_\mathrm{S}$ to be constant.
 
In this toy model, if the variance of the prediction is zero (that is, in a prediction-driven input regime), $\alpha_\mathrm{S} \approx 0$, and, consequently, the bias is $\mathrm{e}^{-T/\tau_M}\cdot \left(\mu - s \right)$. Thus, the bias is independent of the stimulus variance (see Fig. \ref{fig:Fig_5} D).

Likewise, if the variance of the sensory stimulus is zero (that is, in a stimulus-driven input regime), $\alpha_\mathrm{S} \approx 1$, and, consequently, the bias approaches $0$ if the neurons reach their steady state. Thus, decreasing or increasing the trail variance does not have an effect on the bias (see Fig. \ref{fig:Fig_5} C).

%Moreover, the toy model also reveals that if $T\rightarrow \infty$, the slope $m$ goes to 1, suggesting that without further noise in the system, the network can perfectly represent the sensory stimulus (the prediction would approach the current stimulus). 


%\subsection{Comparison to Kalman filter and Bayes Factor surprise}
%%
%Kalman filter. Initialisation
%%
%\begin{align*}
%x_{0|init} &= 0 \\
%P_{0|init} &= \sigma^2\ I
%\end{align*}
%%
%with x being the system state (in my terms the prediction), P is the covariance matrix of the errors of x (in my terms the var of the predictions) and I is the identity matrix.
%%
%Then the "correction" is given by
%%
%\begin{align*}
%K_k &= P_{k|k-1}\ H_k^T\ \left( H_k\ P_{k|k-1}\ H_k^T + R_k \right)^{-1} \\
%x_k &= x_{k|k-1} + K_k\ \left( z_k - H_k\ x_{k|k-1}\right) \\
%P_k &= \left( I - K_k\ H_k\right)\ P_{k|k-1}
%\end{align*}
%%
%with K the kalman gain matrix, H the observation matrix ($z_k = H_k\ x_k + noise$), R the covaraince of the measurement noise and z a new observation. The last part of the Kalman filter is the "prediction":
%%
%\begin{align*}
%x_{k|k-1} &= F_{k-1}\ x_{k-1} + B_{k-1}\ u_{k-1} \\
%P_{k|k-1} &= F_{k-1}\ P_{k-1}\ F_{k-1}^T + Q_{k-1}
%\end{align*}
%%
%with F the transition matrix ($x_{k|k-1} = F_{k-1}\ x_{k-1}$, u a deterministic perturbation, B the dynamics of the deterministic perturbation. In our terms
%%
%\begin{align*}
%\alpha = K_k = \frac{P_{k|k-1}}{R_k + P_{k|k-1}}
%\end{align*}
%%
%$P_{k|k-1}$, is however $\sigma_P^2$ in my implementation and $R_k$ is fixed variance of inputs $\sigma_S^2$. Hence, my implementation represents (?) the Kalman filter. Important to note is, that in my implementation we estimate the variance of inputs dynamically, so it is not set! Another nice advantage here is that I don't need a good estimate for P. I can basically initiate it as I want. Another difference is that I consider the optimal weighting in my "output neuron" and not the prediction itself ... .
%
% Comparison to Bayes Factor surprise

\section{Supplementary Figures}

\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_2_S1.pdf}% [width=1\linewidth]
\caption{\footnotesize{\bf Estimating mean and variance of different stimulus distributions.\newline}  
Top: The normalised absolute difference between the averaged mean and the activity of the M neuron decreases to a near-zero level for all stimulus distributions tested. 
Bottom: The normalised absolute difference between the averaged variance and the activity of the V neuron decreases with small differences between the distributions tested. Parametrisation of the uniform distribution as in Fig. \ref{fig:Fig_2}. 
}
\label{fig:Fig_2_S1}
\end{figure}


\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_2_S2.pdf}% [width=1\linewidth]
\caption{\footnotesize{\bf Estimating mean and variance of sensory stimuli in a rate-based multi-cell population network.\newline}  
{\bf (A)} Illustration of the rate-based multi-cell population network and the stimuli over time. The weights from the PE neurons onto the M or V neuron are scaled by a factor $\gamma$ drawn from a normal distribution $N(\mu_\mathrm{\gamma}, \sigma_\mathrm{\gamma})$.
{\bf (B)} M and V neuron activities over time for one example parameterisation.
{\bf (C)} The normalised absolute difference between the averaged mean and the activity of the M neuron (dark green) or between the averaged variance and the activity of the V neuron (brown) for uncorrelated deviations, that is, increasing $\sigma_\mathrm{\gamma}$ (left), correlated deviations, that is, increasing $\mu_\mathrm{\gamma}$ (middle), and the network sparsity. To speed up simulations, we chose $\lambda^\mathrm{lower}=5\cdot 10^{-2}$.
}
\label{fig:Fig_2_S2}
\end{figure}

\begin{figure}[!h]
	\centering
    \includegraphics[width=1\linewidth]{../results/figures/final/Fig_2_S3.pdf}% [width=1\linewidth]
\caption{\footnotesize{\bf Estimating mean and variance of spatial stimuli.\newline}  
{\bf (A)} Illustration of a network estimating the mean and variance of a stimulus that varies across space. To simulate selectivity, the network comprises $1000$ identical, uncoupled mean-field networks each receiving a different input value drawn from a uniform distribution. 
{\bf (B)} Activity of M neuron (top) and V neuron (bottom) for 2 stimuli. The second stimulus does either differ in the mean (orange) or the variance (yellow) from the first stimulus (indicated in C).
{\bf (C)} The normalised absolute difference between the averaged mean and the activity of the M neuron (dark green, top) or between the averaged variance and the activity of the V neuron (brown, bottom) for a range of different stimulus statistics. The examples from B are shown with colored arrows and markers. To speed up simulations, we chose $\lambda^\mathrm{lower}=3\cdot 10^{-1}$.
}
\label{fig:Fig_2_S3}
\end{figure}


\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_3_S1}% [width=1\linewidth]
\caption{\footnotesize{\bf Dynamic variance estimation allows flexible adaptation to changes in the stimulus statistics and environment. \newline}  
{\bf (A)} Illustration of the sensory weight for different input statistics. Numbers denote specific examples. Arrows denote the transitions between those statistics.
{\bf (B)} The sensory weight over time is shown for all transitions in (A). For the sake of clarity, we only show the trials 40-60. The switch to new input statistics occurs at trial 50. Parameters are listed in the Supporting Information.
}
\label{fig:Fig_3_S1}
\end{figure}


\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_3_S2}% [width=1\linewidth]
\caption{\footnotesize{\bf Perturbing the weighting of sensory inputs and predictions by altering network properties. \newline}  
{\bf (A)} The weights from the PE neurons to the M neuron in the lower-order subnetwork are scaled by a factor 0.3 or 7, leading to a distorted sensory weight. If the update of the M neuron in the lower subnetwork is too slow ($\blacktriangleleft$), the prediction is overrated. If the update of the M neuron in the lower subnetwork is too fast ($\blacktriangleright$), the sensory input is overrated.
{\bf (B)} The precise activation function for the V neurons does not have a major impact on the sensory weight. Only for inputs with high stimulus variability, the sensory stimulus is slightly overrated when the quadratic activation function is replaced by a linear, rectified activation function.
}
\label{fig:Fig_3_S2}
\end{figure}


\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_4_S1.pdf}% [width=1\linewidth]
\caption{\footnotesize{\bf The impact of neuromodulators acting globally on groups of interneurons. \newline}  
The sensory weight changes when groups of interneurons are targeted by a neuromodulator. Whether the sensory weight decreases or increases not also depends on the modulation strength (see Fig. \ref{fig:Fig_4}) but also on how strongly which interneuron is targeted.  As shown in Fig. \ref{fig:Fig_4}, the sensory weight is pushed toward 0.5 if the VIP neuron is stimulated. The sensory weight generally decreases when PV neurons are the main target. Considered are two limit cases (upper row: more sensory-driven before modulation, lower row: more prediction-driven before modulation). The results are shown for three mean-field networks (see Fig. \ref{fig:Fig_4}).
}
\label{fig:Fig_4_S1}
\end{figure}


\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_4_S2}% [width=1\linewidth]
\caption{\footnotesize{\bf The impact of neuromodulators acting locally on groups of interneurons. \newline}  
{\bf (A)} Sensory weight changes with neuromodulators acting on interneurons in the lower PE circuit.
{\bf (B)} Sensory weight changes with neuromodulators acting on interneurons in the higher PE circuit. In general, the changes in the sensory weight is the opposite of the changes seen for neuromodulators acting on the lower-level PE neurons. Simulation parameters, labels and colors as in Fig. \ref{fig:Fig_4}. 
}
\label{fig:Fig_4_S2}
\end{figure}


\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_4_S3}% [width=1\linewidth]
\caption{\footnotesize{\bf Biased mean and variance estimation by changing the baseline and the gain of nPE and pPE.}
In a toy model, described in sections \ref{sec:gain_impact} and \ref{sec:impact_baseline}, the contribution of gain and baseline to the changes in the mean and variance estimation are summarized. The results shown are based on the Eqs. (\ref{eq:prediction_gain}), (\ref{eq:variance_gain}), (\ref{eq:condition_baseline_mean_1}) and  (\ref{eq:condition_baseline_variance_2}).
}
\label{fig:Fig_4_S3}
\end{figure}

\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_4_S4}% [width=1\linewidth]
\caption{\footnotesize{\bf The combined changes in baseline and gain of all PE neurons determine the shift in the sensory weight.\newline}  
Whether and how a neuromodulator changes the sensory weight depends on the interneuron targeted and the effect this interneuron has on the baseline and gain of both PE neurons, which in turn does depend on the network it is embedded in. For small inputs, changes in the baseline dominate, while for large inputs, the changes in the gains dominate the shift in the sensory weight. Gray lines denote different mean inputs, illustrating that the same interneuron can decrease or increase the variance depending on the input regime.
}
\label{fig:Fig_4_S4}
\end{figure}


\begin{figure}[!h]
	\centering
    \includegraphics{../results/figures/final/Fig_5_S1}% [width=1\linewidth]
\caption{\footnotesize{\bf Including scalar variability in the model \newline}  
When scalar variability is included, that is, the stimulus standard deviation depends linearly on the stimulus mean, the bias is larger for stimuli drawn from the upper end of the stimulus distribution than from the lower end. 
}
\label{fig:Fig_5_S1}
\end{figure}


\end{document}
