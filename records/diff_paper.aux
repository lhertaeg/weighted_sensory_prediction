\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{deneve2004bayesian}
\citation{ernst2002humans,battaglia2003bayesian,kording2004bayesian,alais2004ventriloquist,rowland2007bayesian,gu2008neural,fetsch2012neural}
\citation{kording2004bayesian,yon2021precision}
\citation{keller2018predictive}
\citation{eliades2008neural,keller2009neural,ayaz2019layer,audette2021temporally}
\citation{rao1999predictive,keller2018predictive}
\citation{keller2012sensorimotor,attinger2017visuomotor,jordan2020opposing,audette2021temporally}
\citation{markram2004interneurons,rudy2011three,pfeffer2013inhibition,jiang2015principles,tremblay2016gabaergic,campagnola2022local}
\citation{hertag2020learning,hertag2022prediction}
\citation{hollingworth1910central,jazayeri2010temporal,ashourian2011bayesian,petzschner2011iterative,akrami2018posterior,meirhaeghe2021precise}
\citation{hertag2020learning,hertag2022prediction}
\citation{larkum2013cellular,harris2015neocortical}
\citation{mumford1992computational,larkum2013cellular,friston2008hierarchical}
\citation{keller2018predictive}
\citation{larkum2013cellular}
\citation{tremblay2016gabaergic}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Neural network model to track both the uncertainty of sensory inputs and predictions.\newline  } {\bf  (A)} Example illustration for context-dependent integration of information. Left: When walking down an unfamiliar staircase that is visible, the brain might rely solely on external sensory information. Middle: When walking down the same stairs without visual information, the brain might rely on predictions formed by previous experience. Right: When climbing down an unexplored mountain in foggy conditions, the brain might need to integrate sensory information and predictions simultaneously. {\bf  (B)} \DIFaddbeginFL  {\color {blue}\uwave {Top: }}\DIFaddendFL  Illustration of a prediction-error (PE) circuit with both negative and positive PE (nPE/pPE) neurons that receive inhibition from three different inhibitory interneuron types: parvalbumin-expressing (PV), somatostatin-expressing (SOM), and vasoactive intestinal peptide-expressing (VIP) interneurons. Local excitatory connections are not shown for clarity. \DIFaddbeginFL  {\color {blue}\uwave {Bottom: Responses of an nPE and pPE neuron. The nPE neuron only increases its activity relative to a baseline when the sensory input is weaker than predicted, while the pPE neuron only increases its activity relative to a baseline when the sensory input is stronger than predicted. }}\DIFaddendFL  {\bf  (C)} Illustration of network model that estimates the mean and variance of the external sensory stimuli. The core of this network model is the PE circuit shown in (B). The lower-level V neuron encodes the variance, while the lower-level M neuron encodes the mean of the sensory input. {\bf  (D)} Same as in (C) but the feedforward input is the activity of the lower-level M neuron. \relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Fig_1}{{1}{3}{\footnotesize {\bf Neural network model to track both the uncertainty of sensory inputs and predictions.\newline } {\bf (A)} Example illustration for context-dependent integration of information. Left: When walking down an unfamiliar staircase that is visible, the brain might rely solely on external sensory information. Middle: When walking down the same stairs without visual information, the brain might rely on predictions formed by previous experience. Right: When climbing down an unexplored mountain in foggy conditions, the brain might need to integrate sensory information and predictions simultaneously. {\bf (B)} \DIFaddbeginFL \DIFaddFL {Top: }\DIFaddendFL Illustration of a prediction-error (PE) circuit with both negative and positive PE (nPE/pPE) neurons that receive inhibition from three different inhibitory interneuron types: parvalbumin-expressing (PV), somatostatin-expressing (SOM), and vasoactive intestinal peptide-expressing (VIP) interneurons. Local excitatory connections are not shown for clarity. \DIFaddbeginFL \DIFaddFL {Bottom: Responses of an nPE and pPE neuron. The nPE neuron only increases its activity relative to a baseline when the sensory input is weaker than predicted, while the pPE neuron only increases its activity relative to a baseline when the sensory input is stronger than predicted. }\DIFaddendFL {\bf (C)} Illustration of network model that estimates the mean and variance of the external sensory stimuli. The core of this network model is the PE circuit shown in (B). The lower-level V neuron encodes the variance, while the lower-level M neuron encodes the mean of the sensory input. {\bf (D)} Same as in (C) but the feedforward input is the activity of the lower-level M neuron. \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Prediction-error neurons as the basis for estimating mean and variance of sensory stimuli.\newline  } {\bf  (A)} Illustration of the inputs with which the network (Fig. \ref  {fig:Fig_1}C) is stimulated. Network is exposed to a sequence of constant stimuli drawn from a uniform distribution. \DIFaddbeginFL  {\color {blue}\uwave {The gray shaded boxes symbolize different values from the distribution. }}\DIFaddendFL  {\bf  (B)} PE neuron activity hardly changes with stimulus strength (left) but strongly increases with stimulus variability (right). {\bf  (C)} Interneuron activity strongly changes with stimulus strength (left) but hardly changes with stimulus variability (right). {\bf  (D)} M neuron correctly encodes the mean of the sensory stimuli. Left: Illustration of the input synapses onto the M neuron. Middle: Activity of the M neuron over time for one example distribution (black start in right panel). Right: Normalised absolute difference between the averaged mean and the activity of the M neuron in the steady state for different parametrizations of the stimulus distribution. {\bf  (E)} V neuron correctly encodes the variance of the sensory stimuli. Left: Illustration of the input synapses onto the V neuron. Middle: Activity of the V neuron over time for one example distribution (black start in right panel). Right: Normalised absolute difference between the averaged variance and the activity of the V neuron in the steady state for different parametrizations of the stimulus distribution. \relax }}{4}{figure.caption.5}}
\newlabel{fig:Fig_2}{{2}{4}{\footnotesize {\bf Prediction-error neurons as the basis for estimating mean and variance of sensory stimuli.\newline } {\bf (A)} Illustration of the inputs with which the network (Fig. \ref {fig:Fig_1}C) is stimulated. Network is exposed to a sequence of constant stimuli drawn from a uniform distribution. \DIFaddbeginFL \DIFaddFL {The gray shaded boxes symbolize different values from the distribution. }\DIFaddendFL {\bf (B)} PE neuron activity hardly changes with stimulus strength (left) but strongly increases with stimulus variability (right). {\bf (C)} Interneuron activity strongly changes with stimulus strength (left) but hardly changes with stimulus variability (right). {\bf (D)} M neuron correctly encodes the mean of the sensory stimuli. Left: Illustration of the input synapses onto the M neuron. Middle: Activity of the M neuron over time for one example distribution (black start in right panel). Right: Normalised absolute difference between the averaged mean and the activity of the M neuron in the steady state for different parametrizations of the stimulus distribution. {\bf (E)} V neuron correctly encodes the variance of the sensory stimuli. Left: Illustration of the input synapses onto the V neuron. Middle: Activity of the V neuron over time for one example distribution (black start in right panel). Right: Normalised absolute difference between the averaged variance and the activity of the V neuron in the steady state for different parametrizations of the stimulus distribution. \relax }{figure.caption.5}{}}
\citation{pouget2013probabilistic}
\citation{yon2021precision}
\citation{avery2017neuromodulatory}
\citation{cardin2019functional,hattori2017functions,swanson2019hiring}
\citation{wester2014behavioral,hattori2017functions,swanson2019hiring}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Estimating the uncertainty of both the sensory input and the prediction.\newline  } {\bf  (A)} Illustration of the stimulation protocol. The network is exposed to a sequence of stimuli (one stimulus per trial). To account for stimulus variability, each stimulus is represented by $10$ stimulus values drawn from a normal distribution. To account for the volatility of the environment, in each trial, the stimulus mean is drawn from a uniform distribution (denoted \DIFdelbeginFL  {\color {red}\sout {trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {trial-to-trial }}\DIFaddendFL  variability). {\bf  (B)} Illustration of how the weighted output is calculated. The sensory weight $\alpha $ lies between zero (system relies perfectly on prediction) and one (system relies solely on the sensory input). {\bf  (C)} Limit case example in which the stimulus variability is zero but the \DIFdelbeginFL  {\color {red}\sout {trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {trial-to-trial }}\DIFaddendFL  variability is high. Left: Illustration of the stimulation protocol. Middle: Weighted output follows closely the sensory stimuli. Right: Sensory weight (function of the variances, see B) close to 1, indicating that the network ignores the prediction. Input statistics shown in E. {\bf  (D)} Limit case example in which the stimulus variability is high but the \DIFdelbeginFL  {\color {red}\sout {trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {trial-to-trial }}\DIFaddendFL  variability is zero. Left: Illustration of the stimulation protocol. Middle: Weighted output pushed towards the mean of the sensory stimuli. Right: Sensory weight close to zero, indicating that the network ignores the sensory stimuli. Input statistics shown in E. {\bf  (E)} Sensory weight for different input statistics. Predictions are weighted more strongly when the stimulus variability is larger than the \DIFdelbeginFL  {\color {red}\sout {trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {trial-to-trial }}\DIFaddendFL  variability. {\bf  (F)} Sensory weight\DIFdelbeginFL  {\color {red}\sout {throughout a trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {, averaged over many trials, }}\DIFaddendFL  for two different trial durations. \DIFaddbeginFL  {\color {blue}\uwave {Gray shading denotes the SEM. }}\DIFaddendFL  Predictions are weighted more strongly at the beginning of a new trial. \relax }}{6}{figure.caption.7}}
\newlabel{fig:Fig_3}{{3}{6}{\footnotesize {\bf Estimating the uncertainty of both the sensory input and the prediction.\newline } {\bf (A)} Illustration of the stimulation protocol. The network is exposed to a sequence of stimuli (one stimulus per trial). To account for stimulus variability, each stimulus is represented by $10$ stimulus values drawn from a normal distribution. To account for the volatility of the environment, in each trial, the stimulus mean is drawn from a uniform distribution (denoted \DIFdelbeginFL \DIFdelFL {trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {trial-to-trial }\DIFaddendFL variability). {\bf (B)} Illustration of how the weighted output is calculated. The sensory weight $\alpha $ lies between zero (system relies perfectly on prediction) and one (system relies solely on the sensory input). {\bf (C)} Limit case example in which the stimulus variability is zero but the \DIFdelbeginFL \DIFdelFL {trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {trial-to-trial }\DIFaddendFL variability is high. Left: Illustration of the stimulation protocol. Middle: Weighted output follows closely the sensory stimuli. Right: Sensory weight (function of the variances, see B) close to 1, indicating that the network ignores the prediction. Input statistics shown in E. {\bf (D)} Limit case example in which the stimulus variability is high but the \DIFdelbeginFL \DIFdelFL {trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {trial-to-trial }\DIFaddendFL variability is zero. Left: Illustration of the stimulation protocol. Middle: Weighted output pushed towards the mean of the sensory stimuli. Right: Sensory weight close to zero, indicating that the network ignores the sensory stimuli. Input statistics shown in E. {\bf (E)} Sensory weight for different input statistics. Predictions are weighted more strongly when the stimulus variability is larger than the \DIFdelbeginFL \DIFdelFL {trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {trial-to-trial }\DIFaddendFL variability. {\bf (F)} Sensory weight\DIFdelbeginFL \DIFdelFL {throughout a trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {, averaged over many trials, }\DIFaddendFL for two different trial durations. \DIFaddbeginFL \DIFaddFL {Gray shading denotes the SEM. }\DIFaddendFL Predictions are weighted more strongly at the beginning of a new trial. \relax }{figure.caption.7}{}}
\citation{hertag2022prediction}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Neuromodulator-based shifts in the weighting of sensory inputs and predictions. \newline  } {\bf  (A)} Neuromodulators acting on the interneurons can shift the weighting of sensory inputs and predictions. The changes depend on the type of interneuron targeted and the modulation strength (here simulated through an additional excitatory input). Considered are two limit cases (upper row: more sensory-driven before modulation, lower row: more prediction-driven before modulation). The results are shown for three different PE circuits (denotes by different markers). {\bf  (B)} When SOM and VIP neurons are equally modulated, the sensory weight remains unaffected. {\bf  (C)} The V neurons' activities depend on the PE neurons. Hence, perturbing the nPE and pPE \DIFdelbeginFL  {\color {red}\sout {neurons }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {neuron }}\DIFaddendFL  changes the uncertainty estimation. \DIFdelbeginFL  {\color {red}\sout {While }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {Left: }}\DIFaddendFL  stimulating the \DIFaddbeginFL  {\color {blue}\uwave {pPE (triangle) or nPE (upside down triangle) affects the V neuron of the same subnetwork, denoted by matching marker and line colors (dark brown: }}\DIFaddendFL  lower \DIFaddbeginFL  {\color {blue}\uwave {circuit, light brown: higher circuit). Right: while stimulating the lower }}\DIFaddendFL  PE neurons affects \DIFdelbeginFL  {\color {red}\sout {both }}\DIFdelendFL  the \DIFdelbeginFL  {\color {red}\sout {lower and }}\DIFdelendFL  higher-order V \DIFdelbeginFL  {\color {red}\sout {neurons (right)}}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {neuron}}\DIFaddendFL  , stimulating the higher-order PE neurons \DIFdelbeginFL  {\color {red}\sout {only affects }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {does not change }}\DIFaddendFL  the \DIFaddbeginFL  {\color {blue}\uwave {activity of the lower-order }}\DIFaddendFL  V neuron \DIFdelbeginFL  {\color {red}\sout {in the same subnetwork }}\DIFdelendFL  (\DIFdelbeginFL  {\color {red}\sout {left}}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {denoted by not matching marker and line colors}}\DIFaddendFL  ). {\bf  (D)} The V neuron activity, and hence the sensory weight, changes as a result of the modulated PE neuron activity. The PE neuron activity, on the other hand, changes as a result of the interneurons being modulated. The interneurons change the baseline (left) and the gain (right) of the PE neurons. Whether an interneuron increases or decreases the estimated variance depends on both factors. \relax }}{7}{figure.caption.9}}
\newlabel{fig:Fig_4}{{4}{7}{\footnotesize {\bf Neuromodulator-based shifts in the weighting of sensory inputs and predictions. \newline } {\bf (A)} Neuromodulators acting on the interneurons can shift the weighting of sensory inputs and predictions. The changes depend on the type of interneuron targeted and the modulation strength (here simulated through an additional excitatory input). Considered are two limit cases (upper row: more sensory-driven before modulation, lower row: more prediction-driven before modulation). The results are shown for three different PE circuits (denotes by different markers). {\bf (B)} When SOM and VIP neurons are equally modulated, the sensory weight remains unaffected. {\bf (C)} The V neurons' activities depend on the PE neurons. Hence, perturbing the nPE and pPE \DIFdelbeginFL \DIFdelFL {neurons }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {neuron }\DIFaddendFL changes the uncertainty estimation. \DIFdelbeginFL \DIFdelFL {While }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {Left: }\DIFaddendFL stimulating the \DIFaddbeginFL \DIFaddFL {pPE (triangle) or nPE (upside down triangle) affects the V neuron of the same subnetwork, denoted by matching marker and line colors (dark brown: }\DIFaddendFL lower \DIFaddbeginFL \DIFaddFL {circuit, light brown: higher circuit). Right: while stimulating the lower }\DIFaddendFL PE neurons affects \DIFdelbeginFL \DIFdelFL {both }\DIFdelendFL the \DIFdelbeginFL \DIFdelFL {lower and }\DIFdelendFL higher-order V \DIFdelbeginFL \DIFdelFL {neurons (right)}\DIFdelendFL \DIFaddbeginFL \DIFaddFL {neuron}\DIFaddendFL , stimulating the higher-order PE neurons \DIFdelbeginFL \DIFdelFL {only affects }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {does not change }\DIFaddendFL the \DIFaddbeginFL \DIFaddFL {activity of the lower-order }\DIFaddendFL V neuron \DIFdelbeginFL \DIFdelFL {in the same subnetwork }\DIFdelendFL (\DIFdelbeginFL \DIFdelFL {left}\DIFdelendFL \DIFaddbeginFL \DIFaddFL {denoted by not matching marker and line colors}\DIFaddendFL ). {\bf (D)} The V neuron activity, and hence the sensory weight, changes as a result of the modulated PE neuron activity. The PE neuron activity, on the other hand, changes as a result of the interneurons being modulated. The interneurons change the baseline (left) and the gain (right) of the PE neurons. Whether an interneuron increases or decreases the estimated variance depends on both factors. \relax }{figure.caption.9}{}}
\citation{hollingworth1910central,jazayeri2010temporal,ashourian2011bayesian,petzschner2011iterative,akrami2018posterior,meirhaeghe2021precise}
\citation{rakitin1998scalar}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Mechanisms underlying the contraction bias.\newline  } {\bf  (A)} Contraction bias in the model for two different stimulus uncertainties depicted in the inset. Bias is defined as the weighted output minus the stimulus mean. The absolute value of the slope \DIFdelbeginFL  {\color {red}\sout {(see }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {of the }}\DIFaddendFL  linear fit\DIFdelbeginFL  {\color {red}\sout {) }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {, }}\textit  {{\color {blue}\uwave {m}}}{\color {blue}\uwave {, }}\DIFaddendFL  is a measure of the bias. The larger the slope, the larger the bias. {\bf  (B)} As a consequence of the sensory weight, the slope increases with stimulus variability (bias increases) and decreases with \DIFdelbeginFL  {\color {red}\sout {trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {trial-to-trial }}\DIFaddendFL  variability (bias decreases). {\bf  (C)} Bias is independent of the \DIFdelbeginFL  {\color {red}\sout {trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {trial-to-trial }}\DIFaddendFL  variability when the stimulus variability is zero. {\bf  (D)} Bias is independent of the stimulus variability when the \DIFdelbeginFL  {\color {red}\sout {trial }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {trial-to-trial }}\DIFaddendFL  variability is zero. {\bf  (E)} The slope depends on the trial duration. \relax }}{9}{figure.caption.11}}
\newlabel{fig:Fig_5}{{5}{9}{\footnotesize {\bf Mechanisms underlying the contraction bias.\newline } {\bf (A)} Contraction bias in the model for two different stimulus uncertainties depicted in the inset. Bias is defined as the weighted output minus the stimulus mean. The absolute value of the slope \DIFdelbeginFL \DIFdelFL {(see }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {of the }\DIFaddendFL linear fit\DIFdelbeginFL \DIFdelFL {) }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {, }\textit {\DIFaddFL {m}}\DIFaddFL {, }\DIFaddendFL is a measure of the bias. The larger the slope, the larger the bias. {\bf (B)} As a consequence of the sensory weight, the slope increases with stimulus variability (bias increases) and decreases with \DIFdelbeginFL \DIFdelFL {trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {trial-to-trial }\DIFaddendFL variability (bias decreases). {\bf (C)} Bias is independent of the \DIFdelbeginFL \DIFdelFL {trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {trial-to-trial }\DIFaddendFL variability when the stimulus variability is zero. {\bf (D)} Bias is independent of the stimulus variability when the \DIFdelbeginFL \DIFdelFL {trial }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {trial-to-trial }\DIFaddendFL variability is zero. {\bf (E)} The slope depends on the trial duration. \relax }{figure.caption.11}{}}
\citation{han2023behavior}
\citation{kording2004bayesian,yon2021precision}
\citation{ernst2002humans}
\citation{battaglia2003bayesian,kording2004bayesian,alais2004ventriloquist,rowland2007bayesian,gu2008neural,fetsch2012neural}
\citation{wallace1998multisensory,gu2008neural,fetsch2012neural}
\citation{kording2004bayesian,yon2021precision}
\citation{pakan2018impact,han2023behavior}
\citation{yon2021precision}
\citation{herzfeld2014memory}
\citation{yon2021precision}
\citation{cardin2019functional}
\citation{wilmes2023uncertainty}
\citation{o2010coding,o2012can}
\citation{knill2004bayesian}
\citation{hoyer2002interpreting,ma2006bayesian}
\citation{soltani2019adaptive}
\citation{kiani2009representation}
\citation{masset2020behavior}
\citation{rushworth2008choice}
\citation{jo2016differential}
\citation{white2016neurons}
\citation{rutishauser2015representation,rutishauser2018single}
\citation{bastos2012canonical,keller2018predictive}
\citation{bastos2012canonical,heindorf2022reduction}
\citation{o2022prediction}
\citation{eliades2008neural,keller2009neural,ayaz2019layer,audette2021temporally}
\citation{rao1999predictive}
\citation{keller2012sensorimotor,attinger2017visuomotor,jordan2020opposing,audette2021temporally}
\citation{keller2018predictive}
\citation{murray2014hierarchy,chaudhuri2015large,runyan2017distinct}
\citation{yon2021precision}
\citation{yon2021precision}
\citation{yon2021precision}
\citation{yu2005uncertainty}
\citation{yu2005uncertainty}
\citation{yu2005uncertainty}
\citation{hasselmo1997noradrenergic,yon2021precision}
\citation{ridley1981new,janitzky2015optogenetic}
\citation{lawson2021computational}
\citation{yon2021precision}
\citation{urban2016somatostatin,hattori2017functions,swanson2019hiring}
\citation{nadim2014neuromodulation}
\citation{yu2005uncertainty,marshall2016pharmacological}
\citation{marshall2016pharmacological}
\citation{wester2014behavioral,hattori2017functions,swanson2019hiring}
\citation{pawlak2010timing}
\citation{jordan2023locus}
\citation{jordan2023locus}
\citation{wilmes2023uncertainty}
\citation{wilmes2023uncertainty,granier2023precision}
\citation{marshall2016pharmacological,bruckner2022understanding}
\citation{wong2023computational}
\citation{battaglia2003bayesian,alais2004ventriloquist}
\citation{butler2010bayesian}
\citation{fetsch2009dynamic}
\citation{summerfield2011perceptual}
\citation{o2012can}
\citation{soltani2019adaptive}
\citation{liakoni2021learning}
\citation{kutschireiter2023bayesian}
\citation{kutschireiter2023bayesian}
\citation{deneve2008bayesian}
\citation{hertag2022prediction}
\citation{pouget2013probabilistic}
\bibstyle{plainnat}
\bibdata{References_HertaegWilmesClopath_2023}
\bibcite{akrami2018posterior}{{1}{2018}{{Akrami et~al.}}{{Akrami, Kopec, Diamond, and Brody}}}
\bibcite{alais2004ventriloquist}{{2}{2004}{{Alais and Burr}}{{}}}
\bibcite{ashourian2011bayesian}{{3}{2011}{{Ashourian and Loewenstein}}{{}}}
\bibcite{attinger2017visuomotor}{{4}{2017}{{Attinger et~al.}}{{Attinger, Wang, and Keller}}}
\bibcite{audette2021temporally}{{5}{2021}{{Audette et~al.}}{{Audette, Zhou, and Schneider}}}
\bibcite{avery2017neuromodulatory}{{6}{2017}{{Avery and Krichmar}}{{}}}
\bibcite{ayaz2019layer}{{7}{2019}{{Ayaz et~al.}}{{Ayaz, St{\"a}uble, Hamada, Wulf, Saleem, and Helmchen}}}
\bibcite{bastos2012canonical}{{8}{2012}{{Bastos et~al.}}{{Bastos, Usrey, Adams, Mangun, Fries, and Friston}}}
\bibcite{battaglia2003bayesian}{{9}{2003}{{Battaglia et~al.}}{{Battaglia, Jacobs, and Aslin}}}
\bibcite{bruckner2022understanding}{{10}{2022}{{Bruckner et~al.}}{{Bruckner, Heekeren, and Nassar}}}
\bibcite{butler2010bayesian}{{11}{2010}{{Butler et~al.}}{{Butler, Smith, Campos, and B{\"u}lthoff}}}
\bibcite{campagnola2022local}{{12}{2022}{{Campagnola et~al.}}{{Campagnola, Seeman, Chartrand, Kim, Hoggarth, Gamlin, Ito, Trinh, Davoudian, Radaelli, et~al.}}}
\bibcite{cardin2019functional}{{13}{2019}{{Cardin}}{{}}}
\bibcite{chaudhuri2015large}{{14}{2015}{{Chaudhuri et~al.}}{{Chaudhuri, Knoblauch, Gariel, Kennedy, and Wang}}}
\bibcite{deneve2008bayesian}{{15}{2008}{{Deneve}}{{}}}
\bibcite{deneve2004bayesian}{{16}{2004}{{Deneve and Pouget}}{{}}}
\bibcite{eliades2008neural}{{17}{2008}{{Eliades and Wang}}{{}}}
\bibcite{ernst2002humans}{{18}{2002}{{Ernst and Banks}}{{}}}
\bibcite{fetsch2009dynamic}{{19}{2009}{{Fetsch et~al.}}{{Fetsch, Turner, DeAngelis, and Angelaki}}}
\bibcite{fetsch2012neural}{{20}{2012}{{Fetsch et~al.}}{{Fetsch, Pouget, DeAngelis, and Angelaki}}}
\bibcite{friston2008hierarchical}{{21}{2008}{{Friston}}{{}}}
\bibcite{granier2023precision}{{22}{2023}{{Granier et~al.}}{{Granier, Petrovici, Senn, and Wilmes}}}
\bibcite{gu2008neural}{{23}{2008}{{Gu et~al.}}{{Gu, Angelaki, and DeAngelis}}}
\bibcite{han2023behavior}{{24}{2023}{{Han and Helmchen}}{{}}}
\bibcite{harris2015neocortical}{{25}{2015}{{Harris and Shepherd}}{{}}}
\bibcite{hasselmo1997noradrenergic}{{26}{1997}{{Hasselmo et~al.}}{{Hasselmo, Linster, Patil, Ma, and Cekic}}}
\bibcite{hattori2017functions}{{27}{2017}{{Hattori et~al.}}{{Hattori, Kuchibhotla, Froemke, and Komiyama}}}
\bibcite{heindorf2022reduction}{{28}{2022}{{Heindorf and Keller}}{{}}}
\bibcite{hertag2022prediction}{{29}{2022}{{Hert{\"a}g and Clopath}}{{}}}
\bibcite{hertag2020learning}{{30}{2020}{{Hert{\"a}g and Sprekeler}}{{}}}
\bibcite{herzfeld2014memory}{{31}{2014}{{Herzfeld et~al.}}{{Herzfeld, Vaswani, Marko, and Shadmehr}}}
\bibcite{hollingworth1910central}{{32}{1910}{{Hollingworth}}{{}}}
\bibcite{hoyer2002interpreting}{{33}{2002}{{Hoyer and Hyv{\"a}rinen}}{{}}}
\bibcite{janitzky2015optogenetic}{{34}{2015}{{Janitzky et~al.}}{{Janitzky, Lippert, Engelhorn, Tegtmeier, Goldschmidt, Heinze, and Ohl}}}
\bibcite{jazayeri2010temporal}{{35}{2010}{{Jazayeri and Shadlen}}{{}}}
\bibcite{jiang2015principles}{{36}{2015}{{Jiang et~al.}}{{Jiang, Shen, Cadwell, Berens, Sinz, Ecker, Patel, and Tolias}}}
\bibcite{jo2016differential}{{37}{2016}{{Jo and Jung}}{{}}}
\bibcite{jordan2020opposing}{{38}{2020}{{Jordan and Keller}}{{}}}
\bibcite{jordan2023locus}{{39}{2023}{{Jordan and Keller}}{{}}}
\bibcite{keller2009neural}{{40}{2009}{{Keller and Hahnloser}}{{}}}
\bibcite{keller2018predictive}{{41}{2018}{{Keller and Mrsic-Flogel}}{{}}}
\bibcite{keller2012sensorimotor}{{42}{2012}{{Keller et~al.}}{{Keller, Bonhoeffer, and H{\"u}bener}}}
\bibcite{kiani2009representation}{{43}{2009}{{Kiani and Shadlen}}{{}}}
\bibcite{knill2004bayesian}{{44}{2004}{{Knill and Pouget}}{{}}}
\bibcite{kording2004bayesian}{{45}{2004}{{K{\"o}rding and Wolpert}}{{}}}
\bibcite{kutschireiter2023bayesian}{{46}{2023}{{Kutschireiter et~al.}}{{Kutschireiter, Basnak, Wilson, and Drugowitsch}}}
\bibcite{larkum2013cellular}{{47}{2013}{{Larkum}}{{}}}
\bibcite{lawson2021computational}{{48}{2021}{{Lawson et~al.}}{{Lawson, Bisby, Nord, Burgess, and Rees}}}
\bibcite{liakoni2021learning}{{49}{2021}{{Liakoni et~al.}}{{Liakoni, Modirshanechi, Gerstner, and Brea}}}
\bibcite{ma2006bayesian}{{50}{2006}{{Ma et~al.}}{{Ma, Beck, Latham, and Pouget}}}
\bibcite{markram2004interneurons}{{51}{2004}{{Markram et~al.}}{{Markram, Toledo-Rodriguez, Wang, Gupta, Silberberg, and Wu}}}
\bibcite{marshall2016pharmacological}{{52}{2016}{{Marshall et~al.}}{{Marshall, Mathys, Ruge, De~Berker, Dayan, Stephan, and Bestmann}}}
\bibcite{masset2020behavior}{{53}{2020}{{Masset et~al.}}{{Masset, Ott, Lak, Hirokawa, and Kepecs}}}
\bibcite{meirhaeghe2021precise}{{54}{}{{Meirhaeghe et~al.}}{{Meirhaeghe, Sohn, and Jazayeri}}}
\bibcite{mumford1992computational}{{55}{1992}{{Mumford}}{{}}}
\bibcite{murray2014hierarchy}{{56}{2014}{{Murray et~al.}}{{Murray, Bernacchia, Freedman, Romo, Wallis, Cai, Padoa-Schioppa, Pasternak, Seo, Lee, et~al.}}}
\bibcite{nadim2014neuromodulation}{{57}{2014}{{Nadim and Bucher}}{{}}}
\bibcite{o2010coding}{{58}{2010}{{O'Neill and Schultz}}{{}}}
\bibcite{o2012can}{{59}{2012}{{O\IeC {\textquoteright }Reilly et~al.}}{{O\IeC {\textquoteright }Reilly, Jbabdi, and Behrens}}}
\bibcite{o2022prediction}{{60}{2022}{{O\IeC {\textquoteright }Toole et~al.}}{{O\IeC {\textquoteright }Toole, Oyibo, and Keller}}}
\bibcite{pakan2018impact}{{61}{2018}{{Pakan et~al.}}{{Pakan, Currie, Fischer, and Rochefort}}}
\bibcite{pawlak2010timing}{{62}{2010}{{Pawlak et~al.}}{{Pawlak, Wickens, Kirkwood, and Kerr}}}
\bibcite{petzschner2011iterative}{{63}{2011}{{Petzschner and Glasauer}}{{}}}
\bibcite{pfeffer2013inhibition}{{64}{2013}{{Pfeffer et~al.}}{{Pfeffer, Xue, He, Huang, and Scanziani}}}
\bibcite{pouget2013probabilistic}{{65}{2013}{{Pouget et~al.}}{{Pouget, Beck, Ma, and Latham}}}
\bibcite{rakitin1998scalar}{{66}{1998}{{Rakitin et~al.}}{{Rakitin, Gibbon, Penney, Malapani, Hinton, and Meck}}}
\bibcite{rao1999predictive}{{67}{1999}{{Rao and Ballard}}{{}}}
\bibcite{ridley1981new}{{68}{1981}{{Ridley et~al.}}{{Ridley, Haystead, Baker, and Crow}}}
\bibcite{rowland2007bayesian}{{69}{2007}{{Rowland et~al.}}{{Rowland, Stanford, and Stein}}}
\bibcite{rudy2011three}{{70}{2011}{{Rudy et~al.}}{{Rudy, Fishell, Lee, and Hjerling-Leffler}}}
\bibcite{runyan2017distinct}{{71}{2017}{{Runyan et~al.}}{{Runyan, Piasini, Panzeri, and Harvey}}}
\bibcite{rushworth2008choice}{{72}{2008}{{Rushworth and Behrens}}{{}}}
\bibcite{rutishauser2015representation}{{73}{2015}{{Rutishauser et~al.}}{{Rutishauser, Ye, Koroma, Tudusciuc, Ross, Chung, and Mamelak}}}
\bibcite{rutishauser2018single}{{74}{2018}{{Rutishauser et~al.}}{{Rutishauser, Aflalo, Rosario, Pouratian, and Andersen}}}
\bibcite{soltani2019adaptive}{{75}{2019}{{Soltani and Izquierdo}}{{}}}
\bibcite{summerfield2011perceptual}{{76}{2011}{{Summerfield et~al.}}{{Summerfield, Behrens, and Koechlin}}}
\bibcite{swanson2019hiring}{{77}{2019}{{Swanson and Maffei}}{{}}}
\bibcite{tremblay2016gabaergic}{{78}{2016}{{Tremblay et~al.}}{{Tremblay, Lee, and Rudy}}}
\bibcite{urban2016somatostatin}{{79}{2016}{{Urban-Ciecko and Barth}}{{}}}
\bibcite{wallace1998multisensory}{{80}{1998}{{Wallace et~al.}}{{Wallace, Meredith, and Stein}}}
\bibcite{wester2014behavioral}{{81}{2014}{{Wester and McBain}}{{}}}
\bibcite{white2016neurons}{{82}{2016}{{White and Monosov}}{{}}}
\bibcite{wilmes2023uncertainty}{{83}{2023}{{Wilmes et~al.}}{{Wilmes, Petrovici, Sachidhanandam, and Senn}}}
\bibcite{wilson1972excitatory}{{84}{1972}{{Wilson and Cowan}}{{}}}
\bibcite{wong2023computational}{{85}{2023}{{Wong et~al.}}{{Wong, Braun, Malagarriga, Moehlis, Moreno~Bote, Pouget, and Louis}}}
\bibcite{yon2021precision}{{86}{2021}{{Yon and Frith}}{{}}}
\bibcite{yu2005uncertainty}{{87}{2005}{{Yu and Dayan}}{{}}}
\citation{hertag2022prediction}
\citation{hertag2022prediction}
\citation{wilson1972excitatory}
\@writefile{toc}{\contentsline {section}{\numberline {A}Detailed Methods}{20}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Network model}{20}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Prediction-error network model}{20}{subsubsection.A.1.1}}
\citation{wilson1972excitatory}
\citation{pouget2013probabilistic}
\newlabel{eq:RateEqINs}{{9}{21}{Prediction-error network model}{equation.A.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.2}Memory and variance neuron}{21}{subsubsection.A.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.3}Weighted output}{21}{subsubsection.A.1.3}}
\citation{hertag2022prediction}
\citation{hertag2022prediction}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Connectivity}{22}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.1}Connections between neurons of the PE circuit}{22}{subsubsection.A.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.2}Connections between the PE circuit and the M neuron}{22}{subsubsection.A.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Gain factors for nPE and pPE neurons in three different mean-field networks (MFN). Each MFN differs with respect to the inputs onto SOM and VIP neurons. The interneurons either receive the feedforward (FF) or feedback (FB) input. All numbers are rounded to the first digit.}\relax }}{22}{table.caption.26}}
\newlabel{tab:gain_factors_MFN}{{1}{22}{\footnotesize {Gain factors for nPE and pPE neurons in three different mean-field networks (MFN). Each MFN differs with respect to the inputs onto SOM and VIP neurons. The interneurons either receive the feedforward (FF) or feedback (FB) input. All numbers are rounded to the first digit.}\relax }{table.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S0}{\ignorespaces \DIFdelbeginFL  \DIFdelendFL  \DIFaddbeginFL  \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Gain factors of nPE and pPE neurons in the multi-cell population model.\newline  } \DIFaddendFL  {The logarithm of the gain factors of nPE (top) and pPE (bottom) neurons in the \DIFaddbeginFL  {\color {blue}\uwave {multi-cell }}\DIFaddendFL  population model from \citep  {hertag2022prediction}. The network contains $67$ nPE neurons and $66$ pPE neurons. The remaining excitatory neurons were not classified as PE neurons and were not connected to the $M$ neuron.}\relax }}{23}{figure.caption.27}}
\newlabel{fig:Fig_gains}{{S0}{23}{\DIFdelbeginFL \DIFdelendFL \DIFaddbeginFL \footnotesize {\bf Gain factors of nPE and pPE neurons in the multi-cell population model.\newline } \DIFaddendFL {The logarithm of the gain factors of nPE (top) and pPE (bottom) neurons in the \DIFaddbeginFL \DIFaddFL {multi-cell }\DIFaddendFL population model from \citep {hertag2022prediction}. The network contains $67$ nPE neurons and $66$ pPE neurons. The remaining excitatory neurons were not classified as PE neurons and were not connected to the $M$ neuron.}\relax }{figure.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {$w_\mathrm  {X\leftarrow M}$ for the post-synaptic SOM and VIP neurons in all three mean-field networks considered.}\relax }}{23}{table.caption.28}}
\newlabel{tab:wXM}{{2}{23}{\footnotesize {$w_\mathrm {X\leftarrow M}$ for the post-synaptic SOM and VIP neurons in all three mean-field networks considered.}\relax }{table.caption.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.3}Connections between the PE circuit and the V neuron}{23}{subsubsection.A.2.3}}
\newlabel{eq:theta}{{14}{23}{Connections between the PE circuit and the V neuron}{equation.A.14}{}}
\citation{hertag2022prediction}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Inputs}{24}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Simulations}{24}{subsection.A.4}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Supporting analyses}{25}{appendix.B}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Activity of M and V neuron in a simplified model}{25}{subsection.B.1}}
\newlabel{sec:toy}{{B.1}{25}{Activity of M and V neuron in a simplified model}{subsection.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Impact of PE neurons' gain on estimating mean and variance}{26}{subsection.B.2}}
\newlabel{sec:impact_gain}{{B.2}{26}{Impact of PE neurons' gain on estimating mean and variance}{subsection.B.2}{}}
\newlabel{sec:gain_impact}{{B.2}{26}{Impact of PE neurons' gain on estimating mean and variance}{subsection.B.2}{}}
\newlabel{eq:condition_mean_gain_equal}{{21}{26}{Impact of PE neurons' gain on estimating mean and variance}{equation.B.21}{}}
\newlabel{eq:prediction_gain}{{22}{26}{Impact of PE neurons' gain on estimating mean and variance}{equation.B.22}{}}
\newlabel{eq:condition_variance_gain}{{23}{26}{Impact of PE neurons' gain on estimating mean and variance}{equation.B.23}{}}
\newlabel{eq:variance_gain}{{25}{26}{Impact of PE neurons' gain on estimating mean and variance}{equation.B.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Impact of PE neurons' baseline on estimating mean and variance}{27}{subsection.B.3}}
\newlabel{sec:impact_baseline}{{B.3}{27}{Impact of PE neurons' baseline on estimating mean and variance}{subsection.B.3}{}}
\newlabel{eq:condition_baseline_mean}{{26}{27}{Impact of PE neurons' baseline on estimating mean and variance}{equation.B.26}{}}
\newlabel{eq:condition_baseline_mean_1}{{27}{27}{Impact of PE neurons' baseline on estimating mean and variance}{equation.B.27}{}}
\newlabel{eq:condition_baseline_variance}{{28}{27}{Impact of PE neurons' baseline on estimating mean and variance}{equation.B.28}{}}
\newlabel{eq:condition_baseline_variance_1}{{29}{27}{Impact of PE neurons' baseline on estimating mean and variance}{equation.B.29}{}}
\newlabel{eq:condition_baseline_variance_2}{{30}{27}{Impact of PE neurons' baseline on estimating mean and variance}{equation.B.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Modelling the impact of neuromodulators on the sensory weight}{27}{subsection.B.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Sensory weight and contraction bias}{28}{subsection.B.5}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Supplementary Figures}{29}{appendix.C}}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Estimating mean and variance of different stimulus distributions.\newline  } Top: The normalised absolute difference between the averaged mean and the activity of the M neuron decreases to a near-zero level for all stimulus distributions tested. Bottom: The normalised absolute difference between the averaged variance and the activity of the V neuron decreases with small differences between the distributions tested. Parametrisation of the uniform distribution as in Fig. \ref  {fig:Fig_2}. \relax }}{29}{figure.caption.29}}
\newlabel{fig:Fig_2_S1}{{S1}{29}{\footnotesize {\bf Estimating mean and variance of different stimulus distributions.\newline } Top: The normalised absolute difference between the averaged mean and the activity of the M neuron decreases to a near-zero level for all stimulus distributions tested. Bottom: The normalised absolute difference between the averaged variance and the activity of the V neuron decreases with small differences between the distributions tested. Parametrisation of the uniform distribution as in Fig. \ref {fig:Fig_2}. \relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Estimating mean and variance of sensory stimuli in a rate-based population network.\newline  } {\bf  (A)} Illustration of the rate-based population network and the stimuli over time. \DIFaddbeginFL  {\color {blue}\uwave {The weights from the PE neurons onto the M or V neuron are scaled by a factor $\gamma $ drawn from a normal distribution $N(\mu _\mathrm  {\gamma }, \sigma _\mathrm  {\gamma })$. }}\DIFaddendFL  {\bf  (B)} M and V neuron activities over time for one example parameterisation. {\bf  (C)} The normalised absolute difference between the averaged mean and the activity of the M neuron (dark green) or between the averaged variance and the activity of the V neuron (brown) for uncorrelated deviations, that is, increasing \DIFdelbeginFL  {\color {red}\sout {SD of $\gamma $ }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {$\sigma _\mathrm  {\gamma }$ }}\DIFaddendFL  (left), correlated deviations, that is, increasing \DIFdelbeginFL  {\color {red}\sout {mean of $\gamma $ }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {$\mu _\mathrm  {\gamma }$ }}\DIFaddendFL  (middle), and the network sparsity. \DIFaddbeginFL  {\color {blue}\uwave {To speed up simulations, we chose $\lambda ^\mathrm  {lower}=5\cdot 10^{-2}$. }}\DIFaddendFL  \relax }}{30}{figure.caption.30}}
\newlabel{fig:Fig_2_S2}{{S2}{30}{\footnotesize {\bf Estimating mean and variance of sensory stimuli in a rate-based population network.\newline } {\bf (A)} Illustration of the rate-based population network and the stimuli over time. \DIFaddbeginFL \DIFaddFL {The weights from the PE neurons onto the M or V neuron are scaled by a factor $\gamma $ drawn from a normal distribution $N(\mu _\mathrm {\gamma }, \sigma _\mathrm {\gamma })$. }\DIFaddendFL {\bf (B)} M and V neuron activities over time for one example parameterisation. {\bf (C)} The normalised absolute difference between the averaged mean and the activity of the M neuron (dark green) or between the averaged variance and the activity of the V neuron (brown) for uncorrelated deviations, that is, increasing \DIFdelbeginFL \DIFdelFL {SD of $\gamma $ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {$\sigma _\mathrm {\gamma }$ }\DIFaddendFL (left), correlated deviations, that is, increasing \DIFdelbeginFL \DIFdelFL {mean of $\gamma $ }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {$\mu _\mathrm {\gamma }$ }\DIFaddendFL (middle), and the network sparsity. \DIFaddbeginFL \DIFaddFL {To speed up simulations, we chose $\lambda ^\mathrm {lower}=5\cdot 10^{-2}$. }\DIFaddendFL \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S3}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Estimating mean and variance of spatial stimuli.\newline  } {\bf  (A)} Illustration of a network estimating the mean and variance of a stimulus that varies across space. To simulate selectivity, the network comprises $1000$ identical, uncoupled mean-field networks each receiving a different input value drawn from a uniform distribution. {\bf  (B)} Activity of M neuron (top) and V neuron (bottom) for 2 stimuli. The second stimulus does either differ in the mean (orange) or the variance (yellow) from the first stimulus (indicated in C). {\bf  (C)} The normalised absolute difference between the averaged mean and the activity of the M neuron (dark green, top) or between the averaged variance and the activity of the V neuron (brown, bottom) for a range of different stimulus statistics. The examples from B are shown with colored arrows and markers. \DIFaddbeginFL  {\color {blue}\uwave {To speed up simulations, we chose $\lambda ^\mathrm  {lower}=3\cdot 10^{-1}$. }}\DIFaddendFL  \relax }}{30}{figure.caption.31}}
\newlabel{fig:Fig_2_S3}{{S3}{30}{\footnotesize {\bf Estimating mean and variance of spatial stimuli.\newline } {\bf (A)} Illustration of a network estimating the mean and variance of a stimulus that varies across space. To simulate selectivity, the network comprises $1000$ identical, uncoupled mean-field networks each receiving a different input value drawn from a uniform distribution. {\bf (B)} Activity of M neuron (top) and V neuron (bottom) for 2 stimuli. The second stimulus does either differ in the mean (orange) or the variance (yellow) from the first stimulus (indicated in C). {\bf (C)} The normalised absolute difference between the averaged mean and the activity of the M neuron (dark green, top) or between the averaged variance and the activity of the V neuron (brown, bottom) for a range of different stimulus statistics. The examples from B are shown with colored arrows and markers. \DIFaddbeginFL \DIFaddFL {To speed up simulations, we chose $\lambda ^\mathrm {lower}=3\cdot 10^{-1}$. }\DIFaddendFL \relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S4}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Dynamic variance estimation allows flexible adaptation to changes in the stimulus statistics and environment. \newline  } {\bf  (A)} Sensory weight for different input statistics. Numbers denote specific example statistics. Arrows denote the transitions between those statistics. {\bf  (B)} The sensory weight over time is shown for all transitions in (A). For the sake of clarity, we only show the trials \DIFdelbeginFL  {\color {red}\sout {40 -60. }}\DIFdelendFL  \DIFaddbeginFL  {\color {blue}\uwave {40-60. }}\DIFaddendFL  The switch to new input statistics occurs at trial 50. Parameters are listed in the Supporting Information. \relax }}{31}{figure.caption.32}}
\newlabel{fig:Fig_3_S1}{{S4}{31}{\footnotesize {\bf Dynamic variance estimation allows flexible adaptation to changes in the stimulus statistics and environment. \newline } {\bf (A)} Sensory weight for different input statistics. Numbers denote specific example statistics. Arrows denote the transitions between those statistics. {\bf (B)} The sensory weight over time is shown for all transitions in (A). For the sake of clarity, we only show the trials \DIFdelbeginFL \DIFdelFL {40 -60. }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {40-60. }\DIFaddendFL The switch to new input statistics occurs at trial 50. Parameters are listed in the Supporting Information. \relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S5}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Perturbing the weighting of sensory inputs and predictions by altering network properties. \newline  } {\bf  (A)} The weights from the PE neurons to the M neuron in the lower-order subnetwork are scaled by a factor the 0.3 or 7, leading to a distorted sensory weight. If the update of the M neuron in the lower subnetwork is too slow ($\blacktriangleleft $), the prediction is overrated. If the update of the M neuron in the lower subnetwork is too fast ($\blacktriangleright $), the sensory input is overrated. {\bf  (B)} The precise activation function for the V neurons does not have a major impact on the sensory weight. Only for inputs with high stimulus variability, the sensory stimulus is slightly overrated when the quadratic activation function is replaced by a linear, rectified activation function. \relax }}{31}{figure.caption.33}}
\newlabel{fig:Fig_3_S2}{{S5}{31}{\footnotesize {\bf Perturbing the weighting of sensory inputs and predictions by altering network properties. \newline } {\bf (A)} The weights from the PE neurons to the M neuron in the lower-order subnetwork are scaled by a factor the 0.3 or 7, leading to a distorted sensory weight. If the update of the M neuron in the lower subnetwork is too slow ($\blacktriangleleft $), the prediction is overrated. If the update of the M neuron in the lower subnetwork is too fast ($\blacktriangleright $), the sensory input is overrated. {\bf (B)} The precise activation function for the V neurons does not have a major impact on the sensory weight. Only for inputs with high stimulus variability, the sensory stimulus is slightly overrated when the quadratic activation function is replaced by a linear, rectified activation function. \relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S6}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  The impact of neuromodulators acting globally on groups of interneurons. \newline  } The sensory weight changes when groups of interneurons are targeted by a neuromodulator. Whether the sensory weight decreases or increases not also depends on the modulation strength (see Fig. \ref  {fig:Fig_4}) but also on how strongly which interneuron is targeted. As shown in Fig. \ref  {fig:Fig_4}, the sensory weight is pushed toward 0.5 if the VIP neuron is stimulated. The sensory weight generally decreases when PV neurons are the main target. Considered are two limit cases (upper row: more sensory-driven before modulation, lower row: more prediction-driven before modulation). The results are shown for three mean-field networks (see \ref  {fig:Fig_4}). \relax }}{32}{figure.caption.34}}
\newlabel{fig:Fig_4_S1}{{S6}{32}{\footnotesize {\bf The impact of neuromodulators acting globally on groups of interneurons. \newline } The sensory weight changes when groups of interneurons are targeted by a neuromodulator. Whether the sensory weight decreases or increases not also depends on the modulation strength (see Fig. \ref {fig:Fig_4}) but also on how strongly which interneuron is targeted. As shown in Fig. \ref {fig:Fig_4}, the sensory weight is pushed toward 0.5 if the VIP neuron is stimulated. The sensory weight generally decreases when PV neurons are the main target. Considered are two limit cases (upper row: more sensory-driven before modulation, lower row: more prediction-driven before modulation). The results are shown for three mean-field networks (see \ref {fig:Fig_4}). \relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S7}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  The impact of neuromodulators acting locally on groups of interneurons. \newline  } {\bf  (A)} Sensory weight changes with neuromodulators acting on interneurons in the lower PE circuit. {\bf  (B)} Sensory weight changes with neuromodulators acting on interneurons in the higher PE circuit. In general, the changes in the sensory weight is the opposite of the changes seen for neuromodulators acting on the lower-level PE neurons. Simulation parameters, labels and colors as in Fig. \ref  {fig:Fig_4}. \relax }}{33}{figure.caption.35}}
\newlabel{fig:Fig_4_S2}{{S7}{33}{\footnotesize {\bf The impact of neuromodulators acting locally on groups of interneurons. \newline } {\bf (A)} Sensory weight changes with neuromodulators acting on interneurons in the lower PE circuit. {\bf (B)} Sensory weight changes with neuromodulators acting on interneurons in the higher PE circuit. In general, the changes in the sensory weight is the opposite of the changes seen for neuromodulators acting on the lower-level PE neurons. Simulation parameters, labels and colors as in Fig. \ref {fig:Fig_4}. \relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S8}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Biased mean and variance estimation by changing the baseline and the gain of nPE and pPE.} In a toy model, described in sections \ref  {sec:gain_impact} and \ref  {sec:impact_baseline}, the contribution of gain and baseline to the changes in the mean and variance estimation are summarized. The results are based on the Eqs. (\ref  {eq:prediction_gain}), (\ref  {eq:variance_gain}), (\ref  {eq:condition_baseline_mean_1}) and (\ref  {eq:condition_baseline_variance_2}). \relax }}{33}{figure.caption.36}}
\newlabel{fig:Fig_4_S3}{{S8}{33}{\footnotesize {\bf Biased mean and variance estimation by changing the baseline and the gain of nPE and pPE.} In a toy model, described in sections \ref {sec:gain_impact} and \ref {sec:impact_baseline}, the contribution of gain and baseline to the changes in the mean and variance estimation are summarized. The results are based on the Eqs. (\ref {eq:prediction_gain}), (\ref {eq:variance_gain}), (\ref {eq:condition_baseline_mean_1}) and (\ref {eq:condition_baseline_variance_2}). \relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S9}{\ignorespaces \DIFdelbeginFL  \DIFdelendFL  \DIFaddbeginFL  \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  The combined changes in baseline and gain of all PE neurons determine the shift in the sensory weight.\newline  } \DIFaddendFL  Whether and how a neuromodulator changes the sensory weight depends on the interneuron targeted and the effect this interneuron has on the baseline and gain of both PE neurons, which in turn does depend on the network it is embedded in. For small inputs, changes in the baseline dominate, while for large inputs, the changes in the gains dominate the shift in the sensory weight. \relax }}{34}{figure.caption.37}}
\newlabel{fig:Fig_4_S4}{{S9}{34}{\DIFdelbeginFL \DIFdelendFL \DIFaddbeginFL \footnotesize {\bf The combined changes in baseline and gain of all PE neurons determine the shift in the sensory weight.\newline } \DIFaddendFL Whether and how a neuromodulator changes the sensory weight depends on the interneuron targeted and the effect this interneuron has on the baseline and gain of both PE neurons, which in turn does depend on the network it is embedded in. For small inputs, changes in the baseline dominate, while for large inputs, the changes in the gains dominate the shift in the sensory weight. \relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S10}{\ignorespaces \relax \fontsize  {8}{9.5}\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 3\p@ plus\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 3\p@ plus\p@ minus\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\bf  Including scalar variability in the model \newline  } When scalar variability is included, that is, the stimulus standard deviation depends linearly on the stimulus mean, the bias is larger for stimuli drawn from the upper end of the stimulus distribution than from the lower end. \relax }}{34}{figure.caption.38}}
\newlabel{fig:Fig_5_S1}{{S10}{34}{\footnotesize {\bf Including scalar variability in the model \newline } When scalar variability is included, that is, the stimulus standard deviation depends linearly on the stimulus mean, the bias is larger for stimuli drawn from the upper end of the stimulus distribution than from the lower end. \relax }{figure.caption.38}{}}
