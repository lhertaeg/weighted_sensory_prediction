#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Apr  6 07:47:35 2022

@author: loreen.hertaeg
"""

# %% Import

import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns

from src.Default_values import Default_PredProcPara
from src.Functions_Network import Neurons, Network, Activity_Zero, InputStructure, Stimulation, Simulation, RunStaticNetwork
from src.Functions_Network import Stimulation_new, RunStaticNetwork_new, sensory_input_euler
from src.Functions_PredNet import RunPredNet, Neurons, Network, InputStructure, Stimulation, Simulation, Activity_Zero
from src.Functions_PredNet import RunPredNet_MFN

import warnings
warnings.filterwarnings("ignore")

dtype = np.float32

# %% Note

# uncertainty_prediction here does not mean the variance of the prediction but the prediction of the variance! 

# %% Systematically vary key parameters and investigate their influence

# key parameters: mLE, stim_duration, n_trials, n_stim_per_trials, tc
# some of these parameters might only have an inlfuence in combination with another one

# you can also try but probably not relevant: BL (as long as symmetrical), tau, stimuli std, SD

flag = 1

if flag==1:
    
    print('XXX')
    

# %% Mean-field network (see below), input given to the network (not yet generated by another subcircuit)

# Remark: # the more n_trials the closer the lines for 'variance over all' and 'averaged variance over all'

flag = 0

if flag==1:

    ### files and folders
    folder = 'Certainty_prediction'
    folder_pre = 'Results/Data/'
    fln_load_after = 'Data_Optimal_Parameters_MFN_10'
    fln_save_data = 'Example_mean_field'
    
    ### Load data and set default values
    _, mLE, _, _, _, stimuli_weak, stimuli_strong = Default_PredProcPara()
    mLE *= 0.025

    with open(folder_pre + folder + '/' + fln_load_after + '.pickle','rb') as f:
        xopt, W, optimize_flag, _, _ = pickle.load(f)
   
    W[optimize_flag!=0] = xopt
    r_target = dtype([0, 0, 0, 0, 4, 4, 4, 4])
    fixed_input = (np.eye(8) - W) @ r_target
    tau_inv = [dtype(1/60), dtype(1.0/2.0)]
    
    ### Simulation parameters
    dt = dtype(1) # 0.1 
    stim_duration = dtype(1000)
    
    ### Stimulation protocol & Inputs
    ## Constructing the inputs (in a later step this would be replaced by the appropriate circuit that computes the "current" variance)
    n_trials = 40
    n_stim_per_trial = 50
    std_trials = np.random.uniform(0.5,4.5, size=n_trials)
    S = np.array([])
    
    mean_trails = np.zeros(n_trials, dtype=dtype)
    var_trials = np.zeros(n_trials, dtype=dtype)
    
    for i in range(n_trials):
        S_trial = np.random.normal(5, std_trials[i], size=n_stim_per_trial)
        S = np.concatenate((S,S_trial))
        mean_trails[i] = np.mean(S_trial)
        var_trials[i] = np.var(S_trial)
    
    n_stim_total = len(S)
    mean_sensory = np.mean(S) # each trial actually has the same mean
    s, tc = np.arange(n_stim_total), n_stim_per_trial/2
    pred_sensory_mean = (1 - np.exp(-s/tc)) * mean_sensory
    variance_vs_stim = (S - pred_sensory_mean)**2 # could be smoothened but for now ok as is
    
    SD = dtype(0.01)
    stimuli = np.zeros_like(variance_vs_stim, dtype=dtype)
    stimuli_std = np.ones_like(stimuli,dtype=dtype) * 0.01 # basically take it out
    stimuli[:] = variance_vs_stim
    
    ### Define connectivity between PE circuit and L
    U = np.zeros((1,8)) # PE circuit --> L
    U[0,0] = -mLE # nPE onto L
    U[0,1] = mLE # pPE onto L
    
    V = np.zeros((8,1)) # L --> PE circuit
    V[2:4,0] = dtype(1) # onto dendrites
    V[5,0] = dtype(1) # onto PV neuron receiving prediction
    V[7,0] = dtype(1) # onto V neuron
    
    ### Run network
    RunPredNet_MFN(U, V, W, tau_inv, stim_duration, dt, fixed_input, stimuli, stimuli_std, SD, folder, fln_save_data)
    
    ### Plotting
    PathData = 'Results/Data/' + folder
    arr = np.loadtxt(PathData + '/Data_StaticNetwork_MFN_' + fln_save_data + '.dat',delimiter=' ')
    t, R, L = arr[:,0], arr[:, 1:-2], arr[:,-1]
    
    plt.figure(figsize = (10,5), tight_layout=True)
    ax = plt.subplot(1,1,1)
    
    #ax.plot(np.arange(len(stimuli)), stimuli, '-o', color='#8E4162', alpha=0.5, label='momentary variance')
    ax.set_xlim([0,n_trials*n_stim_per_trial])
    for i in range(n_trials):
        ax.axhline(var_trials[i], i/n_trials, (i+1)/n_trials, color='#94B0DA', lw=3) #, label='trail varaince')
    
    ax.plot(t/stim_duration, L, color='#065A82', lw=3, label='prediction of variance')
    ax.axhline(np.mean(stimuli), color='#738C54', ls='-', lw=3, label='averaged variance over all')
    ax.axhline(np.var(S), color='#813405', ls='-', lw=3, label='variance over all')
    ax.legend(loc=0)
    ax.set_ylabel('Variance')
    ax.set_xlabel('stimuli number')
    sns.despine(ax=ax)

# %% First test - input is directly given (not connected to circuit that estimates the trail variance)

flag = 0

if flag==1:

    ### files and folders
    folder = 'Certainty_prediction'
    folder_pre = 'Results/Data/'
    fln_load_after = 'Example_Target_Input_After'
    fln_save_data = 'XX'
    
    ### Load data and set default values
    _, mLE, _, _, _, stimuli_weak, stimuli_strong = Default_PredProcPara()
    
    with open(folder_pre + folder + '/Activity_relative_to_BL_' + fln_load_after + '.pickle','rb') as f:
        _, _, _, _, _, bool_nPE, bool_pPE = pickle.load(f)
    
    
    with open(folder_pre + folder + '/Data_NetworkParameters_' + fln_load_after + '.pickle','rb') as f: 
        NeuPar_PE, NetPar_PE, InPar_PE, _, _, _, _ = pickle.load(f)
    
    
    ### Neuron parameters
    NeuPar = Neurons()
    
    ### Network parameters
    mLE = 1e-3
    NetPar = Network(NeuPar, NetPar_PE, mLE, bool_nPE, bool_pPE, InPar_PE.neurons_motor)
    
    ## Initial activity levels
    RatePar = Activity_Zero(NeuPar, 0)
    RatePar.rL0 = np.array([0], dtype=dtype)
    
    ### Simulation parameters
    SimPar = Simulation(stim_duration=dtype(500), dt=dtype(1))
    
    ### Stimulation protocol & Inputs
    InPar = InputStructure(NeuPar, InPar_PE)
    
    ## Constructing the inputs (in a later step this would be replaced by the appropriate circuit that computes the "current" variance)
    n_stim_per_trial = 500
    S1 = np.random.uniform(0, 10, size=n_stim_per_trial) # trial 1
    S2 = np.random.uniform(3, 7, size=n_stim_per_trial)
    S3 = np.random.uniform(2, 8, size=n_stim_per_trial)
    S4 = np.random.uniform(4, 6, size=n_stim_per_trial)
    S5 = np.random.uniform(0, 10, size=n_stim_per_trial)
    S = np.concatenate((S1, S2, S3, S4, S5))
    
    n_stim_total = len(S)
    mean_sensory = np.mean(S) # each trial actually has the same mean
    s, tc = np.arange(n_stim_total), n_stim_per_trial/2
    pred_sensory_mean = (1 - np.exp(-s/tc)) * mean_sensory
    variance_vs_stim = (S - pred_sensory_mean)**2 # could be smoothened but for now ok as is
    
    stimuli = np.zeros_like(variance_vs_stim, dtype=dtype)
    std_arr = np.ones_like(stimuli,dtype=dtype) * 0.01 # basically take it out
    stimuli[:] = variance_vs_stim
    StimPar = Stimulation(stimuli, std_arr, SD_individual = dtype(0.01))
    
    ### Run network 
    RunPredNet(NeuPar, NetPar, InPar, StimPar, SimPar, RatePar, folder, fln_save_data)
    
    ### Plotting
    PathData = 'Results/Data/' + folder
    arr = np.loadtxt(PathData + '/Data_StaticPredNet_P_' + fln_save_data + '.dat',delimiter=' ')
    t, L = arr[:,0], arr[:, 1]
    
    plt.figure(figsize = (10,5), tight_layout=True)
    
    ax = plt.subplot(1,1,1)
    ax.plot(t/SimPar.stim_duration, stimuli, 'o', color='#8E4162', alpha=0.5, label='momentary variance')
    ax.plot(t/SimPar.stim_duration, L, color='#065A82', lw=3, label='prediction of variance')
    ax.axhline(np.mean(stimuli), color='#738C54', ls='-', lw=3, label='averaged variance over all')
    ax.legend(loc=0)
    ax.set_ylabel('Variance')
    ax.set_xlabel('stimuli number')
    sns.despine(ax=ax)


# %% Test variance over trails -  toy model

# Different types of PE neurons and memory neurons:
    # PE neurons for mean & memory neuron holding the mean
    # PE neurons for varaince & memory neurons holding the variance

flag = 0

if flag==1:
    
    N = 500 # number of trials
    S1 = np.random.uniform(0, 10, size=N)
    S2 = np.random.uniform(3, 7, size=N)
    S3 = np.random.uniform(2, 8, size=N)
    S4 = np.random.uniform(4, 6, size=N)
    S5 = np.random.uniform(0, 10, size=N)
    S = np.concatenate((S1, S2, S3, S4, S5))
    
    eta = 1e-4
    eta_SD = 1e-5
    tau = 100
    tau_SD = 10
    E = 0
    SD = 0
    SD_long = 0
    E_SD = 0
    
    rate_1 = np.zeros_like(S)
    rate_2 = np.zeros_like(S)
    rate_3 = np.zeros_like(S)
    
    for i, s in enumerate(S):
        
        pPE = (np.maximum(s - E,0))**2
        nPE = (np.maximum(E - s,0))**2
        E_SD = (1-1/tau) * E_SD + s/tau
        SD = (1-1/tau_SD) * SD + (nPE+pPE)/tau_SD
        
        for t in np.arange(500):
            
            pPE_mean = (np.maximum(s - E,0))**2
            nPE_mean = (np.maximum(E - s,0))**2
            E += eta * (pPE_mean - nPE_mean)/tau
            
            # pPE = (np.maximum(s - E,0))**2
            # nPE = (np.maximum(E - s,0))**2
            # E_SD = (1-1/tau) * E_SD + s/tau
            # SD = (1-1/tau_SD) * SD + (nPE+pPE)/tau_SD
            
            pPE_SD = (np.maximum(SD - SD_long,0))**2
            nPE_SD = (np.maximum(SD_long - SD,0))**2
            SD_long += eta_SD * (pPE_SD - nPE_SD)/tau
                  
        rate_1[i] = E
        rate_2[i] = SD
        rate_3[i] = SD_long
    
    plt.figure()
    plt.plot(rate_1,'b')
    plt.plot(rate_2,'m')
    plt.plot(rate_3,'r')
    ax = plt.gca()
    ax.axhline(np.var(S), ls=':', color='k')
    
    